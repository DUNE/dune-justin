#!/usr/bin/python3
#
#  wfa-cgi - Workflow Allocator CGI service
#
#  Andrew McNab, University of Manchester.
#  Copyright (c) 2013-22. All rights reserved.
#
#  Redistribution and use in source and binary forms, with or
#  without modification, are permitted provided that the following
#  conditions are met:
#
#    o Redistributions of source code must retain the above
#      copyright notice, this list of conditions and the following
#      disclaimer. 
#    o Redistributions in binary form must reproduce the above
#      copyright notice, this list of conditions and the following
#      disclaimer in the documentation and/or other materials
#      provided with the distribution. 
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
#  CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
#  INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
#  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
#  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
#  BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
#  ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
#  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
#  POSSIBILITY OF SUCH DAMAGE.
#

#  This CGI script must be run from an Apache httpd server with
#  X.509 proxy certificates enabled. On a systemd system (like 
#  CentOS 7) you need to enable this in OpenSSL inside mod_ssl
#  by adding this line to /usr/lib/systemd/system/httpd.service
#  in the [Service] section:
#
#  Environment=OPENSSL_ALLOW_PROXY_CERTS=1

import os
import io
import re
import sys
import time
import json
import uuid
import string
import tarfile
import MySQLdb

# wfs/conf.py must define these variables in a way that is both
# valid Python and valid Bash!
#
# mysqlUser='username'
# mysqlPassword='PAsSWoRd'
#
import wfs.conf

# Global database connection and ID, usable anywhere
db            = None
cur           = None
allocatorName = None

# Return various strings and SQL expressions which are 
# used in subsequent queries
def makeQueryTerms(jsonDict, job):

  query = ('SELECT sites_storages.rse_id,location,rse_name,occupancy '
           'FROM sites_storages '
           'LEFT JOIN storages ON storages.rse_id=sites_storages.rse_id '
           'WHERE sites_storages.site_name="%s" '
           'ORDER BY location,occupancy,RAND();' 
           % job['dune_site'])

  cur.execute(query)
  storageRows = cur.fetchall()

  outputRseList  = [] 
  samesiteList   = []
  nearbyList     = []
  accessibleList = []

  for storageRow in storageRows:

    if storageRow['occupancy'] < 1.0:
      outputRseList.append(storageRow['rse_name'])
  
    if storageRow['location'] == 'accessible':
      accessibleList.append('replicas.rse_id=%s' % storageRow['rse_id'])
    elif storageRow['location'] == 'nearby':
      nearbyList.append('replicas.rse_id=%s' % storageRow['rse_id'])
    elif storageRow['location'] == 'samesite':
      samesiteList.append('replicas.rse_id=%s' % storageRow['rse_id'])

  storageWhere = ' OR '.join(samesiteList + nearbyList)

  if accessibleList:
    if storageWhere:
      storageWhere += ' OR '

    storageWhere += ('(stages.any_location AND (' + 
                     ' OR '.join(accessibleList) + '))')

  if storageWhere:
    storageWhere = ' AND (' + storageWhere + ') '

  # All storages in the same class (samesite, nearby, accessible) get the
  # same ranking score (3,2,1). In the future, we can apply individual
  # scores here to each storage relative to where the job is running.
 
  if samesiteList:
    storageOrder = '3*(' + ' OR '.join(samesiteList) + ')'
  else:
    storageOrder = ''

  if nearbyList:
    if storageOrder:
      storageOrder += ' + '
  
    storageOrder += '2*(' + ' OR '.join(nearbyList) + ')'

  if accessibleList:
    if storageOrder:
      storageOrder += ' + '

    storageOrder += '1*(' + ' OR '.join(accessibleList) + ')'

  # If we got anything for the storage ordering then complete the
  # expression including the comma; otherwise an empty string
  if storageOrder:
    storageOrder += ' DESC,'

  return { "outputRseList"  : outputRseList,
           "samesiteList"   : samesiteList,
           "nearbyList"     : nearbyList,
           "accessibleList" : accessibleList,
           "storageWhere"   : storageWhere,
           "storageOrder"   : storageOrder
         }

# Just in time decision making: identify the best request+stage combination
# based on the immediate situation rather than trying to plan ahead
def findStage(queryTerms, job):

  query = (
 "SELECT stages.request_id,stages.stage_id,"
 "stages.any_location,"
 "files.file_did,files.file_id,storages.rse_name "
 "FROM files "
 "LEFT JOIN stages ON files.request_id=stages.request_id AND "
 "files.stage_id=stages.stage_id "
 "LEFT JOIN requests ON requests.request_id=files.request_id "
 "LEFT JOIN replicas ON files.file_id=replicas.file_id "
 "LEFT JOIN storages ON replicas.rse_id=storages.rse_id "
 "WHERE files.state='unallocated' AND " +
 str(job["processors"])   + " <= stages.max_processors AND " +
 str(job["processors"])   + " >= stages.min_processors AND " +
 str(job["rss_bytes"])    + " >= stages.max_rss_bytes AND " +
 str(job["wall_seconds"]) + " >= stages.max_wall_seconds " +
 queryTerms["storageWhere"] + " AND storages.rse_name IS NOT NULL " +
 " ORDER BY " + 
 queryTerms["storageOrder"] + "files.request_id,files.file_id"
 " LIMIT 1 FOR UPDATE"
 )

  cur.execute(query)
  fileRows = cur.fetchall()
  
  if len(fileRows) == 0:
    return None

  # Take the values of the highest priority result
  requestID   = int(fileRows[0]['request_id'])
  stageID     = int(fileRows[0]['stage_id'])
  anyLocation = bool(fileRows[0]['any_location'])
  
  # The dictionary to return
  stage = { 'request_id'  : requestID,
            'stage_id'    : stageID,
            'any_location': anyLocation }

  return stage

# Make a dictionary containing one job's information
def findJob(jsonDict):

  try:
    jobID  = int(jsonDict['job_id'])
    cookie = str(jsonDict['cookie'])
  except:
    return None

  # Find the job info and the stage's any_location flag
  try:
    query = ('SELECT jobs.request_id,'
             'jobs.stage_id,'
             'jobs.state,'
             'jobs.dune_site,'
             'jobs.hostname,'
             'jobs.cpuinfo,'
             'jobs.os_release,'
             'jobs.rss_bytes,'
             'jobs.processors,'
             'jobs.wall_seconds,'
             'stages.any_location '
             'FROM jobs '
             'LEFT JOIN stages ON jobs.stage_id=stages.stage_id '
             'WHERE jobs.job_id=' + str(jobID) + ' AND ' +
             'jobs.cookie="' + cookie + '"')

    cur.execute(query)
    rows=cur.fetchall()
    
    job = { 'job_id'       : jobID,
            'cookie'       : cookie,
            'request_id'   : int(rows[0]['request_id']),
            'stage_id'     : int(rows[0]['stage_id']),
            'state'        : rows[0]['state'],
            'dune_site'    : rows[0]['dune_site'],
            'hostname'     : rows[0]['hostname'],
            'cpuinfo'      : rows[0]['cpuinfo'],
            'os_release'   : rows[0]['os_release'],
            'rss_bytes'    : int(rows[0]['rss_bytes']),
            'processors'   : int(rows[0]['processors']),
            'wall_seconds' : int(rows[0]['wall_seconds']),
            'any_location' : bool(rows[0]['any_location'])
          }
          
    return job
    
  except:
    # WE SHOULD PROVIDE SOME KIND OF DEBUGGING FOR ALL THIS!!
    return None

# Make a dictionary containing one file's information
def findOneFile(jsonDict, queryTerms, job):

  if job['any_location']:
    # If this stage can access data on any accessible storage, then 
    # use all three lists
    storageWhere = ' OR '.join(queryTerms["samesiteList"] + 
                               queryTerms["nearbyList"] + 
                               queryTerms["accessibleList"])
  else:
    # Otherwise just use samesite and nearby lists of storages
    storageWhere = ' OR '.join(queryTerms["samesiteList"] + 
                               queryTerms["nearbyList"])

  if storageWhere:
    storageWhere = ' AND (' + storageWhere + ') '

  query = (
"SELECT files.file_id,files.file_did,storages.rse_name,"
"replicas.rse_id,replicas.pfn "
"FROM files "
"LEFT JOIN replicas ON files.file_id=replicas.file_id "
"LEFT JOIN storages ON replicas.rse_id=storages.rse_id "
"WHERE files.state='unallocated' AND files.request_id=" +
str(job['request_id']) + " AND files.stage_id=" + str(job['stage_id']) + 
storageWhere + " AND storages.rse_NAME IS NOT NULL "
"ORDER BY " + 
queryTerms["storageOrder"] + "files.file_id"
" LIMIT 1 FOR UPDATE"
) 
   
  print('DEBUG: ' + query, file=sys.stderr)
   
  cur.execute(query)
  fileRows = cur.fetchall()
  
  print('DEBUG: ' + str(fileRows), file=sys.stderr)
  
  if len(fileRows) == 0:
    return None

  try: 
    query = ("UPDATE files SET state='allocated',"
             "allocated_time=NOW(),"
             "allocated_rse_id=" + str(fileRows[0]['rse_id']) + ","
             "job_id=" + str(job['job_id']) + " "
             "WHERE file_id=" + str(fileRows[0]['file_id'])
            )
    cur.execute(query)
  except:
    # If anything goes wrong, we stop straightaway
    return None

  # The dictionary to return
  oneFile = { 'file_did'    : fileRows[0]['file_did'],
              'pfn'         : fileRows[0]['pfn'],
              'rse_name'    : fileRows[0]['rse_name']
            }
                      
  return oneFile

# Successfully identified the request+stage and files so get the 
# bootstrap for this stage and return it
def getBootstrap(queryTerms, stage):

  try:
    query = ("SELECT bootstrap FROM bootstraps "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    cur.execute(query)
    rows = cur.fetchall()
    bootstrap = rows[0]['bootstrap']
  except:
    return None
  
  return bootstrap

def addFileToTarFile(tar, name, value):

  buffer = io.BytesIO()
  buffer.write(value.encode())

  info       = tarfile.TarInfo(name = name)
  info.size  = buffer.tell()
  info.mtime = time.time()

  buffer.seek(0)
  tar.addfile(tarinfo = info, fileobj = buffer)

# Make an uncompressed tar file to return to the generic job. 
# The generic job 'owns' the files in the tar file, one of which is the
# boostrap.sh script which 'owned' by the user. The generic job has the 
# responsibility to upload the output files matching the patterns defined as
# part of the stage 
def makeTarFile(queryTerms, stage, jobID, cookie):

  buffer = io.BytesIO()
  tar = tarfile.TarFile(fileobj = buffer, mode = "w")

  # Get and add the bootstrap script
  bootstrap = getBootstrap(queryTerms, stage)
  if not bootstrap:
    return None

  addFileToTarFile(tar, "wfs-bootstrap.sh", bootstrap)

  # Create a file containing the output file patterns
  try:
    query = ("SELECT pattern,for_next_stage FROM stages_outputs "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    cur.execute(query)
    rows = cur.fetchall()
  except:
    return None

  patternsFile = ''

  for row in rows:
    patternsFile += str(row['for_next_stage']) + ' ' + row['pattern'] + '\n'
    
  addFileToTarFile(tar, 'wfs-output-patterns.txt', patternsFile)

  addFileToTarFile(tar, 'wfs-env.sh', 
       (
         'export WFS_JOB_ID=' + str(jobID) + '\n' +
         'export WFS_REQUEST_ID=' + str(stage['request_id']) + '\n' +
         'export WFS_STAGE_ID=' + str(stage['stage_id']) + '\n' +
         'export WFS_COOKIE="' + cookie + '"\n' +
         'export WFS_RSE_LIST="' + ' '.join(queryTerms['outputRseList']) + '"\n'
       )
                  )

  addFileToTarFile(tar, 'wfs-get-file.json', 
       (
         '{\n' +
         '"method" : "get_file",\n' +
         '"job_id" : ' + str(jobID) + ',\n' +
         '"cookie" : "' + cookie + '"\n' 
         '}\n'
       )
                  )

  tar.close()  
  return buffer.getvalue()

# Try to get the stage with the highest priority files still unallocated
def getStageMethod(jsonDict, jobUserID):

  job = {}

  # Check jsonDict contains required string values (eg dune_site)
  for name in ['job_name', 'dune_site', 'cpuinfo', 'os_release', 'hostname']:
    if name not in jsonDict:
      print('Status: 400 Bad Request')
      print()
      print('Missing value(s) in JSON')
      sys.exit(0)
    else:
      job[name] = str(jsonDict[name])

  # Check jsonDict contains required integer values (eg rss_bytes)
  for name in ['rss_bytes', 'processors', 'wall_seconds']:
    try:
      job[name] = int(jsonDict[name])
    except:
      print('Status: 400 Bad Request')
      print()
      print('Missing integer value(s) in JSON')
      sys.exit(0)

  # Make strings used in SQL queries
  queryTerms = makeQueryTerms(jsonDict, job)

  # Use the Just In Time decision making: identify the best request+stage 
  # candidate combination at this moment
  stage = findStage(queryTerms, job)

  if not stage:
    # No stages/files eligible to be processed by this job
    print('Status: 404 Not Found')
    print('Content-Type: text/plain')
    print()
    print('No eligible stages found')
    sys.exit(0)

  cookie  = str(uuid.uuid4())

  try:
    query = ('INSERT INTO jobs SET '
             'created_time=NOW(),' +
             'allocator_name="' + allocatorName + '",' +
             'job_name="' + job['job_name'] + '",' +
             'request_id=' + str(stage['request_id']) + ','
             'stage_id=' + str(stage['stage_id']) + ','
             'dune_site="' + job['dune_site'] + '",' +
             'cpuinfo="' + job['cpuinfo'] + '",' +
             'os_release="' + job['os_release'] + '",' +
             'hostname="' + job['hostname'] + '",' + 
             'rss_bytes=' + str(job['rss_bytes']) + ',' +
             'processors=' + str(job['processors']) + ',' +
             'wall_seconds=' + str(job['wall_seconds']) + ',' +
             'job_user_id=' + str(jobUserID) + ',' +
             'cookie="' + cookie + '"'
            )
    cur.execute(query)
  except:
    print('Status: 500 Internal Server Error')
    print('Content-Type: text/plain')
    print()
    print('Workflow allocator failed')
    sys.exit(0)

  jobID = cur.lastrowid

  tarFile = makeTarFile(queryTerms, stage, jobID, cookie)
        
  # All done so commit the job details.
  # We do this before the HTTP response in case
  # it is received ok by the job and run 
  # but times out and fails here on the server side
  db.commit()

  # Return the script to the workflow job 
  print('Status: 200 OK')
  print('Content-Type: application/x-tar')
  print()
  sys.stdout.flush()
  sys.stdout.buffer.write(tarFile)
  sys.exit(0)
   
# Get an unallocated file from the given request+stage
def getFileMethod(jsonDict):

  # Find details of this job
  job = findJob(jsonDict)

  if not job:
    print('Status: 400 Bad Request')
    print()
    print('No matching job found')
    sys.exit(0)

  # Make strings used in SQL queries
  queryTerms = makeQueryTerms(jsonDict, job)

  # Create a stage dictionary with the next file in this stage  
  oneFile = findOneFile(jsonDict, queryTerms, job)
  
  if oneFile:
    if job['state'] == 'started':
      try:
        query = ('UPDATE jobs SET state="processing" ' +
                 'WHERE job_id=' + str(job['job_id']))
                )
        cur.execute(query)
      except:
        print('Status: 500 Internal Server Error')
        print('Content-Type: text/plain')
        print()
        print('Unable to update job to processing')
        sys.exit(0)

    # All done so commit the allocation and updates
    # We do this before the HTTP response in case
    # it is received ok by the job and processed
    # but times out and fails here on the server side
    db.commit()

    # Now tell the job what file to process and where it is
    print('Status: 200 OK')
    print('Content-Type: text/plain')
    print()
    print(oneFile['file_did'] +  ' ' + 
          oneFile['pfn'] + ' ' + 
          oneFile['rse_name'])

    sys.exit(0)

  # No file eligible to be processed by this job
  print('Status: 404 Not Found')
  print()
  print('No eligible file found')
  sys.exit(0)   

def updateFileProcessing(fileList, state, job):

  processedList = []

  try:
    for fileDid in fileList:
      if '"' in fileDid or "'" in fileDid:
        continue

      processedList.append('file_did="' + str(fileDid) + '"')
  except:
    return

  # Update the files the job did or did not manage to process
  if processedList:
    try:
      query = ('UPDATE files SET state="' + state + '" '
               'WHERE (' + ' OR '.join(processedList) + ') '
               'AND request_id=' + str(job['request_id']) + ' ' 
               'AND stage_id=' + str(job['stage_id']) + ' ' 
               'AND state="allocated"')
      cur.execute(query)
    except:
      return
    
# Deal with the output files needed by the next stage
def processNextStageOutputs(jsonDict, job):

  try:
    for outputDid in jsonDict['next_stage_outputs']:
      
      if '"' in outputDid or "'" in outputDid:
        continue

      try:
        query = ('INSERT INTO files SET state="find_replicas",'
                 'file_did="'  + str(outputDid) + '",'
                 'request_id=' + str(job['request_id']) + ','
                 'stage_id='   + str(job['stage_id'] + 1)
                )
        cur.execute(query)
      except:
        # Just do our best if anything goes wrong for now
        # Should decide what to do in these partial failure cases
        pass

  except:
    return
                
def resultsMethod(jsonDict):

  # Check jsonDict contains required values (eg job_name)
  for name in ['cookie', 'processed_inputs', 
               'unprocessed_inputs', 'next_stage_outputs']:
    if name not in jsonDict:
      print('Status: 400 Bad Request')
      print()
      print('Missing value (%s) in JSON' % name)
      sys.exit(0)

  job = findJob(jsonDict)
  
  if not job:
    print('Status: 400 Bad Request')
    print()
    print('No matching job')
    sys.exit(0)

  updateFileProcessing(jsonDict['processed_inputs'], 'processed', job)

  updateFileProcessing(jsonDict['unprocessed_inputs'], 'unprocessed', job)

  processNextStageOutputs(jsonDict, job)

  try:
    query = ('UPDATE jobs SET state="finished" ' +
             'WHERE job_id=' + str(job['job_id']))
            )
    cur.execute(query)
  except:
    # Don't give up! Record what we can!
    pass

  # Commit everything, before we return OK to the job
  db.commit()

  # Return OK to the workflow job
  print('Status: 200 OK')
  print()
  sys.exit(0)

def getJobUserID():

  # Use certificates and proxies for now; will transition to tokens in future
  if 'SSL_CLIENT_S_DN' not in os.environ or not os.environ['SSL_CLIENT_S_DN']:
    print('Status: 403 Forbidden')
    print()
    print('Forbidden - identity not provided')
    sys.exit(0)

  # Convert to the older format with slashes 
  clientDN='/'+'/'.join(os.environ['SSL_CLIENT_S_DN'].split(',')[::-1])

  query = ('SELECT user_id FROM users '
           'WHERE x509dn=LEFT("%s",LENGTH(x509dn))' % 
           clientDN.replace('\\','\\\\').replace('"','\\"'))

  try:
    cur.execute(query)
    rows = cur.fetchall()
  except:
    print('Status: 500 Internal Server Error')
    print()
    print('Error reading database')
    sys.exit(0)

  try:
    jobUserID = int(rows[0]['user_id'])
  except:
    print('Status: 403 Forbidden')
    print()
    print('Forbidden - authorized identity not provided')
    sys.exit(0)

  return jobUserID

#
# PROGRAM MAIN
#

# Quickly reject random GETs etc (if not handled by Apache already)
if os.environ['REQUEST_METHOD'] != 'POST':
    print('Status: 405 Method not allowed')
    print()
    print('We only support POST')
    sys.exit(0)

# Create a unique ID string for this instance that may also help in debugging
allocatorName = "%s:%d:%f" % (os.uname()[1], os.getpid(), time.time())

# Get the JSON document POSTed to us
try:
  jsonDict = json.load(sys.stdin)
except:
  print('Status: 400 Bad Request')
  print()
  print('Failed to parse JSON')
  sys.exit(0)

# Check jsonDict specifies a method
if 'method' not in jsonDict:
  print('Status: 400 Bad Request')
  print()
  print('Missing method in JSON')
  sys.exit(0)

# Do as many checks as we can before connecting to the database here
try:
  db  = MySQLdb.connect(host="localhost", user=wfs.conf.mysqlUser, 
                        passwd=wfs.conf.mysqlPassword, db='wfdb')
  db.autocommit(False)
  cur = db.cursor(MySQLdb.cursors.DictCursor) 
except:
  print('Status: 500 Internal Server Error')
  print()
  print('Problem with database connection')
  sys.exit(0)

# Get the stage for the job to work on
if jsonDict['method'] == 'get_stage':
  # getJobUserID() fails with an HTTP error and exit if authorized ID not given
  jobUserID = getJobUserID()

  getStageMethod(jsonDict, jobUserID)

# Get one or more files to process  
elif jsonDict['method'] == 'get_file':
  getFileMethod(jsonDict)
  
# Return results of processing files
elif jsonDict['method'] == 'return_results':
  resultsMethod(jsonDict)

else:
  print('Status: 400 Bad Request')
  print()
  print('Method in JSON not recognised')
  sys.exit(0)

