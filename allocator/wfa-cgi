#!/usr/bin/python3
#
#  wfa-cgi - Workflow Allocator CGI service
#
#  Andrew McNab, University of Manchester.
#  Copyright (c) 2013-21. All rights reserved.
#
#  Redistribution and use in source and binary forms, with or
#  without modification, are permitted provided that the following
#  conditions are met:
#
#    o Redistributions of source code must retain the above
#      copyright notice, this list of conditions and the following
#      disclaimer. 
#    o Redistributions in binary form must reproduce the above
#      copyright notice, this list of conditions and the following
#      disclaimer in the documentation and/or other materials
#      provided with the distribution. 
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
#  CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
#  INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
#  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
#  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
#  BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
#  ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
#  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
#  POSSIBILITY OF SUCH DAMAGE.
#

#  This CGI script must be run from an Apache httpd server with
#  X.509 proxy certificates enabled. On a systemd system (like 
#  CentOS 7) you need to enable this in OpenSSL inside mod_ssl
#  by adding this line to /usr/lib/systemd/system/httpd.service
#  in the [Service] section:
#
#  Environment=OPENSSL_ALLOW_PROXY_CERTS=1

import os
import io
import re
import sys
import time
import json
import uuid
import string
import tarfile
import MySQLdb

# wfs/conf.py must define these variables:
#
# mysqlUser     = 'username'
# mysqlPassword = 'PAsSWoRd'
#
import wfs.conf

# Global database connection and ID, usable anywhere
db          = None
cur         = None
allocatorID = None

# Return various strings and SQL expressions which are 
# used in subsequent queries
def makeQueryTerms(jsonDict):

  duneSite = re.sub('["\']', '_', str(jsonDict['dunesite']))
  query = ('SELECT sites_storages.rse_id,location,rse_name,occupancy '
           'FROM sites_storages '
           'LEFT JOIN storages ON storages.rse_id=sites_storages.rse_id '
           'WHERE sites_storages.site_name="%s" '
           'ORDER BY location,occupancy,RAND();' 
           % duneSite)

  cur.execute(query)
  storageRows = cur.fetchall()

  outputRseList   = [] 
  samesiteList   = []
  nearbyList     = []
  accessibleList = []

  for storageRow in storageRows:
    # In future, will put a test for < 1.0 occupancy here
    outputRseList.append(storageRow['rse_name'])
  
    if storageRow['location'] == 'accessible':
      accessibleList.append('replicas.rse_id=%s' % storageRow['rse_id'])
    elif storageRow['location'] == 'nearby':
      nearbyList.append('replicas.rse_id=%s' % storageRow['rse_id'])
    elif storageRow['location'] == 'samesite':
      samesiteList.append('replicas.rse_id=%s' % storageRow['rse_id'])

  storageWhere = ' OR '.join(samesiteList + nearbyList)

  if accessibleList:
    if storageWhere:
      storageWhere += ' OR '

    storageWhere += ('(stages.any_location AND (' + 
                     ' OR '.join(accessibleList) + '))')

  if storageWhere:
    storageWhere = ' AND (' + storageWhere + ') '
 
  if samesiteList:
    storageOrder = '3*(' + ' OR '.join(samesiteList) + ')'
  else:
    storageOrder = ''

  if nearbyList:
    if storageOrder:
      storageOrder += ' + '
  
    storageOrder += '2*(' + ' OR '.join(nearbyList) + ')'

  if accessibleList:
    if storageOrder:
      storageOrder += ' + '

    storageOrder += '1*(' + ' OR '.join(accessibleList) + ')'

  # If we got anything for the storage ordering then complete the
  # expression including the comma; otherwise an empty string
  if storageOrder:
    storageOrder += ' DESC,'

  return { "outputRseList"  : outputRseList,
           "samesiteList"   : samesiteList,
           "nearbyList"     : nearbyList,
           "accessibleList" : accessibleList,
           "storageWhere"   : storageWhere,
           "storageOrder"   : storageOrder,
           "cookie"         : str(uuid.uuid4()),
           "dunesite"       : re.sub('["\']', '_',
                                     str(jsonDict['dunesite'])),
           "rss_bytes"      : int(jsonDict['rss_bytes']),
           "processors"     : int(jsonDict['processors']),
           "wall_seconds"   : int(jsonDict['wall_seconds']),
           "executor_id"    : re.sub('["\']', '_', 
                                     str(jsonDict['executor_id']))
         }

# Just In Time decision making: identify the best request+stage combination
# based on the immediate situation rather than trying to plan ahead
def chooseStageJustInTime(queryTerms):

  query = (
 "SELECT stages.request_id,stages.stage_id,"
 "stages.max_inputs,stages.any_location,"
 "files.file_did,files.file_id,storages.rse_name "
 "FROM files "
 "LEFT JOIN stages ON files.request_id=stages.request_id AND "
 "files.stage_id=stages.stage_id "
 "LEFT JOIN replicas ON files.file_id=replicas.file_id "
 "LEFT JOIN storages ON replicas.rse_id=storages.rse_id "
 "WHERE files.state='unallocated' AND " +
 str(queryTerms["processors"])   + " <= stages.max_processors AND " +
 str(queryTerms["processors"])   + " >= stages.min_processors AND " +
 str(queryTerms["rss_bytes"])    + " >= stages.max_rss_bytes AND " +
 str(queryTerms["wall_seconds"]) + " >= stages.max_wall_seconds " +
 queryTerms["storageWhere"] + " AND storages.rse_name IS NOT NULL "
 "ORDER BY " + 
 queryTerms["storageOrder"] + "files.request_id,files.file_id"
 " LIMIT 100 FOR UPDATE"
 )

  cur.execute(query)
  fileRows = cur.fetchall()
  
  if len(fileRows) == 0:
    return None

  # Take the values from the highest priority result
  requestID   = int(fileRows[0]['request_id'])
  stageID     = int(fileRows[0]['stage_id'])
  maxInputs   = int(fileRows[0]['max_inputs'])
  anyLocation = bool(fileRows[0]['any_location'])
  
  # Start the dictionary to return
  stage = { 'request_id'  : requestID,
            'stage_id'    : stageID,
            'max_inputs'  : maxInputs,
            'any_location': anyLocation }

  # Stage definition does not ask for any input files in the bootstrap
  if maxInputs == 0:
    stage['files'] = None
    return stage
      
  # Need to return input file(s), so gather them from the results
  stage['files'] = []

  # Keep track of DIDs we've added already
  # The same file/DID can appear in multiple rows due to replicas
  addedDids = []
  
  for fileRow in fileRows:
    if (requestID == int(fileRow['request_id']) and
        stageID   == int(fileRow['stage_id']) and
        fileRow['file_did'] not in addedDids):
        
      addedDids.append(fileRow['file_did'])          
      stage['files'].append( { 'file_did': fileRow['file_did'],
                               'rse_name': fileRow['rse_name'],
                               'file_id' : int(fileRow['file_id'])
                             } 
                           )

      # Stop once we have enough inputs
      if len(addedDids) >= maxInputs:
        break

  return stage

# Make a stage dictionary containing stage and one file's information
def makeStageOneFile(jsonDict, queryTerms):

  # Take the values from the highest priority result
  requestID = int(jsonDict['request_id'])
  stageID   = int(jsonDict['stage_id'])

  # Find the any_location flag for this stage from the database
  try:
    query = ("SELECT any_location FROM stages "
             "WHERE request_id=" + str(requestID) + " AND " +
             "stage_id=" + str(stageID))

    cur.execute(query)
    rows=cur.fetchall()
    anyLocation = bool(rows[0]['any_location'])
  except:
    return None

  if anyLocation:
    # If this stage can access data on any accessible storage, then 
    # use all three lists
    storageWhere = ' OR '.join(queryTerms["samesiteList"] + 
                               queryTerms["nearbyList"] + 
                               queryTerms["accessibleList"])
  else:
    # Otherwise just use samesite and nearby lists of storages
    storageWhere = ' OR '.join(queryTerms["samesiteList"] + 
                               queryTerms["nearbyList"])

  if storageWhere:
    storageWhere = ' AND (' + storageWhere + ') '

  query = (
"SELECT files.file_id,files.file_did,storages.rse_name "
"FROM files "
"LEFT JOIN replicas ON files.file_id=replicas.file_id "
"LEFT JOIN storages ON replicas.rse_id=storages.rse_id "
"WHERE files.state='unallocated' AND files.request_id=" +
str(requestID) + " AND files.stage_id=" + str(stageID) + 
storageWhere + " AND storages.rse_NAME IS NOT NULL "
"ORDER BY " + 
queryTerms["storageOrder"] + "files.file_id"
" LIMIT 1 FOR UPDATE"
) 
   
  cur.execute(query)
  fileRows = cur.fetchall()
  
  if len(fileRows) == 0:
    return None

  # Start the dictionary to return
  stage = { 'request_id'  : requestID,
            'stage_id'    : stageID,
            'max_inputs'  : 1,
            'any_location': anyLocation,
            'files'       : [ { 'file_did': fileRows[0]['file_did'],
                                'rse_name': fileRows[0]['rse_name'],
                                'file_id' : int(fileRows[0]['file_id'])
                              }
                            ]
          }
                      
  return stage

# Set the files listed in the stage dictionary to allocated and record
# executor ID etc
def allocateFiles(queryTerms, stage):

  # List to return to executor
  allocatedFiles = []
  fileIdList = []

  for fileDict in stage['files']: 
    # We only include values we will return to the executor
    # So that means we miss out file_id in particular
    # rse_name is a just hint and the executor should use RUCIO to get the
    # best replica and its transfer URL etc to use
    allocatedFiles.append({"file_did": fileDict['file_did'],
                           "rse_name": fileDict['rse_name']
                          }
                         )

    fileIdList.append('file_id=' + str(fileDict['file_id']))
    
  try: 
    query = ("UPDATE files SET state='allocated',"
             "cookie='" + queryTerms['cookie'] + "',"  
             "allocator_id='" + allocatorID + "',"
             "allocated_time=NOW(),"
             "executor_id='" + queryTerms['executor_id'] + "' "
             "WHERE " + " OR ".join(fileIdList)
            )
    cur.execute(query)
  except:
    # If anything goes wrong, we stop straightaway
    return None

  # Return the list of files for this request+stage or an empty list
  return allocatedFiles

# Successfully identified the request+stage and files so get the 
# bootstrap template for this stage and convert it
def makeBootstrap(queryTerms, stage, allocatedFiles):

  try:
    query = ("SELECT bootstrap FROM bootstraps "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    cur.execute(query)
    rows = cur.fetchall()
    bootstrap = rows[0]['bootstrap']
  except:
    return None

  # Transform template to bootstrap by replacing patterns

  if allocatedFiles:
    fileDidStr    = ''
    fileDidRseStr = ''
    for fileDict in allocatedFiles:
      if fileDidStr:
        fileDidStr    += '\n'
        fileDidRseStr += '\n'
    
      fileDidStr    += fileDict['file_did']
      fileDidRseStr += fileDict['file_did'] + ' ' + fileDict['rse_name']
    
    bootstrap = bootstrap.replace('##wfa_files_did##',     fileDidStr)
    bootstrap = bootstrap.replace('##wfa_files_did_rse##', fileDidRseStr)
    bootstrap = bootstrap.replace('##wfa_files_json##',    json.dumps(allocatedFiles))

#  bootstrap = bootstrap.replace('##wfa_cookie##',    queryTerms['cookie'])
  bootstrap = bootstrap.replace('##wfa_request_id##',str(stage['request_id']))
  bootstrap = bootstrap.replace('##wfa_stage_id##',  str(stage['stage_id']))

  # Remove any unused patterns from the template
  bootstrap = re.sub('##wfa_[a-z,0-9,_]*##', '', bootstrap)
  
  return bootstrap

def addFileToTarFile(tar, name, value):

  buffer = io.BytesIO()
  buffer.write(value.encode())

  info       = tarfile.TarInfo(name = name)
  info.size  = buffer.tell()
  info.mtime = time.time()

  buffer.seek(0)
  tar.addfile(tarinfo = info, fileobj = buffer)

# Make an uncompressed tar file to return to the executor in the generic
# job. The generic job 'owns' the files in the tar file, one of which is the
# boostrap.sh script which 'owned' by the user. The generic job has the 
# responsibility to upload the output files matching the patterns defined as
# part of the stage 
def makeTarFile(queryTerms, stage, allocatedFiles):

  buffer = io.BytesIO()
  tar = tarfile.TarFile(fileobj = buffer, mode = "w")

  # Make and add the bootstrap script
  bootstrap = makeBootstrap(queryTerms,
                            stage,
                            allocatedFiles)
  if not bootstrap:
    return None

  addFileToTarFile(tar, "wfa-bootstrap.sh", bootstrap)

  # Create a file containing the output file patterns
  try:
    query = ("SELECT pattern,for_next_stage FROM stages_outputs "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    cur.execute(query)
    rows = cur.fetchall()
  except:
    return None

  patternsFile = ''

  for row in rows:
    patternsFile += str(row['for_next_stage']) + ' ' + row['pattern'] + '\n'
    
  addFileToTarFile(tar, 'wfa-output-patterns.txt', patternsFile)

  # Create a file containing the ordered RSE list from the JIT algorithm
  addFileToTarFile(tar, 'wfa-rse-list.txt', '\n'.join(queryTerms['outputRseList']))

  # Create a file containing the cookie needed to return results to WFA
  addFileToTarFile(tar, 'wfa-cookie.txt', queryTerms['cookie'])

  tar.close()  
  return buffer.getvalue()

# Try to get the stage with the highest priority files still unallocated
def getStageMethod(jsonDict):

  # Check jsonDict contains required string values (eg dunesite)
  for name in ['dunesite', 'executor_id']:
    if name not in jsonDict:
      print('Status: 400 Bad Request')
      print()
      print('Missing value(s) in JSON')
      sys.exit(0)

  # Check jsonDict contains required integer values (eg rss_bytes)
  for name in ['rss_bytes', 'processors', 'wall_seconds']:
    try:
      i = int(jsonDict[name])
    except:
      print('Status: 400 Bad Request')
      print()
      print('Missing integer value(s) in JSON')
      sys.exit(0)

  # Make strings used in SQL queries
  queryTerms = makeQueryTerms(jsonDict)

  # Use the Just In Time decision making: identify the best request+stage 
  # candidate combination at this moment
  stage = chooseStageJustInTime(queryTerms)

  if stage:
    # Make a set of allocated files for this request+stage
    allocatedFiles = allocateFiles(queryTerms, stage)
  
    if allocatedFiles:
      # Send the bootstrap to the executor
      tarFile = makeTarFile(queryTerms, stage, allocatedFiles)

      if tarFile:
        # All done so commit the allocations
        # We do this before the HTTP response in case
        # it is received ok by the executor and run 
        # but times out and fails here on the server side
        db.commit()

        # Return the script to the workflow executor 
        print('Status: 200 OK')
        print('Content-Type: application/x-tar')
        print()
        sys.stdout.flush()
        sys.stdout.buffer.write(tarFile)
        sys.exit(0)

  # No files eligible to be processed by this executor
  print('Status: 404 Not Found')
  print('Content-Type: text/plain')
  print()
  print('No eligible files found')
  sys.exit(0)
   
# Get an unallocated file from the given request+stage
def getFileMethod(jsonDict):

  # Check jsonDict contains required values (eg dunesite)
  for name in ['dunesite', 'executor_id', 'request_id', 'stage_id']:
    if name not in jsonDict:
      print('Status: 400 Bad Request')
      print()
      print('Missing value(s) in JSON')
      sys.exit(0)

  # Make strings used in SQL queries
  queryTerms = makeQueryTerms(jsonDict)

  # Create a stage dictionary with the next file in this stage  
  stage = makeStageOneFile(jsonDict, queryTerms)
  
  if stage:
    allocatedFiles = allocateFiles(queryTerms, stage)

    if allocatedFiles:
      # All done so commit the allocations
      # We do this before the HTTP response in case
      # it is received ok by the executor and processed
      # but times out and fails here on the server side
      db.commit()

      # Now tell the executor what file to process
      print('Status: 200 OK')
      print('Content-Type: text/plain')
      print()
      print(queryTerms['cookie'] + ' ' + allocatedFiles[0]['file_did'])

      sys.exit(0)

  # No file eligible to be processed by this executor
  print('Status: 404 Not Found')
  print('Content-Type: text/plain')
  print()
  print('No eligible file found')
  sys.exit(0)   

def updateFileProcessing(fileList, state, requestID, stageID, 
                         cookie, executorID):
  processedList   = []

  try:
    for fileDid in fileList:
      if '"' in fileDid or "'" in fileDid:
        continue

      processedList.append('file_did="' + str(fileDid) + '"')
  except:
    return

  # Update the files the executor did or did not manage to process
  if processedList:
    try:
      query = ('UPDATE files SET state="' + state + '" '
               'WHERE (' + ' OR '.join(processedList) + ') '
               'AND request_id=' + str(requestID) + ' ' 
               'AND stage_id=' + str(stageID) + ' ' 
               'AND cookie="' + cookie + '" AND state="allocated"')
      cur.execute(query)
    except:
      return
    
# Deal with the output files needed by the next stage
def processNextStageOutputs(jsonDict, requestID, stageID, executorID):

  try:
    for outputDid in jsonDict['next_stage_outputs']:
      
      if '"' in outputDid or "'" in outputDid:
        continue

      try:
        query = ('INSERT INTO files SET state="find_replicas",'
                 'file_did="'  + str(outputDid) + '",'
                 'request_id=' + str(requestID) + ','
                 'stage_id='   + str(stageID + 1)
                )
        cur.execute(query)
      except:
        # Just do our best if anything goes wrong for now
        # Should decide what to do in these partial failure cases
        pass

  except:
    return
                
def resultsMethod(jsonDict):

  # Check jsonDict contains required values (eg executor_id)
  for name in ['cookie', 'executor_id', 'request_id', 'stage_id', 
               'processed_inputs', 'unprocessed_inputs', 'next_stage_outputs']:
    if name not in jsonDict:
      print('Status: 400 Bad Request')
      print()
      print('Missing value (%s) in JSON' % name)
      sys.exit(0)

  try:
    requestID = int(jsonDict['request_id'])
    stageID   = int(jsonDict['stage_id'])
  except:
    print('Status: 400 Bad Request')
    print()
    print('Missing value (%s) in JSON')
    sys.exit(0)

  cookie = re.sub('[^a-z,A-Z,0-9,.,_,-]', '', str(jsonDict['cookie']))
  
  # Make a sanitised string which identifies the executor
  executorID = re.sub('["\']', '_', str(jsonDict['executor_id']).lower())

  updateFileProcessing(jsonDict['processed_inputs'], 'processed',
                       requestID, stageID, cookie, executorID)

  updateFileProcessing(jsonDict['unprocessed_inputs'], 'unprocessed',
                       requestID, stageID, cookie, executorID)

  processNextStageOutputs(jsonDict, requestID, stageID, executorID)

  # Commit everything, before we return OK to the executor
  db.commit()

  # Return OK to the workflow executor
  print('Status: 200 OK')
  print('Content-Type: text/plain')
  print()
  sys.exit(0)

#
# PROGRAM MAIN
#

# Quickly reject random GETs etc (if not hanlded by Apache already)
if os.environ['REQUEST_METHOD'] != 'POST':
    print('Status: 405 Method not allowed')
    print()
    print('We only support POST')
    sys.exit(0)

# Create a unique ID string for this instance that may also help in debugging
allocatorID = "%s:%d:%f" % (os.uname()[1], os.getpid(), time.time())

# Use certificates and proxies for now; will transition to tokens in future
if 'SSL_CLIENT_S_DN' not in os.environ or not os.environ['SSL_CLIENT_S_DN']:
    print('Status: 403 Forbidden')
    print()
    print('Forbidden - identity not provided')
    sys.exit(0)

# Convert to the older format with slashes 
clientDN='/'+'/'.join(os.environ['SSL_CLIENT_S_DN'].split(',')[::-1])

# In production this will be the DN of the Generic Job Factory
factoryDN = '/DC=org/DC=cilogon/C=US/O=Fermi National Accelerator Laboratory/OU=People/CN=Andrew McNab/CN=UID:amcnab'
if (clientDN != factoryDN and 
    not clientDN.startswith(factoryDN + '/CN=')):
    # Unless we found a matching DN then refuse
    print('Status: 403 Forbidden')
    print()
    print('Forbidden - acceptable identity not provided')
    sys.exit(0)

# Get the JSON document POSTed to us
try:
  jsonDict = json.load(sys.stdin)
except:
  print('Status: 400 Bad Request')
  print()
  print('Failed to parse JSON')
  sys.exit(0)

# Check jsonDict specifies a method
if 'method' not in jsonDict:
  print('Status: 400 Bad Request')
  print()
  print('Missing method in JSON')
  sys.exit(0)

# Do as many checks as we can before connecting to the database here
try:
  db  = MySQLdb.connect(host="localhost", user=wfs.conf.mysqlUser, 
                        passwd=wfs.conf.mysqlPassword, db='wfdb')
  db.autocommit(False)
  cur = db.cursor(MySQLdb.cursors.DictCursor) 
except:
  print('Status: 500 Internal Server Error')
  print()
  print('Problem with database connection')
  sys.exit(0)

# Get the stage and files to process
if jsonDict['method'] == 'get_stage':
  getStageMethod(jsonDict)

# Get one or more files to process  
elif jsonDict['method'] == 'get_file':
  getFileMethod(jsonDict)
  
# Return results of processing files
elif jsonDict['method'] == 'return_results':
  resultsMethod(jsonDict)

else:
  print('Status: 400 Bad Request')
  print()
  print('Method in JSON not recognised')
  sys.exit(0)

