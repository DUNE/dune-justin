#!/usr/bin/python3
#
#  wfa-cgi - Workflow Allocator CGI service
#
#  Andrew McNab, University of Manchester.
#  Copyright (c) 2013-22. All rights reserved.
#
#  Redistribution and use in source and binary forms, with or
#  without modification, are permitted provided that the following
#  conditions are met:
#
#    o Redistributions of source code must retain the above
#      copyright notice, this list of conditions and the following
#      disclaimer. 
#    o Redistributions in binary form must reproduce the above
#      copyright notice, this list of conditions and the following
#      disclaimer in the documentation and/or other materials
#      provided with the distribution. 
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
#  CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
#  INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
#  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
#  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
#  BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
#  ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
#  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
#  POSSIBILITY OF SUCH DAMAGE.
#

#  This CGI script must be run from an Apache httpd server with
#  X.509 proxy certificates enabled. On a systemd system (like 
#  CentOS 7) you need to enable this in OpenSSL inside mod_ssl
#  by adding this line to /usr/lib/systemd/system/httpd.service
#  in the [Service] section:
#
#  Environment=OPENSSL_ALLOW_PROXY_CERTS=1

import os
import io
import re
import sys
import time
import json
import uuid
import string
import tarfile
import MySQLdb

# wfs/conf.py must define these variables in a way that is both
# valid Python and valid Bash!
#
# mysqlUser='username'
# mysqlPassword='PAsSWoRd'
#
import wfs

# Successfully identified the request+stage and files so get the 
# bootstrap for this stage and return it
def getBootstrap(queryTerms, stage):

  try:
    query = ("SELECT bootstrap FROM bootstraps "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    wfs.db.cur.execute(query)
    rows = wfs.db.cur.fetchall()
    bootstrap = rows[0]['bootstrap']
  except:
    return None
  
  return bootstrap

def addFileToTarFile(tar, name, value):

  buffer = io.BytesIO()
  buffer.write(value.encode())

  info       = tarfile.TarInfo(name = name)
  info.size  = buffer.tell()
  info.mtime = time.time()

  buffer.seek(0)
  tar.addfile(tarinfo = info, fileobj = buffer)

# Make an uncompressed tar file to return to the generic job. 
# The generic job 'owns' the files in the tar file, one of which is the
# boostrap.sh script which 'owned' by the user. The generic job has the 
# responsibility to upload the output files matching the patterns defined as
# part of the stage 
def makeTarFile(queryTerms, stage, wfsJobID, cookie):

  buffer = io.BytesIO()
  tar = tarfile.TarFile(fileobj = buffer, mode = "w")

  # Get and add the bootstrap script
  bootstrap = getBootstrap(queryTerms, stage)
  if not bootstrap:
    return None

  addFileToTarFile(tar, "wfs-bootstrap.sh", bootstrap)

  # Create a file containing the output file patterns
  try:
    query = ("SELECT pattern,for_next_stage FROM stages_outputs "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    wfs.db.cur.execute(query)
    rows = wfs.db.cur.fetchall()
  except:
    return None

  patternsFile = ''

  for row in rows:
    patternsFile += str(row['for_next_stage']) + ' ' + row['pattern'] + '\n'
    
  addFileToTarFile(tar, 'wfs-output-patterns.txt', patternsFile)

  addFileToTarFile(tar, 'wfs-env.sh', 
       (
         'export WFS_JOB_ID=' + str(wfsJobID) + '\n' +
         'export WFS_REQUEST_ID=' + str(stage['request_id']) + '\n' +
         'export WFS_STAGE_ID=' + str(stage['stage_id']) + '\n' +
         'export WFS_COOKIE="' + cookie + '"\n' +
         'export WFS_RSE_LIST="' + ' '.join(queryTerms['outputRseList']) + '"\n'
       )
                  )

  addFileToTarFile(tar, 'wfs-get-file.json', 
       (
         '{\n' +
         '"method" : "get_file",\n' +
         '"wfs_job_id" : ' + str(wfsJobID) + ',\n' +
         '"cookie" : "' + cookie + '"\n' 
         '}\n'
       )
                  )

  tar.close()  
  return buffer.getvalue()

# Try to get the stage with the highest priority files still unallocated
def getStageMethod(jsonDict, jobUserID):

  job = {}

  # Check jsonDict contains required string values (eg site_name)
  for name in ['jobsub_id', 'site_name', 'cpuinfo', 'os_release', 'hostname']:
    if name not in jsonDict:
      print('Status: 400 Bad Request')
      print()
      print('Missing value(s) in JSON')
      sys.exit(0)
    else:
      job[name] = str(jsonDict[name])

  # Check jsonDict contains required integer values (eg rss_bytes)
  for name in ['rss_bytes', 'processors', 'wall_seconds']:
    try:
      job[name] = int(jsonDict[name])
    except:
      print('Status: 400 Bad Request')
      print()
      print('Missing integer value(s) in JSON')
      sys.exit(0)

  # Make strings used in SQL queries
  queryTerms = wfs.allocator.makeQueryTerms(jsonDict, job)

  # Use the Just In Time decision making: identify the best request+stage 
  # candidate combination at this moment
  stage = wfs.allocator.findStage(queryTerms, job)

  if not stage:
    # No stages/files eligible to be processed by this job
    print('Status: 404 Not Found')
    print('Content-Type: text/plain')
    print()
    print('No eligible stages found')
    sys.exit(0)

  cookie = str(uuid.uuid4())

  try:
    query = ('INSERT INTO wfs_jobs SET '
             'allocated_time=NOW(),' 
             'allocator_name="' + allocatorName + '",' 
             'jobsub_id="' + job['jobsub_id'] + '",'              
             'request_id=' + str(stage['request_id']) + ','
             'stage_id=' + str(stage['stage_id']) + ','
             'cpuinfo="' + job['cpuinfo'] + '",' 
             'os_release="' + job['os_release'] + '",' 
             'hostname="' + job['hostname'] + '",' 
             'rss_bytes=' + str(job['rss_bytes']) + ',' 
             'processors=' + str(job['processors']) + ',' 
             'wall_seconds=' + str(job['wall_seconds']) + ',' 
             'job_user_id=' + str(jobUserID) + ',' 
             'cookie="' + cookie + '",' 
             'site_id=(SELECT site_id FROM sites ' 
                      'WHERE site_name="' + job['site_name'] + "),'
             'submission_id=(SELECT submission_id FROM jobsub_jobs ' 
                      'WHERE jobsub_id="' + job['jobsub_id'] + ")'
            )
    wfs.db.cur.execute(query)
  except:
    print('Status: 500 Internal Server Error')
    print('Content-Type: text/plain')
    print()
    print('Workflow allocator failed')
    sys.exit(0)

  wfsJobID = wfs.db.cur.lastrowid

  try:
    query = ('UPDATE jobsub_jobs SET allocated_time=NOW() '
             'WHERE jobsub_id="' + job['jobsub_id'] + '"')
             
    wfs.db.cur.execute(query)
    
  except Exception as e:
    # We log this but carry on - someone testing?
    logLine('Update of jobsub_jobs from ' + job['jobsub_id'] + ' fails: '
            + str(e))

  try:
    query = ('UPDATE entries SET last_allocated_time=NOW() '
             'WHERE entry_id=(SELECT entry_id FROM jobsub_jobs '
             'WHERE jobsub_id="' + job['jobsub_id'] + '")')
             
    wfs.db.cur.execute(query)
    
  except Exception as e:
    # We log this but carry on - someone testing?
    logLine('Update of entries from ' + job['jobsub_id'] + ' fails: '
            + str(e))

  tarFile = makeTarFile(queryTerms, stage, wfsJobID, cookie)
        
  # All done so commit the job details.
  # We do this before the HTTP response in case
  # it is received ok by the job and run 
  # but times out and fails here on the server side
  wfs.db.conn.commit()

  # Return the script to the workflow job 
  print('Status: 200 OK')
  print('Content-Type: application/x-tar')
  print()
  sys.stdout.flush()
  sys.stdout.buffer.write(tarFile)
  sys.exit(0)
   
# Get an unallocated file from the given request+stage
def getFileMethod(jsonDict):

  # Find details of this job
  job = wfs.allocator.findJob(jsonDict)

  if not job:
    print('Status: 400 Bad Request')
    print()
    print('No matching job found')
    sys.exit(0)

  # Make strings used in SQL queries
  queryTerms = wfs.allocator.makeQueryTerms(jsonDict, job)

  # Create a stage dictionary with the next file in this stage  
  oneFile = wfs.allocator.findOneFile(jsonDict, queryTerms, job)
  
  if oneFile:
    if job['state'] == 'started':
      try:
        query = ('UPDATE wfs_jobs SET state="processing" ' +
                 'WHERE wfs_job_id=' + str(job['wfs_job_id'])
                )
        wfs.db.cur.execute(query)
      except:
        print('Status: 500 Internal Server Error')
        print('Content-Type: text/plain')
        print()
        print('Unable to update job to processing')
        sys.exit(0)

    # All done so commit the allocation and updates
    # We do this before the HTTP response in case
    # it is received ok by the job and processed
    # but times out and fails here on the server side
    wfs.db.conn.commit()

    # Now tell the job what file to process and where it is
    print('Status: 200 OK')
    print('Content-Type: text/plain')
    print()
    print(oneFile['file_did'] +  ' ' + 
          oneFile['pfn'] + ' ' + 
          oneFile['rse_name'])

    sys.exit(0)

  # No file eligible to be processed by this job
  print('Status: 404 Not Found')
  print()
  print('No eligible file found')
  sys.exit(0)   

def updateFileProcessing(fileList, state, job):

  processedList = []

  try:
    for fileDid in fileList:
      if '"' in fileDid or "'" in fileDid:
        continue

      processedList.append('files.file_did="' + str(fileDid) + '"')
  except:
    return

  # Update the files the job did or did not manage to process
  if processedList:
    try:
      query = ('UPDATE files SET state="' + state + '" '
               'WHERE (' + ' OR '.join(processedList) + ') '
               'AND request_id=' + str(job['request_id']) + ' ' 
               'AND stage_id=' + str(job['stage_id']) + ' ' 
               'AND state="allocated"')
      wfs.db.cur.execute(query)
    except:
      pass

    if state == 'processed':
      try:
        query = ('UPDATE allocations SET processed=TRUE,'
                 'processed_time=NOW() '
                 'WHERE wfs_job_id=%s AND '
                 'allocations.file_id IN (SELECT files.file_id FROM files '
                 'WHERE files.request_id=%s AND files.stage_id=%s '
                 'AND (%s)) '
                 'ORDER BY allocation_id DESC LIMIT 1'
                 % (str(job['wfs_job_id']), 
                    str(job['request_id']),
                    str(job['stage_id']),
                    ' OR '.join(processedList)))

        wfs.db.cur.execute(query)
      except:
        pass
    
# Deal with the output files needed by the next stage
def processNextStageOutputs(jsonDict, job):

  try:
    for outputDid in jsonDict['next_stage_outputs']:
      
      if '"' in outputDid or "'" in outputDid:
        continue

      try:
        query = ('INSERT INTO files SET state="find_replicas",'
                 'file_did="'  + str(outputDid) + '",'
                 'request_id=' + str(job['request_id']) + ','
                 'stage_id='   + str(job['stage_id'] + 1)
                )
        wfs.db.cur.execute(query)
      except:
        # Just do our best if anything goes wrong for now
        # Should decide what to do in these partial failure cases
        pass

  except:
    return
                
# Update the counters used for quick listing of stages
def updateStageCounts(job):

  try:
    # Use a brute force recount of everything for this 
    # stage rather than try to use increments
    query = ('UPDATE stages SET '
             'num_finding=(SELECT COUNT(*) FROM files'
             ' WHERE state="finding" AND request_id=%d AND stage_id=%d),'
             'num_unallocated=(SELECT COUNT(*) FROM files'
             ' WHERE state="unallocated" AND request_id=%d AND stage_id=%d),'
             'num_allocated=(SELECT COUNT(*) FROM files'
             ' WHERE state="allocated" AND request_id=%d AND stage_id=%d),'
             'num_processed=(SELECT COUNT(*) FROM files'
             ' WHERE state="processed" AND request_id=%d AND stage_id=%d) '
             'WHERE request_id=%d AND stage_id=%d' % 
             (job['request_id'], job['stage_id'],
              job['request_id'], job['stage_id'],
              job['request_id'], job['stage_id'],
              job['request_id'], job['stage_id'],
              job['request_id'], job['stage_id']))
            )
    wfs.db.cur.execute(query)
  except:
    pass
                
def resultsMethod(jsonDict):

  # Check jsonDict contains required values (eg cookie)
  for name in ['cookie', 'processed_inputs', 
               'unprocessed_inputs', 'next_stage_outputs']:
    if name not in jsonDict:
      print('Status: 400 Bad Request')
      print()
      print('Missing value (%s) in JSON' % name)
      sys.exit(0)

  job = findJob(jsonDict)
  
  if not job:
    print('Status: 400 Bad Request')
    print()
    print('No matching job')
    sys.exit(0)

  updateFileProcessing(jsonDict['processed_inputs'], 'processed', job)

  updateFileProcessing(jsonDict['unprocessed_inputs'], 'unprocessed', job)

  processNextStageOutputs(jsonDict, job)
  
  updateStageCounts(job)

  try:
    query = ('UPDATE wfs_jobs SET state="finished",finished_time=NOW() ' +
             'WHERE wfs_job_id=' + str(job['wfs_job_id'])
            )
    wfs.db.cur.execute(query)
  except:
    # Don't give up! Record what we can!
    pass

  # Commit everything, before we return OK to the job
  wfs.db.conn.commit()

  # Return OK to the workflow job
  print('Status: 200 OK')
  print()
  sys.exit(0)

def getJobUserID():

  # Use certificates and proxies for now; will transition to tokens in future
  if 'SSL_CLIENT_S_DN' not in os.environ or not os.environ['SSL_CLIENT_S_DN']:
    print('Status: 403 Forbidden')
    print()
    print('Forbidden - identity not provided')
    sys.exit(0)

  # Convert to the older format with slashes 
  clientDN='/'+'/'.join(os.environ['SSL_CLIENT_S_DN'].split(',')[::-1])

  query = ('SELECT user_id FROM users '
           'WHERE x509dn=LEFT("%s",LENGTH(x509dn))' % 
           clientDN.replace('\\','\\\\').replace('"','\\"'))

  try:
    wfs.db.cur.execute(query)
    rows = wfs.db.cur.fetchall()
  except:
    print('Status: 500 Internal Server Error')
    print()
    print('Error reading database')
    sys.exit(0)

  try:
    jobUserID = int(rows[0]['user_id'])
  except:
    print('Status: 403 Forbidden')
    print()
    print('Forbidden - authorized identity not provided')
    sys.exit(0)

  return jobUserID

#
# PROGRAM MAIN
#

# Quickly reject random GETs etc (if not handled by Apache already)
if os.environ['REQUEST_METHOD'] != 'POST':
    print('Status: 405 Method not allowed')
    print()
    print('We only support POST')
    sys.exit(0)

# Create a unique ID string for this instance that may also help in debugging
allocatorName = "%s:%d:%f" % (os.uname()[1], os.getpid(), time.time())

# Get the JSON document POSTed to us
try:
  jsonDict = json.load(sys.stdin)
except:
  print('Status: 400 Bad Request')
  print()
  print('Failed to parse JSON')
  sys.exit(0)

# Check jsonDict specifies a method
if 'method' not in jsonDict:
  print('Status: 400 Bad Request')
  print()
  print('Missing method in JSON')
  sys.exit(0)

# Do as many checks as we can before connecting to the database here
try:
  wfs.db.conn = MySQLdb.connect(host="localhost", user=wfs.conf.mysqlUser, 
                                passwd=wfs.conf.mysqlPassword, db='wfdb')
  wfs.db.conn.autocommit(False)
  wfs.db.cur = wfs.db.conn.cursor(MySQLdb.cursors.DictCursor) 
except:
  print('Status: 500 Internal Server Error')
  print()
  print('Problem with database connection')
  sys.exit(0)

# Get the stage for the job to work on
if jsonDict['method'] == 'get_stage':
  # getJobUserID() fails with an HTTP error and exit if authorized ID not given
  jobUserID = getJobUserID()

  getStageMethod(jsonDict, jobUserID)

# Get one or more files to process  
elif jsonDict['method'] == 'get_file':
  getFileMethod(jsonDict)
  
# Return results of processing files
elif jsonDict['method'] == 'return_results':
  resultsMethod(jsonDict)

else:
  print('Status: 400 Bad Request')
  print()
  print('Method in JSON not recognised')
  sys.exit(0)

