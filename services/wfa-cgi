#!/usr/bin/python3
#
#  wfa-cgi - Workflow Allocator CGI service
#
#  Andrew McNab, University of Manchester.
#  Copyright (c) 2013-22. All rights reserved.
#
#  Redistribution and use in source and binary forms, with or
#  without modification, are permitted provided that the following
#  conditions are met:
#
#    o Redistributions of source code must retain the above
#      copyright notice, this list of conditions and the following
#      disclaimer. 
#    o Redistributions in binary form must reproduce the above
#      copyright notice, this list of conditions and the following
#      disclaimer in the documentation and/or other materials
#      provided with the distribution. 
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
#  CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
#  INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
#  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
#  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
#  BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
#  ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
#  OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
#  POSSIBILITY OF SUCH DAMAGE.
#

#  This CGI script must be run from an Apache httpd server with
#  X.509 proxy certificates enabled. On a systemd system (like 
#  CentOS 7) you need to enable this in OpenSSL inside mod_ssl
#  by adding this line to /usr/lib/systemd/system/httpd.service
#  in the [Service] section:
#
#  Environment=OPENSSL_ALLOW_PROXY_CERTS=1

import os
import io
import re
import sys
import time
import json
import uuid
import base64
import string
import tarfile
import MySQLdb

# wfs/conf.py must define these variables in a way that is both
# valid Python and valid Bash!
#
# mysqlUser='username'
# mysqlPassword='PAsSWoRd'
#
import wfs

def httpError(code, message = None):
  if code[:3] != '200':
    print('WFA fails with %s (%s)' % (code, str(message)), file=sys.stderr)
    
  print('Status: ' + code)
  print('Content-Type: text/plain')
  print()
  if message:
    print(message)
  sys.exit(0)

# Successfully identified the request+stage and files so get the 
# bootstrap for this stage and return it
def getBootstrap(stage):

  try:
    query = ("SELECT bootstrap FROM bootstraps "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    wfs.db.cur.execute(query)
    row = wfs.db.cur.fetchone()
    bootstrap = row['bootstrap']
  except:
    return None
  
  return bootstrap

def addFileToTarFile(tar, name, value):

  buffer = io.BytesIO()
  buffer.write(value.encode())

  info       = tarfile.TarInfo(name = name)
  info.size  = buffer.tell()
  info.mtime = time.time()

  buffer.seek(0)
  tar.addfile(tarinfo = info, fileobj = buffer)

# Make an uncompressed tar file to return to the generic job. 
# The generic job 'owns' the files in the tar file, one of which is the
# boostrap.sh script which 'owned' by the user. The generic job has the 
# responsibility to upload the output files matching the patterns defined as
# part of the stage 
def makeTarFile(jsonDict, jobDict, stage, cookie):

  buffer = io.BytesIO()
  tar = tarfile.TarFile(fileobj = buffer, mode = "w")

  # Get and add the bootstrap script
  bootstrap = getBootstrap(stage)
  if not bootstrap:
    return None

  addFileToTarFile(tar, "wfs-bootstrap.sh", bootstrap)

  # Create a file containing the output file patterns
  try:
    query = ("SELECT dataset,file_scope,file_pattern,for_next_stage "
             "FROM stages_outputs "
             "WHERE request_id=%d AND stage_id=%d" 
             % (stage['request_id'], stage['stage_id']))

    wfs.db.cur.execute(query)
    rows = wfs.db.cur.fetchall()
  except:
    return None

  patternsFile = ''

  for row in rows:
    patternsFile += (str(row['for_next_stage']) + ' ' + 
                     row['dataset'] + ' ' +
                     row['file_scope'] + ' ' +
                     row['file_pattern'] + '\n')
    
  addFileToTarFile(tar, 'wfs-output-patterns.txt', patternsFile)

  # Create an ordered output RSE list specific to this stage
  #
  # When the occupancy figures are usable this needs to have 
  #  'storages.occupancy < 1.0 AND ' added to WHERE and 
  #  storages.occupancy added to ORDER BY
  try:
    query = ('SELECT rse_name,deterministic_rse,pfn_prefix '
             'FROM storages '
             'LEFT JOIN stages_output_storages '
             'ON (storages.rse_id=stages_output_storages.rse_id '
             'AND stages_output_storages.request_id=%d '
             'AND stages_output_storages.stage_id=%d) '
             'LEFT JOIN sites_storages '
             'ON (storages.rse_id=sites_storages.rse_id '
             'AND sites_storages.site_id=%d) '
             'WHERE (use_for_output OR '
             'stages_output_storages.rse_id IS NOT NULL) AND '
             'sites_storages.distance IS NOT NULL AND '
             'storages.rse_write '
             'ORDER BY (stages_output_storages.rse_id IS NULL),'
             'distance,RAND()' 
             % (stage['request_id'], 
                stage['stage_id'],
                jobDict['site_id']                
               )
            )

    wfs.db.cur.execute(query)
    rseRows = wfs.db.cur.fetchall()
  except:
    return None

  outputRseFile = ''
  for rseRow in rseRows:
    if rseRow['deterministic_rse']:
      outputRseFile += rseRow['rse_name'] + ' \n' 
    else:
      outputRseFile += rseRow['rse_name'] + ' ' + rseRow['pfn_prefix'] + '\n'
      
  addFileToTarFile(tar, 'wfs-output-rse-list.txt', outputRseFile)
                  
  addFileToTarFile(tar, 'wfs-env.sh', 
       (
         'export WFS_SITE_NAME=' + str(jobDict['site_name']) + '\n' +
         'export WFS_REQUEST_ID=' + str(stage['request_id']) + '\n' +
         'export WFS_STAGE_ID=' + str(stage['stage_id']) + '\n' +
         'export WFS_COOKIE="' + cookie + '"\n' 
       )  
                  )
  addFileToTarFile(tar, 'wfs-get-file.json', 
       (
         '{\n' +
         '"method" : "get_file",\n' +
         '"jobsub_id" : "' + str(jsonDict['jobsub_id']) + '",\n' +
         '"cookie" : "' + cookie + '"\n' 
         '}\n'
       )
                  )
                  
  tar.close()  
  return buffer.getvalue()

# Try to get the stage with the highest priority files still unallocated
def getStageMethod(jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not wfs.db.stringIsJobsubID(jsonDict['jobsub_id']):
    httpError('400 Bad Request', 'Missing jobsub_id in JSON')

  # Check jsonDict contains required integer values (eg rss_bytes)
  for name in ['rss_bytes', 'processors', 'wall_seconds']:  
    try:
      n = int(jsonDict[name])
    except:
      httpError('400 Bad Request', 'Missing/invalid integer value(s) in JSON')

  # Check jsonDict contains required string values
  for name in ['cpuinfo', 'os_release', 'hostname']:
    if name not in jsonDict or not wfs.db.stringNoQuotes(jsonDict[name]):
      httpError('400 Bad Request', 'Missing/invalid value(s) in JSON')

  # Find details of the job
  jobDict = wfs.allocator.makeJobDict(jsonDict['jobsub_id'])

  if jobDict['error_message']:
    httpError('400 Bad Request', jobDict['error_message'])

  #Â Soem kind of mismatch. Maybe the idle job was marked as stalled.
  if jobDict['allocation_state'] != 'submitted':
    httpError('410 Gone', 'Job now unavailable for allocation')

  try:
    query = ('UPDATE sites SET last_get_stage_time=NOW() '
             'WHERE site_id=%d' % jobDict['site_id'])
             
    wfs.db.cur.execute(query)
  except:
    httpError('500 Internal Server Error',
              'Workflow allocator fails to record get_stage time')

  # Use the Just In Time decision making: identify the best request+stage 
  # candidate combination at this moment
  try:
    stage = wfs.allocator.findStage(jobDict, forUpdate = True)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Workflow allocator fails finding stage: ' + str(e))

  if not stage:
    # No stages/files eligible to be processed by this job
    try:
      query = ('UPDATE slot_sizes SET last_no_match_time=NOW() '
               'WHERE slot_size_id=' + str(jobDict['slot_size_id']))
             
      wfs.db.cur.execute(query)    
      wfs.db.conn.commit()
    except Exception as e:
      pass

    httpError('404 Not Found', 'No eligible stages found')

  cookie = str(uuid.uuid4())

  try:
    query = ('UPDATE jobs SET '
             'allocation_time=NOW(),' 
             'heartbeat_time=NOW(),' 
             'allocation_state="started",'
             'allocator_name="' + allocatorName + '",' 
             'request_id=' + str(stage['request_id']) + ','
             'stage_id=' + str(stage['stage_id']) + ','
             'cpuinfo="' + str(jsonDict['cpuinfo']) + '",' 
             'os_release="' + str(jsonDict['os_release']) + '",' 
             'hostname="' + str(jsonDict['hostname']) + '",' 
             'rss_bytes=' + str(jsonDict['rss_bytes']) + ',' 
             'processors=' + str(jsonDict['processors']) + ',' 
             'wall_seconds=' + str(jsonDict['wall_seconds']) + ',' 
             'cookie="' + cookie + '" ' 
             'WHERE jobsub_id="' + str(jsonDict['jobsub_id']) + '"'
            )
    wfs.db.cur.execute(query)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Workflow allocator failed: ' + str(e))

  try:
    query = ('UPDATE slot_sizes SET last_allocation_time=NOW() '
             'WHERE slot_size_id=' + str(jobDict['slot_size_id']))
             
    wfs.db.cur.execute(query)
    
  except Exception as e:
    # We log this but carry on - someone testing?
    httpError('500 Internal Server Error',
              'Workflow allocator failed: ' + str(e))

  tarFile = makeTarFile(jsonDict, jobDict, stage, cookie)
  
  if not tarFile:
    httpError('500 Internal Server Error', 'Failed to create tar file')
        
  wfs.allocator.updateStageCounts(stage['request_id'], 
                                  stage['stage_id'])
 
  # All done so commit the job details.
  # We do this before the HTTP response in case
  # it is received ok by the job and run 
  # but times out and fails here on the server side
  try:
    wfs.db.conn.commit()
  except Exception as e:
    httpError('500 Internal Server Error',
              'Workflow allocator get_stage commit failed: ' + str(e))
  
  # Return the script to the workflow job 
  print('Status: 200 OK')
  print('Content-Type: application/x-tar')
  print()
  sys.stdout.flush()
  sys.stdout.buffer.write(tarFile)
  sys.exit(0)
   
# Record heartbeats from generic jobs
def sendHeartbeatMethod(jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not wfs.db.stringIsJobsubID(jsonDict['jobsub_id']):
    httpError('400 Bad Request', 'Missing jobsub_id in JSON')

  try:
    query = ('UPDATE jobs SET heartbeat_time=NOW() ' 
             'WHERE jobsub_id="' + str(jsonDict['jobsub_id']) + '"'
            )
    wfs.db.cur.execute(query)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Workflow allocator failed: ' + str(e))

  # All done so commit the job details.
  try:
    wfs.db.conn.commit()
  except Exception as e:
    httpError('500 Internal Server Error',
              'Workflow allocator send_heartbeat commit failed: ' + str(e))

  print('Status: 200 OK')
  print()
  sys.exit(0)
   
# Job aborted 
def jobAbortedMethod(jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not wfs.db.stringIsJobsubID(jsonDict['jobsub_id']):
    httpError('400 Bad Request', 'Missing jobsub_id in JSON')

  try:
    httpCode = int(jsonDict['http_code'])
  except:
    httpError('400 Bad Request', 'Missing/invalid http_code in JSON')

  if 'aborted_method' not in jsonDict or \
     not wfs.db.stringNoQuotes(jsonDict['aborted_method']):
    httpError('400 Bad Request', 'Missing/invalid aborted_method in JSON')

  if httpCode == 404 and jsonDict['aborted_method'] == 'get_stage':
    state = 'notused'
  else:
    state = 'aborted'
 
  try:
    query = ('UPDATE jobs SET heartbeat_time=NOW(),allocation_state="%s",'
             'finished_time=NOW() '
             'WHERE jobsub_id="%s"' %
             (state, jsonDict['jobsub_id']))
    wfs.db.cur.execute(query)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Workflow allocator job_aborted failed: ' + str(e))

  # All done so commit the job details.
  try:
    wfs.db.conn.commit()
  except Exception as e:
    httpError('500 Internal Server Error',
              'Workflow allocator job_aborted commit failed: ' + str(e))

  print('Status: 200 OK')
  print()
  sys.exit(0)
   
# Get an unallocated file from the given request+stage
def getFileMethod(jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not wfs.db.stringIsJobsubID(jsonDict['jobsub_id']):
    httpError('400 Bad Request', 'Missing/invalid jobsub_id in JSON')

  if 'cookie' not in jsonDict or \
     not wfs.db.stringNoQuotes(jsonDict['cookie']):
    httpError('400 Bad Request', 'Missing/invalid cookie in JSON')

  # Lookup job details
  jobDict = wfs.allocator.makeJobDict(jsonDict['jobsub_id'],
                                      jsonDict['cookie'])
                                          
  if jobDict['error_message']:
    httpError('400 Bad Request', jobDict['error_message'])

  if jobDict['allocation_state'] != 'started' and \
     jobDict['allocation_state'] != 'processing':
    httpError('403 Forbidden', 
         'Job in wrong state to find file (%s)' % jobDict['allocation_state'])
    
  #Â Create a stage dictionary with the next file in this stage  
  oneFile = wfs.allocator.findFile(jobDict)

  if oneFile['error_message']:
    httpError('500 Internal Server Error', 
              'Failed finding one file: ' + oneFile['error_message'])
  
  if oneFile['file_did']:
    if jobDict['allocation_state'] == 'started':
      try:
        query = ('UPDATE jobs SET allocation_state="processing" '
                 'WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"'
                )
        wfs.db.cur.execute(query)

        wfs.allocator.updateStageCounts(jobDict['request_id'], 
                                        jobDict['stage_id'])

      except Exception as e:
        httpError('500 Internal Server Error',
                  'Unable to update job to processing: ' + str(e))


    # All done so commit the allocation and updates
    # We do this before the HTTP response in case
    # it is received ok by the job and processed
    # but times out and fails here on the server side
    try:
      wfs.db.conn.commit()
    except Exception as e:
      httpError('500 Internal Server Error',
                'Workflow allocator get_file commit failed: ' + str(e))

    # Now tell the job what file to process and where it is
    print('Status: 200 OK')
    print('Content-Type: text/plain')
    print()
    print(oneFile['file_did'] +  ' ' + 
          oneFile['pfn'] + ' ' + 
          oneFile['rse_name'])

    sys.exit(0)

  # No file eligible to be processed by this job
  httpError('404 Not Found', 'No eligible file found')

def updateFileProcessing(fileIDList,
                         state, wfsJobID, requestID, stageID):
  # Exceptions in this function must by handled by the caller!

  testFileIDs = []

  for fileID in fileIDList:
    testFileIDs.append('file_id=%d' % fileID)

  #Â Update the files the job did or did not manage to process
  if testFileIDs:

    if state == 'unprocessed':
      # If already at max allocations, go to failed rather than unallocated!
      query = ('UPDATE files SET '
               'state=IF(allocations < ' + str(wfs.db.maxAllocations) + ','
               '"unallocated","failed") '
               'WHERE (' + ' OR '.join(testFileIDs) + ') '
               'AND wfs_job_id=' + str(wfsJobID) + ' '
               'AND request_id=' + str(requestID) + ' '
               'AND stage_id=' + str(stageID) + ' '
               'AND state="allocated"')
    
    else:
      query = ('UPDATE files SET state="outputting" '
               'WHERE (' + ' OR '.join(testFileIDs) + ') '
               'AND wfs_job_id=' + str(wfsJobID) + ' '
               'AND request_id=' + str(requestID) + ' '
               'AND stage_id=' + str(stageID) + ' '
               'AND state="allocated"')

    wfs.db.cur.execute(query)
    
# Deal with the output files needed by the next stage
# Caller must handle exceptions!
def processOutputDids(outputDids, requestID, stageID, wfsJobID):

  for outputDid in outputDids:
      
    if '"' in outputDid or "'" in outputDid:
      continue

    query = ('INSERT INTO files SET state="recorded",'
             'file_did="'          + str(outputDid) + '",'
             'creator_wfs_job_id=' + str(wfsJobID)  + ','
             'request_id='         + str(requestID) + ','
             'stage_id='           + str(stageID + 1)
            )
    wfs.db.cur.execute(query)

# Before uploading output files the generic jobs record any results from 
# the bootstrap script, but just puts them in the "outputting" state
def recordResultsMethod(jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not wfs.db.stringIsJobsubID(jsonDict['jobsub_id']):
    httpError('400 Bad Request', 'Missing jobsub_id in JSON')

  # Check jsonDict contains required lists (can be empty)
  for name in ['processed_dids', 'processed_pfns', 
               'output_dids', 'next_stage_dids']:
    if name not in jsonDict:
      httpError('400 Bad Request', 'Missing value (%s) in JSON' % name)

    for fileDID in jsonDict[name]:
      if not wfs.db.stringNoQuotes(fileDID):    
        httpError('400 Bad Request', 'Invalid DID or PFN in ' + name)
      
  try:
    query = ('SELECT wfs_job_id, request_id, stage_id '
             'FROM jobs WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"')

    wfs.db.cur.execute(query)
    row = wfs.db.cur.fetchone()

    wfsJobID  = int(row['wfs_job_id'])
    requestID = int(row['request_id'])
    stageID   = int(row['stage_id'])
  except:
    httpError('500 Internal Server Error', 'Failed finding job')

  try:
    # Use the event records to find the right replicas for this job
    query = ('SELECT events.file_id,file_did,pfn '
             'FROM events '
             'LEFT JOIN replicas ON (replicas.file_id=events.file_id AND '
             'replicas.rse_id=events.rse_id) '
             'LEFT JOIN files ON events.file_id=files.file_id '
             'WHERE events.wfs_job_id=%d AND events.event_type_id=%d'
             % (wfsJobID, wfs.db.event_FILE_ALLOCATED))
             
    wfs.db.cur.execute(query)
    jobFiles = wfs.db.cur.fetchall()
    
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Failed getting files for the job: ' + str(e))

  processedFileIDs   = []
  unprocessedFileIDs = []

  for jobFile in jobFiles:
    if jobFile['file_did'] in jsonDict['processed_dids'] or \
       jobFile['pfn'] in jsonDict['processed_pfns']:
      processedFileIDs.append(jobFile['file_id'])
    else:
      unprocessedFileIDs.append(jobFile['file_id'])

  # For processed inputs, we just record they got as far as outputting
  # the corresponding output files, for now 
  try:
    updateFileProcessing(processedFileIDs,
                         'outputting',
                         wfsJobID, requestID, stageID)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Failed updating file statuses: ' + str(e))

  # For unprocessed inputs, we can put them straight back into the
  # unprocessed states (unallocated or failed)
  try:
    updateFileProcessing(unprocessedFileIDs,
                         'unprocessed',
                         wfsJobID, requestID, stageID)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Failed updating file statuses: ' + str(e))
          
  try:
    processOutputDids(jsonDict['next_stage_dids'],
                      requestID, stageID, wfsJobID)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Failed recording next stage outputs: ' + str(e))

  try:
    # We set stageID=-1 so that when 1 is added, the output only 
    #Â StageID of 0 is recorded
    processOutputDids(jsonDict['output_dids'], requestID, -1, wfsJobID)
  except Exception as e:
    httpError('500 Internal Server Error', 
              'Failed recording outputs: ' + str(e))

  try:
    query = ('UPDATE jobs SET allocation_state="outputting",'
             'outputting_time=NOW(),heartbeat_time=NOW() '
             'WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"')

    wfs.db.cur.execute(query)
  except Exception as e:
    httpError('500 Internal Server Error', 'Failed updating jobs: ' + str(e))

  try:
    query = ('INSERT INTO jobs_logs SET wfs_job_id=' + str(wfsJobID) + ','
             'bootstrap_log="%s"' %
        base64.b64decode(jsonDict['bootstrap_log']).decode().replace('"', '_')
            )

#    print(str(jsonDict),file=sys.stderr)
#    print(query,file=sys.stderr)
    
    wfs.db.cur.execute(query)
  except Exception as e:
    httpError('500 Internal Server Error', 'Failed saving job log: ' + str(e))

  # Update counts for this stage
  wfs.allocator.updateStageCounts(requestID, stageID)

  #Â Commit everything, before we return OK to the job
  try:
    wfs.db.conn.commit()
  except Exception as e:
    httpError('500 Internal Server Error',
              'Workflow allocator record_results commit failed: ' + str(e))

  # Return OK to the job
  print('Status: 200 OK')
  print()
  sys.exit(0)

# After the output files have been successfully uploaded, the generic job
# comes back with this method which changes the state of input files from
# uploading to processed, and of output files from recorded to finding.
def confirmResultsMethod(jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not wfs.db.stringIsJobsubID(jsonDict['jobsub_id']):
    httpError('400 Bad Request', 'Missing jobsub_id in JSON')

  try:
    query = ('SELECT wfs_job_id,request_id,stage_id,site_id '
             'FROM jobs WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"')

    wfs.db.cur.execute(query)
    row = wfs.db.cur.fetchone()

    wfsJobID  = int(row['wfs_job_id'])
    requestID = int(row['request_id'])
    stageID   = int(row['stage_id'])
    siteID    = int(row['site_id'])
  except:
    httpError('500 Internal Server Error', 'Failed finding job')

  try:
    query = ('UPDATE files SET state="processed" WHERE '
             'state="outputting" AND wfs_job_id=%d' % wfsJobID)

    wfs.db.cur.execute(query)
  except:
    httpError('500 Internal Server Error', 'Updating processed input files')

  try:
    query = ('SELECT file_did,file_id,stage_id FROM files '
             'WHERE state="recorded" AND '
             'creator_wfs_job_id=%d' % wfsJobID)

    wfs.db.cur.execute(query)
    fileRows = wfs.db.cur.fetchall()
  except:
    httpError('500 Internal Server Error', 'Getting recorded output files')

  for fileRow in fileRows:
  
    try:
      query = ('UPDATE files SET state="%s" WHERE file_id=%d' %
               ('finding' if (fileRow['stage_id'] > 0) else 'output', 
                fileRow['file_id']))
               
      wfs.db.cur.execute(query)
      
      wfs.db.logEvent(eventTypeID = wfs.db.event_FILE_CREATED,
                      requestID = requestID,
                      stageID = fileRow['stage_id'],
                      fileID = fileRow['file_id'],
                      wfsJobID = wfsJobID,
                      siteID = siteID,
                      rseName = jsonDict['output_dids'][fileRow['file_did']]
                     )
    except Exception as e:
      httpError('500 Internal Server Error', 
                'Updating recorded output file: ' + str(e))
  
  try:
    query = ('UPDATE jobs SET allocation_state="finished",'
             'finished_time=NOW(),heartbeat_time=NOW() '
             'WHERE wfs_job_id=%d AND allocation_state="outputting"'
             % wfsJobID)

    wfs.db.cur.execute(query)

    wfs.db.logEvent(eventTypeID = wfs.db.event_JOB_FINISHED,
                    requestID = requestID,
                    stageID = stageID,
                    wfsJobID = wfsJobID,
                    siteID = siteID
                   )
  except:
    httpError('500 Internal Server Error', 'Updating job state to finished')
  
  wfs.allocator.updateStageCounts(requestID, stageID)

  #Â Commit everything, before we return OK to the job
  try:
    wfs.db.conn.commit()
  except Exception as e:
    httpError('500 Internal Server Error',
              'Workflow allocator confirm_results commit failed: ' + str(e))

  # Return OK to the job
  print('Status: 200 OK')
  print()
  sys.exit(0)

def checkJobUser():

  # Use certificates and proxies for now; will transition to tokens in future
  if 'SSL_CLIENT_S_DN' not in os.environ or not os.environ['SSL_CLIENT_S_DN']:
    httpError('403 Forbidden', 'Forbidden - identity not provided')

  # Convert to the older format with slashes 
  clientDN='/'+'/'.join(os.environ['SSL_CLIENT_S_DN'].split(',')[::-1])

  query = ('SELECT users.generic_jobs FROM x509 '
           'LEFT JOIN users ON users.user_id=x509.user_id '
           'WHERE x509dn=LEFT("%s",LENGTH(x509dn)) ORDER BY users.user_id' % 
           clientDN.replace('\\','\\\\').replace('"','\\"'))

  try:
    wfs.db.cur.execute(query)
    rows = wfs.db.cur.fetchall()
  except:
    httpError('500 Internal Server Error', 'Error reading database')

  # User must be have generic_jobs=TRUE
  if not rows[0]['generic_jobs']:
    httpError('403 Forbidden', 'Forbidden - authorized identity not provided')

#
# PROGRAM MAIN
#

# Quickly reject random GETs etc (if not handled by Apache already)
if os.environ['REQUEST_METHOD'] != 'POST':
    httpError('405 Method not allowed', 'We only support POST')

# Create a unique ID string for this instance that may also help in debugging
allocatorName = "%s:%d:%f" % (os.uname()[1], os.getpid(), time.time())

# Get the WFS configuration
wfs.conf.readConf()

# Get the JSON document POSTed to us
try:
  jsonDict = json.load(sys.stdin)
except:
  httpError('400 Bad Request', 'Failed to parse JSON')

# Check jsonDict specifies a method
if 'method' not in jsonDict:
  httpError('400 Bad Request', 'Missing method in JSON')

# Do as many checks as we can before connecting to the database here
try:
  wfs.db.conn = MySQLdb.connect(host=wfs.conf.mysqlHostname, 
                                user=wfs.conf.mysqlUsername, 
                                passwd=wfs.conf.mysqlPassword, 
                                db=wfs.conf.mysqlDbName)
  wfs.db.conn.autocommit(False)
  wfs.db.cur = wfs.db.conn.cursor(MySQLdb.cursors.DictCursor) 
except:
  httpError('500 Internal Server Error', 'Problem with database connection')

# Get the stage for the job to work on
if jsonDict['method'] == 'get_stage':
  # Fails with an HTTP error and exit if authorized ID not given
  checkJobUser()
  getStageMethod(jsonDict)

# Record heartbeats from the generic jobs
elif jsonDict['method'] == 'send_heartbeat':
  # Fails with an HTTP error and exit if authorized ID not given
  checkJobUser()
  sendHeartbeatMethod(jsonDict)

# Job aborted
elif jsonDict['method'] == 'job_aborted':
  # Fails with an HTTP error and exit if authorized ID not given
  checkJobUser()
  jobAbortedMethod(jsonDict)

# Get one or more files to process  
elif jsonDict['method'] == 'get_file':
  getFileMethod(jsonDict)
  
# Record results of processing files
elif jsonDict['method'] == 'record_results':
  # Fails with an HTTP error and exit if authorized ID not given
  checkJobUser()
  recordResultsMethod(jsonDict)

# Confirm that uploads went ok
elif jsonDict['method'] == 'confirm_results':
  # Fails with an HTTP error and exit if authorized ID not given
  checkJobUser()
  confirmResultsMethod(jsonDict)

else:
  httpError('400 Bad Request', 'Method in JSON not recognised')

