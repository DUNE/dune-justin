#
# justin-wsgi-allocator - justIN allocator API service
#
# Copyright 2013-25, Andrew McNab for the University of Manchester
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# This WSGI script relies on justin-api-import-script being run by
# the mod_wsgi directive WSGIImportScript

import os
import io
import re
import sys
import time
import stat
import json
import hmac
import secrets
import random
import string
import urllib
import hashlib

#import M2Crypto

# After 0.39.0, M2Crypto needs "from M2Crypto import" syntax 
from M2Crypto import RSA,BIO,X509,ASN1,EVP

#import htcondor

import justin

#sitesRankCacheTimeout = 300

def httpError(startResponse, code, message):

  try:
    justin.conn.rollback()
  except:
    pass

  print('justin-wsgi-allocator fails with %s (%s)' % 
        (code, str(message)), file=sys.stderr)

  startResponse(code,
                [('Content-type',   'text/plain'),
                 ('Content-length', str(len(message)))
                ])

  return [message.encode('UTF-8')]

def httpOK(startResponse, outputString, codeStr = '200 OK'):

  try:
    justin.conn.commit()
  except Exception as e:
    codeStr = '500 Internal Server Error'
    outputString = 'Database commit fails: ' + str(e)

  startResponse(codeStr,
                [('Content-type',   'text/plain'),
                 ('Content-length', str(len(outputString)))
                ])

  return [outputString.encode('UTF-8')]

# Work out an ordered list of stages suited to this entry
def updateSitesRankCache(siteName):

  # We use this to time queries
  startTime = time.time()

  try:
    siteRow = justin.select('SELECT site_id FROM sites '
                            'WHERE site_name="%s" AND enabled' 
                            % siteName, justOne = True)
  except Exception as e: 
    print('updateSitesRankCache() site query fails: ' + str(e), 
          file=sys.stderr)
    return None

  if not siteRow:
    print('updateSitesRankCache() empty site query', file=sys.stderr)
    return None

  try:
    # for the nearest 10 unprocessed files for each stage, calculate the
    # the average distance, and return that distance for the nearest,
    # highest priority,oldest stages
    query = (
      'SELECT stages.stage_priority,'
      'CONCAT("w",workflows.workflow_id,"s",stages.stage_id) AS ws,'
        '(SELECT AVG(sites_storages.distance) '
        'FROM files '
        'LEFT JOIN replicas ON replicas.file_id=files.file_id '
        'LEFT JOIN sites_storages ON sites_storages.rse_id=replicas.rse_id '
        'AND sites_storages.site_id=%d '
        'LEFT JOIN stages_input_storages '
        'ON stages_input_storages.workflow_id=files.workflow_id '
        'AND stages_input_storages.stage_id=files.stage_id '
        'AND stages_input_storages.rse_id=replicas.rse_id '
        'LEFT JOIN storages ON storages.rse_id=replicas.rse_id '
        'WHERE files.state="unallocated" '
        'AND files.workflow_id=stages.workflow_id '
        'AND files.stage_id=stages.stage_id '
        'AND stages_input_storages.rse_id IS NOT NULL '
        'AND sites_storages.distance IS NOT NULL '
        'AND sites_storages.distance <= stages.max_distance '
        'AND storages.rucio_read AND storages.justin_read '
        'ORDER BY sites_storages.distance '
        'LIMIT 10) AS avg_distance '
      'FROM workflows '
      'LEFT JOIN stages ON stages.workflow_id=workflows.workflow_id '
      'LEFT JOIN stages_sites '
      'ON stages_sites.workflow_id=stages.workflow_id '
      'AND stages_sites.stage_id=stages.stage_id '
      'AND stages_sites.site_id=%d '
      'LEFT JOIN scopes ON scopes.scope_id=workflows.scope_id '
      'LEFT JOIN named_quotas ON named_quotas.quota_id=scopes.quota_id '
      'WHERE workflows.workflow_id<>%d '
      'AND workflows.state="running" '
      'AND stages.stage_priority IS NOT NULL '
      'AND stages_sites.site_id IS NOT NULL '
      'AND named_quotas.running_jobs < named_quotas.max_running_jobs '
      'AND named_quotas.processing_enabled '
# WE SHOULD CHECK processing_enabled FOR THE SUBMITTER OF THE WORKFLOW TOO!
      'ORDER BY avg_distance,stages.stage_priority DESC,'
      'workflows.workflow_id LIMIT 100'
      % (siteRow['site_id'], siteRow['site_id'], justin.awtWorkflowID))

    print('updateSitesRankCache() query: ' + query, file=sys.stderr)
    stages = justin.select(query)
    print('updateSitesRankCache() stages: ' + str(stages), file=sys.stderr)
  except Exception as e:
    print('updateSitesRankCache() fails: ' + str(e), file=sys.stderr)
    return None

  ranks = []
  for stage in stages:
    # the rank of each stage is 100-distance as the integer plus the 
    # priority/100 where priority is 1-99 as the fraction. these ranks are
    # used unmodified as HTCondor ranks with higher being higher priority
    if stage['avg_distance'] is not None:
      ranks.append('%s=%.2f' %
          (stage['ws'], 
           100.0 - int(stage['avg_distance']) + stage['stage_priority']/100.0))

  rankText = '[' + ';'.join(ranks) + ']'

  try:
    justin.insertUpdate('REPLACE INTO sites_ranks_cache '
       'SET site_id=%d,rank_text="%s",cache_time=NOW(),query_seconds=%f'
       % (siteRow['site_id'], rankText, time.time() - startTime))
  except Exception as e:
    print('updateSitesRankCache() REPLACE fails: ' + str(e), file=sys.stderr)
    return None

  return rankText

# Return Class Ads to for glideins to publish
def getClassadsCall(startResponse, environ):

  try:
    inputLength = int(environ.get('CONTENT_LENGTH', '0'))
    inputString = environ['wsgi.input'].read(inputLength).decode()
  except Exception as e:
    return httpError(startResponse, 
                     '400 Bad Request', 'Failed to get inputString: ' + str(e))

  try:
    i = inputString.index('GLIDEIN_UUID ')
    glideinUUID = inputString[i+13:].split('\n')[0]

    if not justin.stringNoQuotes(glideinUUID):
      raise RuntimeError('Quotes in UUID')
  except Exception as e:
    return httpError(startResponse, 
                     '400 Bad Request', 'Failed to get UUID: ' + str(e))

  try:
    i = inputString.index('GLIDEIN_DUNESite ')
    siteName = inputString[i+17:].split('\n')[0]

    if not justin.stringNoQuotes(siteName):
      raise RuntimeError('Quotes in siteName')
  except Exception as e:
    return httpError(startResponse, 
                     '400 Bad Request', 'Failed to get siteName: ' + str(e))

  try:
    i = inputString.index('GLIDEIN_Entry_Name ')
    entryName = inputString[i+19:].split('\n')[0]

    if not justin.stringNoQuotes(entryName):
      raise RuntimeError('Quotes in entryName')
  except Exception as e:
    return httpError(startResponse, 
                     '400 Bad Request', 'Failed to get entryName: ' + str(e))

  try:
    i = inputString.index('HOSTNAME=')
    pilotHostname = inputString[i+9:].split('\n')[0]

    if not justin.stringNoQuotes(pilotHostname):
      raise RuntimeError('Quotes in pilotHostname')
  except Exception as e:
    pilotHostname=''
#    return httpError(startResponse, 
#                     '400 Bad Request', 'Failed to get Hostname: ' + str(e))

  try:
    siteEntryRow = justin.select('SELECT '
         '(SELECT site_id FROM sites WHERE site_name="%s") AS site_id,'
         '(SELECT entry_id FROM entries WHERE entry_name="%s") AS entry_id'
         % (siteName, entryName), justOne = True)

    siteID  = int(siteEntryRow['site_id'])
    entryID = int(siteEntryRow['entry_id'])
  except Exception as e:
    return httpError(startResponse, '404 Not Found', 'Not Found: ' + str(e))

  try:
    justin.insertUpdate('INSERT INTO pilot_events SET '
                        'glidein_uuid="%s",entry_id=%d,hostname="%s",'
                        'event_time=NOW()'
                        % (glideinUUID, entryID, pilotHostname))
  except:
    pass
    
  try:
    row = justin.select('SELECT rank_text,'
                        'TIMESTAMPDIFF(SECOND, cache_time, NOW()) AS seconds '
                        'FROM sites_ranks_cache '
                        'WHERE site_id=%d AND cache_time IS NOT NULL'
                        % siteID, justOne = True)
  except Exception as e:
    return httpError(startResponse, '404 Not Found', 'Not Found: ' + str(e))

  if row is None:
    rankText     = '[]'
    cacheSeconds = justin.sitesRankCacheTimeout + 1
  else:
    rankText     = row['rank_text']
    cacheSeconds = row['seconds']

  if cacheSeconds > justin.sitesRankCacheTimeout:
    try:
      getCacheLock('justin_allocator_ranks_cache', 0)
    except:
      pass
    else:
      rankText = updateSitesRankCache(siteName)
      releaseCacheLock('justin_allocator_ranks_cache')

  # This needs to be the same from all instances as justin-get-classads.sh
  # run inside the GlideIn queries all allocator instances and takes 
  # the Rank expression from the LAST one
  justinRank = ('isUndefined(My.JUSTIN_FNAL_Rank)?0:My.JUSTIN_FNAL_Rank + '
                'isUndefined(My.JUSTIN_RAL_Rank )?0:My.JUSTIN_RAL_Rank  + '
                'isUndefined(My.JUSTIN_DEV_Rank )?0:My.JUSTIN_DEV_Rank  + '
                'isUndefined(My.JUSTIN_INT_Rank )?0:My.JUSTIN_INT_Rank')

  # Return ClassAds to be published by the pilot/glidein
  # If a justIN job has a stage not listed in the rankText dictionary
  # then it gets rank Undefined which equals 0
  instanceUpper = justin.instance.upper()
  return httpOK(startResponse, 
           "JUSTIN_%s_ClassAds_Time %d\n"
           "JUSTIN_%s_Stage_Ranks_Raw %s\n"
           "JUSTIN_%s_Stage_Ranks "
             "isUndefined(My.GLIDEIN_PS_JUSTIN_%s_Stage_Ranks_Raw)"
             "?My.JUSTIN_%s_Stage_Ranks_Raw"
             ":My.GLIDEIN_PS_JUSTIN_%s_Stage_Ranks_Raw\n"
           "JUSTIN_%s_Rank isUndefined(Target.JUSTIN_%s_Stage)"
             "?0:My.JUSTIN_%s_Stage_Ranks[Target.JUSTIN_%s_Stage]\n"
           "Rank %s\n"
              % (instanceUpper, int(time.time()),
                 instanceUpper, rankText,
                 instanceUpper, instanceUpper, instanceUpper, instanceUpper,
                 instanceUpper, instanceUpper, 
                   instanceUpper, instanceUpper,
                 justinRank)
                )

# Return information about the job from the jobsubID
def makeJobDict(jsonDict, jobscriptSecret = None):

  # Find the job info and the stage's max_distance value
  try:
    query = ('SELECT '
             'jobs.job_state,'
             'jobs.jobscript_secret,'
             'stages.max_distance,'
             'workflows.mql,'
             'workflows.state,'
             'jobs.workflow_id,'
             'jobs.stage_id,'
             'jobs.justin_job_id,'
             'jobs.site_id,'
             'jobs.requested_processors,'
             'jobs.requested_rss_bytes,'
             'jobs.requested_wall_seconds,'
             'jobs.has_inner_apptainer,'
             'jobs.jobscript_exit,'
             'jobs.allocated_files,'
             'sites.site_name,'
             'sites.running_jobs,'
             'sites.enabled,'
             'sites.always_inner_apptainer AS site_a_i_a,'
             'entries.always_inner_apptainer AS entry_a_i_a,'
             'entries.entry_name,'
             'entries.entry_id,'
             'scopes.scope_name,'
             'wlcg_groups.wlcg_group_name,'
             'named_quotas.processing_enabled AS quota_processing_enabled,'
             'users.processing_enabled AS user_processing_enabled,'
             'stages_sites.site_id AS s_s_site_id,'
             '(SELECT key_value FROM keys_names_values '
             ' WHERE key_name="ping_rucio_milliseconds") '
             ' AS ping_rucio_milliseconds ' 
             'FROM jobs '
             'LEFT JOIN stages ON jobs.workflow_id=stages.workflow_id '
             'AND jobs.stage_id=stages.stage_id '
             'LEFT JOIN sites ON jobs.site_id=sites.site_id '
             'LEFT JOIN stages_sites '
             'ON stages_sites.workflow_id=jobs.workflow_id '
             'AND stages_sites.stage_id=jobs.stage_id '
             'AND stages_sites.site_id=jobs.site_id '
             'LEFT JOIN entries ON jobs.entry_id=entries.entry_id '
             'LEFT JOIN workflows ON workflows.workflow_id=jobs.workflow_id '
             'LEFT JOIN scopes ON scopes.scope_id=workflows.scope_id '
             'LEFT JOIN named_quotas ON scopes.quota_id=named_quotas.quota_id '
             'LEFT JOIN wlcg_groups '
             'ON wlcg_groups.wlcg_group_id=named_quotas.wlcg_group_id '
             'LEFT JOIN users ON users.user_id=workflows.user_id '
             'WHERE jobs.jobsub_id="%s"'
             % jsonDict['jobsub_id'])

    job = justin.select(query, justOne = True)
  except Exception as e:
    return { "error_message": "Error finding job from jobsubID " + 
             jsonDict['jobsub_id'] + ": " + str(e) }

  if not job:
    return { "error_message": "Failed to find job from jobsubID " + 
             jsonDict['jobsub_id'] }

  if jobscriptSecret is not None and \
     job['jobscript_secret'] != jobscriptSecret:
    return { "error_message": "Jobscript secret mismatch" }

  return { "error_message"          : None,
           "workflow_id"            : job['workflow_id'],
           "stage_id"               : job['stage_id'],
           "workflow_state"         : job['state'],
           "site_id"                : job['site_id'],
           "site_name"              : job['site_name'],
           "site_a_i_a"             : job['site_a_i_a'],
           "entry_id"               : job['entry_id'],
           "entry_name"             : job['entry_name'],
           "entry_a_i_a"            : job['entry_a_i_a'],
           "running_jobs"           : job['running_jobs'],
           "site_enabled"           : job['enabled'],
           "justin_job_id"          : job['justin_job_id'],
           "job_state"              : job['job_state'],
           "jobscript_secret"       : job['jobscript_secret'],
           "max_distance"           : job['max_distance'],
           "mql"                    : job['mql'],
           "requested_processors"   : job['requested_processors'],
           "requested_rss_bytes"    : job['requested_rss_bytes'],
           "requested_wall_seconds" : job['requested_wall_seconds'],
           "has_inner_apptainer"    : job['has_inner_apptainer'],
           "jobscript_exit"         : job['jobscript_exit'],
           "allocated_files"        : job['allocated_files'],
           "scope_name"             : job['scope_name'] 
                                      if job['scope_name'] else '',
           "wlcg_group_name"        : job['wlcg_group_name']
                                      if job['wlcg_group_name'] else '',
           "quota_processing_enabled" : bool(job['quota_processing_enabled']),
           "user_processing_enabled"  : bool(job['user_processing_enabled']),
           "ping_rucio_milliseconds": int(job['ping_rucio_milliseconds'])
         }

# Make a new certificate from the certificate workflow sent by the 
# wrapper job, signed by the VOMS proxy loaded from local disk
# Exceptions must by caught by the caller
def delegateJobsProxy(newCertRequest, jobsProxyString):

#  print('newCertRequest: ' + newCertRequest, file=sys.stderr)
#  print('jobsProxyString: ' + str(jobsProxyString), file=sys.stderr)
  if not jobsProxyString:
    print('Failed to read proxy file from disk?', file=sys.stderr)

  jobsProxyKey      = RSA.load_key_string(jobsProxyString)
  jobsProxyCertsBIO = BIO.MemoryBuffer(jobsProxyString)

  jobsProxyCerts = []
  while True:
    try:
      jobsProxyCerts.append(X509.load_cert_bio(jobsProxyCertsBIO))
    except:
      jobsProxyCertsBIO.close()
      break

  newPubKey = X509.load_request_string(newCertRequest, 
                                       X509.FORMAT_PEM).get_pubkey()

  # Create and populate new certificate   
  newCert = X509.X509()
  newCert.set_pubkey(newPubKey)
  newCert.set_serial_number(int(time.time() * 100))
  newCert.set_issuer_name(jobsProxyCerts[0].get_subject())
  newCert.set_version(2) # "2" is X.509 for "v3" ...

  # Add CN=CENTISECONDS to existing VOMS proxy DN
  newSubject = jobsProxyCerts[0].get_subject()
  newSubject.add_entry_by_txt(field = "CN",
                              type  = 0x1001,
                              entry = str(int(time.time() * 100)),
                              len   = -1,
                              loc   = -1,
                              set   = 0)

  newCert.set_subject_name(newSubject)

  # Set not before time to now
  newNotBefore = ASN1.ASN1_UTCTIME()
  newNotBefore.set_time(int(time.time()))
  newCert.set_not_before(newNotBefore)

  # Set not after time to now + 7 days
  newNotAfter = ASN1.ASN1_UTCTIME()
  newNotAfter.set_time(int(time.time()) + 604800)
  newCert.set_not_after(newNotAfter)

  # Usual extension giving key usage
  newCert.add_ext(X509.new_extension("keyUsage", 
                  "Digital Signature, Key Encipherment, Key Agreement", 1))

  # Special extension for RFC X.509 proxies 
  newCert.add_ext(X509.new_extension("proxyCertInfo", 
                  "critical, language:1.3.6.1.4.1.3536.1.1.1.9", 1, 0))

  # Sign this new cert with the VOMS proxy private key
  jobsProxyKeyEVP = EVP.PKey()
  jobsProxyKeyEVP.assign_rsa(jobsProxyKey)
  newCert.sign(jobsProxyKeyEVP, 'sha256')

  # Return the results as PEM encoded strings
  jobsProxyCertsChain = ''
  for oneCert in jobsProxyCerts:
     jobsProxyCertsChain += oneCert.as_pem().decode()

  return (newCert.as_pem().decode(), jobsProxyCertsChain)

def makeJobscriptDict(jsonDict, jobDict):
  # Collect values to be transmitted to the user jobscript

  jobscriptDict = {}
  scopeName = jobDict['scope_name']

  try:
    query = ("SELECT jobscript,principal_name,jobscript_image,jobscript_git,"
             "quota_name "
             "FROM stages_jobscripts "
             "LEFT JOIN workflows "
             "ON workflows.workflow_id=stages_jobscripts.workflow_id "
             "LEFT JOIN stages "
             "ON stages.workflow_id=stages_jobscripts.workflow_id "
             "AND stages.stage_id=stages_jobscripts.stage_id "
             "LEFT JOIN scopes ON scopes.scope_id=workflows.scope_id "
             "LEFT JOIN named_quotas "
             "ON named_quotas.quota_id=scopes.quota_id "
             "LEFT JOIN users ON users.user_id=workflows.user_id "
             "LEFT JOIN principal_names "
             "ON principal_names.pn_id=users.main_pn_id "
             "WHERE stages_jobscripts.workflow_id=%d "
             "AND stages_jobscripts.stage_id=%d" 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    row = justin.select(query, justOne = True)
    jobscriptDict['principal_name']   = row['principal_name']
    jobscriptDict['jobscript']        = row['jobscript']
    jobscriptDict['jobscript_image']  = row['jobscript_image']
    jobscriptDict['quota_name']       = row['quota_name']

    # TO BE REMOVED ONCE ALL USERS HAVE PERSONAL QUOTAS
    if jobscriptDict['quota_name'] == 'usertests':
      jobscriptDict['quota_name'] = 'dunepro'      

    if row['jobscript_git']:
      jobscriptDict['jobscript_git'] = row['jobscript_git']

  except Exception as e:
    print('SELECT stages_jobscripts fails: ' + str(e), file=sys.stderr)
    return None

  
  # Create and add files for VOMS proxies for the job
  try:
    (newCertNoRoles, newChainNoRoles) = \
                 delegateJobsProxy(jsonDict['csr-no-roles'],
                                   justin.jobsNoRolesProxyString)

    (newCertProduction, newChainProduction) = \
                 delegateJobsProxy(jsonDict['csr-production'],
                                   justin.jobsProductionProxyString)

  except Exception as e:
    print('Failed to create certs for delegated proxies: ' + str(e), 
          file=sys.stderr)
    return None

  jobscriptDict['justin-jobs-no-roles.cert.pem']    = newCertNoRoles
  jobscriptDict['justin-jobs-no-roles.chain.pem']   = newChainNoRoles
  jobscriptDict['justin-jobs-production.cert.pem']  = newCertProduction
  jobscriptDict['justin-jobs-production.chain.pem'] = newChainProduction

  # Create a list of the output file patterns
  try:
    query = ("SELECT destination,file_pattern,"
             "for_next_stage,pattern_id "
             "FROM stages_outputs "
             "LEFT JOIN stages "
             "ON stages.workflow_id=stages_outputs.workflow_id "
             "AND stages.stage_id=stages_outputs.stage_id "
             "LEFT JOIN workflows "
             "ON workflows.workflow_id=stages_outputs.workflow_id "
             "WHERE stages_outputs.workflow_id=%d "
             "AND stages_outputs.stage_id=%d" 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    rows = justin.select(query)
  except:
    return None

  jobscriptDict['patterns'] = []

  for row in rows:
    if row['destination'].startswith('https://'):
      patternType = 'url'
    elif row['for_next_stage']:
      patternType = 'for_next_stage'
    else:
      patternType = ''

    jobscriptDict['patterns'].append((patternType,
                                      row['file_pattern'],
                                      row['pattern_id']))

  # Create a list of RSEs for Workflow Test jobs
  # RSEs for uploads by the wrapper job are listed in the record_results
  # response dictionary not here!
  if jobDict['workflow_id'] == justin.awtWorkflowID:
    try:
      query = ('SELECT rse_name,lan_write_scheme,wan_write_scheme,distance,'
               '(SELECT wan_pfn FROM replicas WHERE workflow_id=%d AND '
               ' replicas.rse_id=storages.rse_id ORDER BY replica_id DESC LIMIT 1)'
               ' AS wan_pfn,'
               '(SELECT lan_pfn FROM replicas WHERE workflow_id=%d AND '
               ' replicas.rse_id=storages.rse_id ORDER BY replica_id DESC LIMIT 1)'
               ' AS lan_pfn '
               'FROM storages '
               'LEFT JOIN sites_storages '
               'ON sites_storages.rse_id=storages.rse_id '
               'AND sites_storages.site_id=%d '
               'WHERE rse_name<>"MONTECARLO" '
               'AND NOT storages.decommissioned GROUP BY rse_name' % 
               (justin.awtWorkflowID,
                justin.awtWorkflowID,
                jobDict['site_id']) )

      rseRows = justin.select(query)
    except Exception as e:
      print('AWT list of RSEs query fails with: ' + str(e), file=sys.stderr)
      return None

    jobscriptDict['awt_rses'] = []
    for rseRow in rseRows:
      if rseRow['lan_pfn'] and rseRow['distance'] == 0:
        pfn = rseRow['lan_pfn']
      elif rseRow['wan_pfn']:
        pfn = rseRow['wan_pfn']
      else:
        continue

      jobscriptDict['awt_rses'].append((rseRow['rse_name'],
                                        rseRow['lan_write_scheme']
                                        if rseRow['distance'] == 0
                                        else rseRow['wan_write_scheme'],
                                        pfn))

  jobscriptDict['site_name']        = jobDict['site_name']
  jobscriptDict['workflow_id']      = jsonDict['workflow_id']
  jobscriptDict['stage_id']         = jsonDict['stage_id']
  jobscriptDict['scope']            = scopeName

  jobscriptDict['mql']                    = jobDict['mql']
  jobscriptDict['requested_processors']   = jobDict['requested_processors']
  jobscriptDict['requested_rss_bytes']    = jobDict['requested_rss_bytes']
  jobscriptDict['requested_wall_seconds'] = jobDict['requested_wall_seconds']

  jobscriptDict['jobsub_id']        = jsonDict['jobsub_id']
  jobscriptDict['jobscript_secret'] = jobDict['jobscript_secret']
  jobscriptDict['sam_web_uri']      = \
    'https://justin.dune.hep.ac.uk/api/samweb/%s/%s' \
     % (jsonDict['jobsub_id'], jobDict['jobscript_secret'])

  # User defined environment variables for the jobscript
  jobscriptDict['stage_env'] = []

  try:
    query = ('SELECT env_name,env_value FROM stages_environment '
             'WHERE workflow_id=%d AND stage_id=%d ORDER BY env_name' 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    envRows = justin.select(query)
  except:
    return None
  else:
    # Add the user defined ones
    for envRow in envRows:
      jobscriptDict['stage_env'].append((envRow['env_name'],
                                         envRow['env_value']))

  return jobscriptDict

def updateTerminalJobState(justinJobID = 0, jobState = '', 
                           workflowID = 0, stageID = 0, siteID = 0,
                           entryID = 0):

# Update job_state with one of the terminal states
# THIS DOES A COMMIT TOO AND MUST ONLY BE USED IMMEDIATELY BEFORE
# httpError() IN THE CALLING FUNCION

  if jobState == 'finished':
    eventTypeID = justin.event_JOB_FINISHED
  elif jobState == 'notused':
    eventTypeID = justin.event_JOB_NOTUSED
  elif jobState == 'aborted':
    eventTypeID = justin.event_JOB_ABORTED
  elif jobState == 'jobscript_error':
    eventTypeID = justin.event_JOB_SCRIPT_ERROR
  elif jobState == 'outputting_failed':
    eventTypeID = justin.event_JOB_OUTPUTTING_FAILED
  elif jobState == 'none_processed':
    eventTypeID = justin.event_JOB_NONE_PROCESSED
  else:
    raise RuntimeError('state %s not handled by updateTerminalJobState()'
                       % jobState)
  try:
    justin.insertUpdate(
             'UPDATE jobs SET job_state="%s",'
             'heartbeat_time=NOW(),'
             'finished_time=NOW() '
             'WHERE justin_job_id=%d' % (jobState, justinJobID))

    justin.logEvent(eventTypeID = eventTypeID,
                    workflowID  = workflowID, 
                    stageID     = stageID,
                    justinJobID = justinJobID,
                    siteID      = siteID,
                    entryID     = entryID
                   )

    justin.conn.commit()
  except: 
    pass

# Get details of this jobs workflow/stage
def getJobscriptMethod(startResponse, jsonDict, environ):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request', 
                     'Missing jobsub_id in JSON')

  # Check jsonDict contains required integer/bool values (eg rss_bytes)
  for name in ['workflow_id', 'stage_id', 'rss_bytes', 'processors',
               'wall_seconds', 'has_inner_apptainer']:
    try:
      n = int(jsonDict[name])
    except:
      return httpError(startResponse,
                       '400 Bad Request', 
                       'Missing/invalid integer value(s) in JSON')

  # Check jsonDict contains required string values
  for name in ['site_name', 'entry_name', 'cpuinfo', 'os_release', 
               'hostname', 'site_job_id']:
    if name not in jsonDict or not justin.stringNoQuotes(jsonDict[name]) \
       or not jsonDict[name]:
      return httpError(startResponse, 
                       '400 Bad Request', 
                       'Missing/invalid value for %s in JSON' % name)

  if jsonDict['site_name'] == 'XX_UNKNOWN' or \
     jsonDict['entry_name'] == 'UNKNOWN':
    print('Job %s has unknown site_name or entry_name - aborting' 
          % jsonDict['jobsub_id'], file=sys.stderr)
    return httpError(startResponse, '400 Bad Request', 'Unknown site')

  # If the job is in the submitted state then update with host details
  # If not in submitted, then some mismatch which we catch and return 
  # 410 below
  try:
    if jsonDict['gpu_uuid']:
      gpuInfo = ('%s %s %s %s %dMiB' 
                  % (jsonDict['gpu_name'], jsonDict['gpu_driver_version'],
                     jsonDict['gpu_compute_cap'], jsonDict['gpu_vbios_version'],
                     int(jsonDict['gpu_nonreserved_bytes'] / 1048576)))
    else:
      gpuInfo = ''

    justin.insertUpdate('UPDATE jobs SET '
                           'site_id=(SELECT site_id FROM sites '
                           'WHERE site_name="%s"),'
                           'entry_id=(SELECT entry_id FROM entries '
                           'WHERE entry_name="%s"),'
                           'cpuinfo="%s",' 
                           'os_release="%s",' 
                           'hostname="%s",' 
                           'rss_bytes=%d,' 
                           'processors=%d,' 
                           'wall_seconds=%d,'
                           'has_inner_apptainer=%s,'
                           'gpu_info="%s" ' 
                           'WHERE jobsub_id="%s" AND '
                           'workflow_id=%d AND stage_id=%d AND '
                           'job_state="submitted"' % 
                           (jsonDict['site_name'], 
                            jsonDict['entry_name'], 
                            jsonDict['cpuinfo'],
                            jsonDict['os_release'],
                            jsonDict['hostname'],
                            jsonDict['rss_bytes'],
                            jsonDict['processors'],
                            jsonDict['wall_seconds'],
                            jsonDict['has_inner_apptainer'],
                            gpuInfo,
                            jsonDict['jobsub_id'],
                            jsonDict['workflow_id'],
                            jsonDict['stage_id']
                           ))
    justin.conn.commit()
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error',
                     'justIN allocator service fails to update job with site_id '
                     '(site_name=%s): %s' %
                     (str(jsonDict['site_name']), str(e)))
    
  # Find details of the job
  jobDict = makeJobDict(jsonDict)

  if jobDict['ping_rucio_milliseconds'] >= justin.overloadRucioMilliseconds:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'],
                           jobState    = 'notused')
    return httpError(startResponse,
                     '503 Service Unavailable',
                     'New processing suspended while Rucio is overloaded')

  if jobDict['workflow_state'] != 'running':
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'],
                           jobState    = 'notused')
    return httpError(startResponse,
                     '503 Service Unavailable',
                     'Workflow not running (%s)' % jobDict['workflow_state'])

  if not jobDict['user_processing_enabled']:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'],
                           jobState    = 'notused')
    return httpError(startResponse,
                     '403 Forbidden',
                     'Processing disabled for this user')

  if not jobDict['quota_processing_enabled']:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'],
                           jobState    = 'notused')
    return httpError(startResponse,
                     '403 Forbidden',
                     'Processing disabled for this named quota')

  if (jobDict['workflow_id'] != jsonDict['workflow_id']) or \
     (jobDict['stage_id'] != jsonDict['stage_id']):
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'],
                           jobState    = 'aborted')
    return httpError(startResponse,
                     '403 Forbidden',
                     'workflow ID / stage ID do not match jobsub ID')

  # Record whether job slots from this entry always support Inner Apptainer
  # 0=not always, 1=not known, 2=always
  if jsonDict['has_inner_apptainer']:
    if jobDict['entry_a_i_a'] == 1:
      entryApptainerSQL = ',always_inner_apptainer=2'
    else:
      entryApptainerSQL = ''

    if jobDict['site_a_i_a'] == 1:
      siteApptainerSQL = ',always_inner_apptainer=2'
    else:
      siteApptainerSQL = ''
  else:
    siteApptainerSQL  = ',always_inner_apptainer=0'
    entryApptainerSQL = ',always_inner_apptainer=0'

  try:
    justin.insertUpdate('UPDATE sites SET last_get_jobscript_time=NOW()%s '
                        'WHERE site_id=%d' 
                        % (siteApptainerSQL, jobDict['site_id']))
    justin.insertUpdate('UPDATE entries SET last_get_jobscript_time=NOW()%s '
                        'WHERE entry_id=%d' 
                        % (entryApptainerSQL, jobDict['entry_id']))
    justin.conn.commit()
  except:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse, 
                 '500 Internal Server Error',
                 'justIN allocator service fails to record get_jobscript time')

  # Check that we're not trying to run unprivileged jobscripts 
  # without Inner Apptainer: some kind of out of date matching?
  if jsonDict['workflow_id'] != justin.awtWorkflowID and \
     not jsonDict['has_inner_apptainer'] and \
     (not jobDict['wlcg_group_name'] 
      or jobDict['wlcg_group_name'] != '/dune/production'):
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'notused')
    return httpError(startResponse,
              '404 Not Found',
              'Tried to run an unprivileged workflow without Inner Apptainer')

  if jobDict['error_message']:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse,
                     '400 Bad Request', 
                     jobDict['error_message'] + ' in getJobscriptMethod')

  # Some kind of mismatch. Maybe the idle job was marked as stalled. Maybe 
  # HTCondor rescheduled the job and it has tried to run again. In all cases 
  # leave the job state as is.
  if jobDict['job_state'] != 'submitted':
    return httpError(startResponse, 
                     '410 Gone', 
                     'Cannot get stage as job already in %s state' 
                     % jobDict['job_state'])

  # Site not enabled so no matches allowed 
  if not jobDict['site_enabled'] and \
     jsonDict['workflow_id'] != justin.awtWorkflowID:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'notused')
    return httpError(startResponse, 
                     '404 Not Found',
                     'Cannot start as site %s not enabled' 
                     % jobDict['site_name'])

  jobscriptDict = makeJobscriptDict(jsonDict, jobDict)
  if not jobscriptDict:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed to create jobscript dictionary')
           
  try:
    # Unique ID string for this instance that may also help in debugging
    allocatorName = environ['SERVER_NAME']

    justin.insertUpdate(
             'UPDATE jobs SET '
             'site_job_id="%s",'
             'allocation_time=NOW(),' 
             'heartbeat_time=NOW(),'
             'finished_time="%s",' 
             'job_state="%s",'
             'allocator_name="%s",' 
             'workflow_id=%d,'
             'stage_id=%d '
             'WHERE jobsub_id="%s"'
             % (str(jsonDict['site_job_id']), 
                justin.unixEpoch,                
                'processing' if jsonDict['workflow_id']==justin.awtWorkflowID 
                else 'started',
                allocatorName,
                jsonDict['workflow_id'],
                jsonDict['stage_id'],
                jsonDict['jobsub_id']))

    justin.logEvent(eventTypeID = justin.event_JOB_STARTED,
                    workflowID = jsonDict['workflow_id'],
                    stageID = jsonDict['stage_id'],
                    justinJobID = jobDict['justin_job_id'],
                    siteID = jobDict['site_id'],
                    entryID = jobDict['entry_id']
                   )

    if jsonDict['workflow_id']==justin.awtWorkflowID:
      # AWT jobs go straight to processing too
      justin.logEvent(eventTypeID = justin.event_JOB_PROCESSING,
                      workflowID = jsonDict['workflow_id'],
                      stageID = jsonDict['stage_id'],
                      justinJobID = jobDict['justin_job_id'],
                      siteID = jobDict['site_id'],
                      entryID = jobDict['entry_id']
                     )

  except Exception as e:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'justIN allocator service failed updating allocated job: ' 
                     + str(e))
  try:
    justin.conn.commit()
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Database commit fails: ' + str(e))

  s = json.dumps(jobscriptDict).encode('UTF-8')
  startResponse('200 OK',
                [('Content-type',   'application/json'),
                 ('Content-length', str(len(s)))
                ])

  return [s]
   
# Record heartbeats from wrapper jobs
def sendHeartbeatMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request', 
                     'Missing jobsub_id in JSON')

  try:
    justin.insertUpdate('UPDATE jobs SET heartbeat_time=NOW() ' 
                        'WHERE jobsub_id="' + str(jsonDict['jobsub_id']) + '"'
                       )
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'justIN allocator service failed recording heartbeat: ' 
                     + str(e))

  return httpOK(startResponse, '')
   
# Job aborted 
def jobAbortedMethod(startResponse, jsonDict):

  try:
    # Default event type ID is just the abort_code
    eventTypeID = int(jsonDict['abort_code'])
  except:
    return httpError(startResponse, 
                     '400 Bad Request', 
                     'Missing/invalid http_code in JSON')

  if 'aborted_method' not in jsonDict or \
     not justin.stringNoQuotes(jsonDict['aborted_method']):
    return httpError(startResponse,  
                     '400 Bad Request', 
                     'Missing/invalid aborted_method in JSON')

  jobDict = makeJobDict(jsonDict)

  if jobDict['error_message']:
    return httpError(startResponse,
                     '400 Bad Request', 
                     jobDict['error_message'] + ' in jobAbortedMethod')

  if jobDict['job_state'] not in ['submitted','notused','started',
                                  'processing','outputting',
                                  'jobscript_error','none_processed']:
    return httpError(startResponse, 
                     '410 Gone', 
                     'Cannot set job to aborted as job already in %s state' 
                     % jobDict['job_state'])

  # Work out the job state to set, from the given event type ID

  if eventTypeID == justin.event_JOB_ABORT_SCRIPT_ERROR:
    state = 'jobscript_error'
    
  elif eventTypeID in [ justin.event_JOB_ABORT_CREATING_LOGS_TGZ,
                        justin.event_JOB_ABORT_METACAT_LOGS_CREATION,
                        justin.event_JOB_ABORT_METACAT_LOGS_REGISTRATION,
                        justin.event_JOB_ABORT_RUCIO_UPLOAD_LOGS,
                        justin.event_JOB_ABORT_WEBDAV_UPLOAD,
                        justin.event_JOB_ABORT_CREATING_METADATA,
                        justin.event_JOB_ABORT_METACAT_REGISTRATION,
                        justin.event_JOB_ABORT_RUCIO_UPLOAD_FILE,
                        justin.event_JOB_ABORT_METACAT_UPLOADED,
                        justin.event_JOB_ABORT_RUCIO_SILENT_FAILURE ]:
# WANT TO RESET FILE ALLOCATIONS TOO HERE?
    state = 'outputting_failed'
  else:
    state = 'aborted'

  print('Job %s aborted: %d %s' 
        % (str(jsonDict['jobsub_id']),
           eventTypeID,
           str(jsonDict['aborted_method'])),
        file=sys.stderr)

  try:
    justin.insertUpdate(
             'UPDATE jobs SET job_state="%s",'
             'heartbeat_time=NOW(),'
             'finished_time=NOW() '
             'WHERE jobsub_id="%s"' %
             (state, jsonDict['jobsub_id']))

    if 'rse_name' in jsonDict:
      rseName = jsonDict['rse_name']
    else:
      rseName = None

    justin.logEvent(eventTypeID   = eventTypeID,
                    workflowID    = jobDict['workflow_id'],
                    stageID       = jobDict['stage_id'],
                    justinJobID   = jobDict['justin_job_id'],
                    jobscriptExit = jobDict['jobscript_exit'],
                    siteID        = jobDict['site_id'],
                    entryID       = jobDict['entry_id'],
                    rseName       = rseName
                   )

  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'justIN allocator service job_aborted failed: ' + str(e))

  return httpOK(startResponse, '')

def cacheFiles(siteID, workflowID, stageID, maxDistance):
  # Cache best replicas info for the best stage for a site, 
  # relative to that site

  # We use this time to remove previous cache entries
  cutoffTime = int(time.time())

  try:
    replicaRows = justin.select('SELECT files.file_id,'
                   'replicas.replica_id,'
                   'replicas.rse_id,'
                   'distance '
                   'FROM files '
                   'LEFT JOIN replicas ON files.file_id=replicas.file_id '
                   'LEFT JOIN storages ON replicas.rse_id=storages.rse_id '
                   'LEFT JOIN sites_storages ON '
                   'replicas.rse_id=sites_storages.rse_id AND '
                   'sites_storages.site_id=%d '
                   'LEFT JOIN stages_input_storages '
                   'ON stages_input_storages.workflow_id=files.workflow_id '
                   'AND stages_input_storages.stage_id=files.stage_id '
                   'AND stages_input_storages.rse_id=replicas.rse_id '
                   'WHERE files.workflow_id=%d AND files.stage_id=%d AND '
                   'files.state="unallocated" AND '
                   'distance<=%f AND '
                   'accessible_until > NOW() AND '
                   'storages.rucio_read AND storages.justin_read AND '
                   'NOT storages.decommissioned AND '
                   'stages_input_storages.rse_id IS NOT NULL '
                   'ORDER by distance,replicas.file_id LIMIT 1000' % 
                   (siteID, workflowID, stageID, maxDistance))
  except Exception as e:
    print('Failed getting replica info to cache: %s' % str(e), file=sys.stderr)
    return

  filesSeen = set()

  for replicaRow in replicaRows:
      # Go through the replicas, caching them.
      # Do not add more distant replicas of files already seen.
      # This deduplication in the Python agent reduces the big scans of
      # the database which block other services.
      if len(filesSeen) >= 500:
        break
     
      if replicaRow['file_id'] in filesSeen:
        continue
      
      filesSeen.add(replicaRow['file_id'])
   
      try:
        justin.insertUpdate('INSERT INTO sites_files_cache SET '
                               'site_id=%d,'
                               'workflow_id=%d,'
                               'stage_id=%d,'
                               'distance=%f,'
                               'file_id=%d,'
                               'rse_id=%d,'
                               'replica_id=%d,'
                               'cache_time=FROM_UNIXTIME(%d)' %
                               (siteID,
                                workflowID,
                                stageID,
                                replicaRow['distance'],
                                replicaRow['file_id'],
                                replicaRow['rse_id'],
                                replicaRow['replica_id'],
                                cutoffTime
                               )
                              )
      except Exception as e:
        print('Failed caching replica info: %s' % str(e), file=sys.stderr)
        continue

  try:
    justin.insertUpdate('INSERT INTO sites_files_cache_state SET '
                          'site_id=%d,'
                          'workflow_id=%d,'
                          'stage_id=%d,'
                          'number_found=%d,'
                          'cache_time=FROM_UNIXTIME(%d)' %
                          (siteID,
                           workflowID,
                           stageID,
                           len(filesSeen),
                           cutoffTime
                          )
                         )
  except Exception as e:
    print('Failed creating new sites_files_cache_state row: %s' % str(e),
          file=sys.stderr)

  # Whatever else happened we clear out any old cache entries
  # for this site/workflow/stage
  try:
    justin.insertUpdate('DELETE FROM sites_files_cache '
                        'WHERE site_id=%d AND workflow_id=%d AND stage_id=%d '
                        'AND cache_time < FROM_UNIXTIME(%d)'
                        % (siteID, workflowID, stageID, cutoffTime))
  except Exception as e:
    print('Delete previous sites_files_cache fails with: ' + str(e), 
          file=sys.stderr)
 
  try:
    justin.insertUpdate('DELETE FROM sites_files_cache_state '
                        'WHERE site_id=%d AND workflow_id=%d AND stage_id=%d '
                        'AND cache_time < FROM_UNIXTIME(%d)'
                        % (siteID, workflowID, stageID, cutoffTime))
  except Exception as e:
    print('Delete previous sites_files_cache_state fails with: ' + str(e), 
          file=sys.stderr)
 
  justin.conn.commit()

# Try to get the file cache service lock or raise an exception if
# unable to do this for any reason, including a timeout
# No try here since we expose exceptions to the caller!
def getCacheLock(lockName, timeoutSeconds):
  row = justin.select(
          'SELECT GET_LOCK("%s", %d) '
          'AS result' % (lockName, timeoutSeconds), justOne = True)

  if not row or 'result' not in row:
    # Something went wrong!
    raise RuntimeError('justin_allocator lock query failure')
  
  if row['result'] == 0:
    # Failed to get lock in time so raise exception
    raise RuntimeError('justin_allocator lock timeout')

  # Success!  
  return

# Release the file cache lock if we can
def releaseCacheLock(lockName):
  try:
    justin.select('SELECT RELEASE_LOCK("%s")' % lockName)
  except Exception as e:
# REMOVE THIS print FOR PROD
    print('Exception releasing lock: ' + str(e))
    pass

# Get a list of cached files for this workflow/stage at this site
# Special treatment of file_id=0 which records if no-cacheable-files
def findCachedFiles(siteID, workflowID, stageID, maxFiles):
  query = (
    "SELECT file_did,sites_files_cache.file_id,lan_pfn,wan_pfn,"
    "rse_name,sites_files_cache.rse_id,distance "
    "FROM sites_files_cache "
    "LEFT JOIN files ON files.file_id=sites_files_cache.file_id "
    "LEFT JOIN replicas ON replicas.replica_id=sites_files_cache.replica_id "
    "LEFT JOIN storages ON storages.rse_id=sites_files_cache.rse_id "
    "WHERE "
    "sites_files_cache.site_id=%d AND "
    "sites_files_cache.workflow_id=%d AND "
    "sites_files_cache.stage_id=%d AND "
    "files.state='unallocated' "
    "ORDER BY sites_files_cache.cache_time DESC,"
    "sites_files_cache.distance,sites_files_cache.file_id "
    "LIMIT %d FOR UPDATE" %
    (siteID, workflowID, stageID, maxFiles))

  return justin.select(query)
  
def findBestFile(jobDict):

  # When there are less cached files than this we update the cache
  minCacheSize = 20

  replicaRows = findCachedFiles(jobDict['site_id'], jobDict['workflow_id'], 
                                jobDict['stage_id'], minCacheSize)

#  DEBUGGING print() STATEMENTS BELOW NEED TO BE REMOVED
#  print('For %s/%d/%d got %d rows' 
#        % (jobDict['site_name'], jobDict['workflow_id'], jobDict['stage_id'], 
#           len(replicaRows)))

  if len(replicaRows) < minCacheSize:    
    # Too few returned: might be time to update the cache?
    try:
      # Check if there just weren't enough available to be cached
      cacheStateRow = justin.select(
        'SELECT number_found,'
        'TIMESTAMPDIFF(SECOND, cache_time, NOW()) AS seconds '
        'FROM sites_files_cache_state '
        'WHERE site_id=%d AND workflow_id=%d and stage_id=%d'
        % (jobDict['site_id'], jobDict['workflow_id'], jobDict['stage_id']),
        justOne = True)
    except Exception as e:
      print('Failed to get number_found: ' + str(e), file=sys.stderr)
      cacheStateRow = None

    print('%s/%d/%d cacheStateRow=%s, len(replicaRows)=%d'
          % (jobDict['site_name'],
             jobDict['workflow_id'],
             jobDict['stage_id'],
             str(cacheStateRow), len(replicaRows)), file=sys.stderr)
    if cacheStateRow is None or \
       cacheStateRow['seconds'] > 300 or \
       cacheStateRow['number_found'] >= minCacheSize:
      # If there are no cache state entries yet,
      # or plenty of files were found, then we update the cache
      # Cache needs to be updated
      print('%s/%d/%d cache needs to be updated'
            % (jobDict['site_name'],
               jobDict['workflow_id'],
               jobDict['stage_id']), file=sys.stderr)
      try:
        getCacheLock('justin_allocator_file_cache', 0)
      except Exception as e:
# THIS EXCEPTION WILL HAPPEN ALL THE TIME NORMALLY SO print() NOT FOR PROD?
        # We didn't get the cache lock so we carry on with what was returned
        print('%s/%d/%d Did not get file cache lock: %s'
              % (jobDict['site_name'],
                 jobDict['workflow_id'],
                 jobDict['stage_id'], str(e)), file=sys.stderr)
      else:
        print('%s/%d/%d Got cache lock, update cache'
              % (jobDict['site_name'],
                 jobDict['workflow_id'],
                 jobDict['stage_id']),
              file=sys.stderr)
        cacheFiles(jobDict['site_id'], jobDict['workflow_id'], 
                   jobDict['stage_id'], jobDict['max_distance'])
        releaseCacheLock('justin_allocator_file_cache')

        # If we had zero results before, rerun the query
        if len(replicaRows) == 0:
          print('Rerun findCachedFiles()', file=sys.stderr)
          replicaRows = findCachedFiles(jobDict['site_id'], 
                                        jobDict['workflow_id'], 
                                        jobDict['stage_id'], 1)

  if len(replicaRows) == 0:
    # No matches found
    print('No matches found', file=sys.stderr)
    return { 'error_message': None,
             'file_did'     : None  }

  try: 
    query = ("UPDATE files SET state='allocated',"
             "allocations=allocations+1,"
             "justin_job_id=" + str(jobDict['justin_job_id']) + " "
             "WHERE state='unallocated' AND "
             "file_id=" + str(replicaRows[0]['file_id'])
            )
    affectedRows = justin.update(query)
  except Exception as e:
    # If anything goes wrong, we stop straightaway
    return { 'error_message': 'Failed recording state change: ' + str(e) }

  if affectedRows != 1:
# CAN WE GET RID OF THE RACE SCENARIO THAT PRODUCES THESE AT 0.3% LEVEL?
# SOME LIMITED RETRYING OF THE OTHER RETURNED FILES?
    print('Failed to allocate the chosen file (%s): already allocated???'
          % str(replicaRows[0]['file_did']), file=sys.stderr)

    # But we return as if no file was found
    return { 'error_message': None,
             'file_did'     : None  }
    
  justin.logEvent(eventTypeID = justin.event_FILE_ALLOCATED,
                     workflowID     = jobDict['workflow_id'],
                     stageID        = jobDict['stage_id'],
                     fileID         = replicaRows[0]['file_id'],
                     justinJobID    = jobDict['justin_job_id'],
                     siteID         = jobDict['site_id'],
                     entryID        = jobDict['entry_id'],
                     rseID          = replicaRows[0]['rse_id'])

  # If storage is at the same site and a lan PFN is given, use it
  if replicaRows[0]['distance'] == 0 and replicaRows[0]['lan_pfn']:
    pfn = replicaRows[0]['lan_pfn']
  else:
    pfn = replicaRows[0]['wan_pfn']

  # The dictionary to return, with the highest priority result
  replica = { 'error_message' : None,
              'file_did'      : replicaRows[0]['file_did'],
              'pfn'           : pfn,
              'rse_name'      : replicaRows[0]['rse_name']
            }

  return replica

# Get an unallocated file from the given workflow+stage
def getFileMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing/invalid jobsub_id in JSON')

  if 'jobscript_secret' not in jsonDict or \
     not justin.stringNoQuotes(jsonDict['jobscript_secret']):
    return httpError(startResponse,
                     '400 Bad Request',
                     'Missing/invalid jobscript secret in JSON')

  # Lookup job details
  jobDict = makeJobDict(jsonDict, jsonDict['jobscript_secret'])

  if jobDict['error_message']:
    return httpError(startResponse,
                     '400 Bad Request', 
                     jobDict['error_message'] + ' in getFileMethod')

  if jobDict['job_state'] != 'started' and \
     jobDict['job_state'] != 'processing':
    return httpError(startResponse,
                     '403 Forbidden', 
                     'Job in wrong state to find file (%s)' % 
                     jobDict['job_state'])

  if jobDict['workflow_state'] != 'running':
    return httpError(startResponse,
                     '404 Not Found', 
                     'Workflow not in running state (%s)' % 
                     jobDict['workflow_state'])

  if not jobDict['user_processing_enabled']:
    return httpError(startResponse,
                     '404 Not Found', 
                     'Processing now disabled for this user')

  if not jobDict['quota_processing_enabled']:
    return httpError(startResponse,
                     '404 Not Found', 
                     'Processing now disabled for this named quota')

  if jobDict['allocated_files'] >= justin.maxFilesPerJob:
    return httpError(startResponse,
                     '404 Not Found', 
                     'Only %d files can be allocated to each job'
                     % justin.maxFilesPerJob)

  if jobDict['ping_rucio_milliseconds'] >= justin.overloadRucioMilliseconds:
    return httpError(startResponse,
                     '503 Service Unavailable', 
                     'File allocations suspended while Rucio is overloaded')

  # Create a stage dictionary with the next file in this stage  
  oneFile = findBestFile(jobDict)

  if oneFile['error_message']:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed finding one file: ' + oneFile['error_message'])
  
  if oneFile['file_did']:
    try:
      # Update heartbeat_time and make sure job is set to processing now
      justin.insertUpdate('UPDATE jobs SET job_state="processing",'
                          'heartbeat_time=NOW(),'
                          'allocated_files=allocated_files+1,'
                          'sent_get_file=TRUE '
                          'WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"'
                         )
      # Note that AWT jobs go to processing when they get the jobscript
      # as they do not use the get_file method implemented here!

      if jobDict['job_state'] == 'started':
        # If just here to update the hearbeat, do not log a state change
        justin.logEvent(eventTypeID = justin.event_JOB_PROCESSING,
                        workflowID  = jobDict['workflow_id'],
                        stageID     = jobDict['stage_id'],
                        justinJobID = jobDict['justin_job_id'],
                        siteID      = jobDict['site_id'],
                        entryID     = jobDict['entry_id']
                       )

    except Exception as e:
       return httpError(startResponse,
                        '500 Internal Server Error',
                        'Unable to update job to processing: ' + str(e))

    if jsonDict['method'] == 'samweb_getnextfile':
      return httpOK(startResponse, oneFile['pfn'])

    else: # method = 'get_file'
      return httpOK(startResponse,
                    oneFile['file_did'] + ' ' +
                    oneFile['pfn'] + ' ' +
                    oneFile['rse_name'])

  # No file eligible to be processed by this job
  try:
    # Update heartbeat_time and make sure job is set to processing now
    justin.insertUpdate('UPDATE jobs '
                        'SET heartbeat_time=NOW(),sent_get_file=TRUE '
                        'WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"'
                       )
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Unable to update job to sent_get_file=TRUE: ' + str(e))

  # Use httpOK() since this does not do a rollback
  return httpOK(startResponse,
                'No eligible file found',
                codeStr = '404 Not Found')

def updateFileProcessing(fileIDList,
                         state, justinJobID, workflowID, stageID):
  # Exceptions in this function must by handled by the caller!

  testFileIDs = []

  for fileID in fileIDList:
    testFileIDs.append('file_id=%d' % fileID)

  # Update the files the job did or did not manage to process
  if testFileIDs:

    if state == 'unprocessed':
      # If already at max allocations, go to failed rather than unallocated!
      query = ('UPDATE files SET '
               'state=IF(allocations < max_allocations,'
               '"unallocated","failed") '
               'WHERE (' + ' OR '.join(testFileIDs) + ') '
               'AND justin_job_id=' + str(justinJobID) + ' '
               'AND workflow_id=' + str(workflowID) + ' '
               'AND stage_id=' + str(stageID) + ' '
               'AND state="allocated"')
    else:
      query = ('UPDATE files SET state="outputting" '
               'WHERE (' + ' OR '.join(testFileIDs) + ') '
               'AND justin_job_id=' + str(justinJobID) + ' '
               'AND workflow_id=' + str(workflowID) + ' '
               'AND stage_id=' + str(stageID) + ' '
               'AND state="allocated"')

    justin.insertUpdate(query)
    
# Deal with the output files needed by the next stage
# Caller must handle exceptions!
def processOutputFiles(outputFiles, patternsDict, 
                       scopeName, workflowID, stageID, justinJobID):

  for (fileName, patternID) in outputFiles:
      
    if '"' in fileName or "'" in fileName:
      continue

    if patternsDict[patternID]['destination'].startswith('https://'):
      justin.insertUpdate('INSERT INTO files SET state="recorded",'
                          'file_did="%s/%s",'
                          'creator_justin_job_id=%d,'
                          'workflow_id=%d,'
                          'creator_stage_id=%d,'
                          'creator_pattern_id=%d,'
                          'stage_id=0' %
                          (patternsDict[patternID]['destination'], fileName,
                           justinJobID, 
                           workflowID, stageID, patternID)
                         )
    else:
      justin.insertUpdate('INSERT INTO files SET state="recorded",'
                          'file_did="%s:%s",'
                          'creator_justin_job_id=%d,'
                          'workflow_id=%d,'
                          'creator_stage_id=%d,'
                          'creator_pattern_id=%d,'
                          'stage_id=%d' %
          (scopeName, fileName, justinJobID, workflowID, stageID, patternID,
           stageID+1 if patternsDict[patternID]['for_next_stage'] else 0)
                         )

def saveAwtResults(justinJobID, siteID, entryID, 
                   jobscriptLog, hasInnerApptainer, requestedGPU):

# REMOVE prints FOR PRODUCTION
#  print('Start saveAwtResults', file=sys.stderr)

  for line in jobscriptLog.splitlines():
    if line.startswith('==awt== '):
      lineSplitted = line.split()
      
      try:
        justin.insertUpdate('UPDATE sites_storages SET '
                            'justin_job_id=%d,read_result=%d,write_result=%d '
                            'WHERE site_id=%d AND '
                            'rse_id=(SELECT rse_id FROM storages '
                            'WHERE rse_name="%s")' 
                            % (justinJobID, 
                               int(lineSplitted[3]),
                               int(lineSplitted[4]),
                               siteID,
                               lineSplitted[2])
                           )

        # non-zero shell error code is True in Python...
        justin.logEvent(eventTypeID = 
                          justin.event_AWT_READ_FAIL if int(lineSplitted[3]) 
                          else justin.event_AWT_READ_OK,
                        workflowID = justin.awtWorkflowID,
                        stageID = 1,
                        justinJobID = justinJobID,
                        siteID = siteID,
                        entryID = entryID,
                        rseName = lineSplitted[2]
                       )

        # non-zero shell error code is True in Python...
        justin.logEvent(eventTypeID = 
                          justin.event_AWT_WRITE_FAIL if int(lineSplitted[4]) 
                          else justin.event_AWT_WRITE_OK,
                        workflowID = justin.awtWorkflowID,
                        stageID = 1,
                        justinJobID = justinJobID,
                        siteID = siteID,
                        entryID = entryID,
                        rseName = lineSplitted[2]
                       )

      except Exception as e:
        print('Update awt results fails with %s' % str(e), 
              file=sys.stderr)

  try:
    if requestedGPU:
      gpuStr = ', last_awt_gpu_time=NOW(), last_awt_gpu_job_id=%d' \
               % justinJobID
    else:
      gpuStr = ''

    justin.insertUpdate('UPDATE sites SET '
                        'last_awt_time=NOW(), last_awt_job_id=%d%s '
                        'WHERE site_id=%d' % (justinJobID, gpuStr, siteID))

    justin.insertUpdate('UPDATE entries SET '
                        'last_awt_time=NOW(), last_awt_job_id=%d '
                        'WHERE entry_id=%d' % (justinJobID, entryID))

  except Exception as e:
    print('Update awt last time fails with %s' % str(e), 
          file=sys.stderr)

# Before uploading output files the wrapper jobs record any results from 
# the jobscript, but just puts them in the "outputting" state
def recordResultsMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing jobsub_id in JSON')

  # Check jsonDict contains required lists (can be empty)
  for name in ['processed_dids', 'processed_pfns']:
    if name not in jsonDict:
      return httpError(startResponse,
                       '400 Bad Request',
                       'Missing value (%s) in JSON' % name)

    for fileDID in jsonDict[name]:
      if not justin.stringNoQuotes(fileDID):
        return httpError(startResponse,
                         '400 Bad Request',
                         'Invalid DID or PFN in ' + name)

  if 'output_files' not in jsonDict:
    return httpError(startResponse,
                     '400 Bad Request',
                     'Missing value output_files in JSON')

  for fileName,patternID in jsonDict['output_files']:
    if not justin.stringNoQuotes(fileName):
      return httpError(startResponse,
                       '400 Bad Request',
                       'Invalid filename ' + fileName)

  responseDict = {}
      
  try:
    query = ('SELECT jobs.justin_job_id, jobs.workflow_id, jobs.stage_id,'
             'jobs.site_id, jobs.entry_id, users.access_token, '
             'jobs.has_inner_apptainer, jobs.job_state, workflows.state, '
             'scope_name, jobs.requested_gpu, scopes.quota_id, '
             'quota_name, named_quotas.processing_enabled '
             'FROM jobs '
             'LEFT JOIN workflows ON workflows.workflow_id=jobs.workflow_id '
             'LEFT JOIN users ON users.user_id=workflows.user_id '
             'LEFT JOIN scopes ON workflows.scope_id=scopes.scope_id '
             'LEFT JOIN named_quotas ON named_quotas.quota_id=scopes.quota_id '
             'WHERE jobs.jobsub_id="' + jsonDict['jobsub_id'] + '"')

    row = justin.select(query, justOne = True)

    justinJobID       = int(row['justin_job_id'])
    workflowID        = int(row['workflow_id'])
    stageID           = int(row['stage_id'])
    scopeName         = row['scope_name']
    siteID            = int(row['site_id'])
    entryID           = int(row['entry_id'])
    hasInnerApptainer = row['has_inner_apptainer']
    requestedGPU      = row['requested_gpu']
    jobState          = row['job_state']
    responseDict['user_access_token'] = row['access_token']
    quotaID           = row['quota_id']
  except:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed finding job')

  # This is mainly to catch outputs from workflows what have been aborted
  # by the user
  if row['state'] != 'running' and row['state'] != 'paused':
    return httpError(startResponse, 
                     '410 Gone', 
                     'Cannot record results as workflow now in %s state' 
                     % row['state'])
    
  if not row['processing_enabled']:
    return httpError(startResponse,
                  '410 Gone', 
                  'Cannot record results as processing now not enabled for %s'
                  % row['quota_name'])

  if jobState != 'started' and jobState != 'processing':
    return httpError(startResponse, 
                     '410 Gone', 
                     'Cannot record results as job already in %s state' 
                     % jobState)

  try:
    justin.insertUpdate(
        'UPDATE jobs_logs SET '
        'jobscript_log="%s",saved_time=NOW() WHERE justin_job_id=%d' %
        (jsonDict['jobscript_log'].replace('\\','\\\\').replace('"','\\"'),
         justinJobID)  )

  except Exception as e:
    # Try to keep going. 
    print('Failed to save jobscript log for %s: %s' 
          % (str(justinJobID), str(e)), file=sys.stderr)

  if workflowID == justin.awtWorkflowID:
    saveAwtResults(justinJobID, 
                   siteID, 
                   entryID,
                   jsonDict['jobscript_log'],
                   hasInnerApptainer,
                   requestedGPU)

  # Create an ordered output RSE list specific to this stage, used for logs
  # and any output files going to RSEs
  try:
    # 10000000000 bytes = 10 GB is buffer space to allow for other jobs
    query = ('SELECT rse_name,lan_write_scheme,wan_write_scheme,distance '
             'FROM stages_output_storages '
             'LEFT JOIN storages '
             'ON stages_output_storages.rse_id=storages.rse_id '
             'LEFT JOIN sites_storages '
             'ON (stages_output_storages.rse_id=sites_storages.rse_id '
             'AND sites_storages.site_id=%d) '
             'LEFT JOIN storages_quotas '
             'ON storages_quotas.quota_id=%d '
             'AND storages_quotas.rse_id=storages.rse_id '
             'WHERE '
             '(bytes + 10000000000 < bytes_limit) AND '
             'stages_output_storages.workflow_id=%d AND '
             'stages_output_storages.stage_id=%d AND '
             'sites_storages.distance IS NOT NULL AND '
             'storages.rucio_write AND '
             'storages.justin_write AND '
             'storages.occupancy < 1.0 AND '
             'NOT storages.decommissioned '
             'ORDER BY stages_output_storages.preferred DESC,'
             'distance,storages.occupancy,RAND()'
             % (siteID, quotaID, workflowID, stageID)
            )

    rseRows = justin.select(query)
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed getting output RSEs list: ' + str(e))

  responseDict['output_rses'] = []
  for rseRow in rseRows:
    responseDict['output_rses'].append((rseRow['rse_name'],
                                        rseRow['lan_write_scheme'] 
                                        if rseRow['distance'] == 0 
                                        else rseRow['wan_write_scheme']))

  if 'jobscript_exit' in jsonDict and jsonDict['jobscript_exit'] != 0:
    # Jobscript said it failed by exiting with non-zero value

    # Record job as finished due to jobscript_error
    try:
      justin.insertUpdate('UPDATE jobs SET '
                          'job_state="jobscript_error",'
                          'jobscript_exit=%d,'
                          'heartbeat_time=NOW(),'
                          'finished_time=NOW() '
                          'WHERE jobsub_id="%s"' %
                          (jsonDict['jobscript_exit'],
                           jsonDict['jobsub_id']))

      justin.logEvent(eventTypeID    = justin.event_JOB_SCRIPT_ERROR,
                      workflowID     = workflowID,
                      stageID        = stageID,
                      justinJobID    = justinJobID,
                      jobscriptExit  = jsonDict['jobscript_exit'],
                      siteID         = siteID,
                      entryID        = entryID
                     )

    except Exception as e:
      return httpError(startResponse,
                       '500 Internal Server Error',
                       'Failed updating job: ' + str(e))
     
    # Do file allocation resets
    try:
      justin.insertUpdate('UPDATE files SET '
                          'state=IF(allocations < max_allocations,' 
                          '"unallocated","failed") '
                          'WHERE justin_job_id=%d AND '
                          '(state="allocated" OR state="outputting")' 
                          % justinJobID)
    except Exception as e:
      return httpError(startResponse,
                       '500 Internal Server Error',
                       'Failed resetting allocated files: ' + str(e))

    # Return 200 OK and a partially filled in resonseDict
    return httpOK(startResponse, 
                  json.dumps(responseDict))

  try:
    # Use the event records to find the right replicas for this job
    query = ('SELECT events.file_id,file_did,wan_pfn,lan_pfn '
             'FROM events '
             'LEFT JOIN replicas ON (replicas.file_id=events.file_id AND '
             'replicas.rse_id=events.rse_id) '
             'LEFT JOIN files ON events.file_id=files.file_id '
             'WHERE events.justin_job_id=%d AND events.event_type_id=%d'
             % (justinJobID, justin.event_FILE_ALLOCATED))
             
    jobFiles = justin.select(query)

  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed getting files for the job: ' + str(e))

  processedFileIDs   = []
  unprocessedFileIDs = []

  for jobFile in jobFiles:
    if jobFile['file_did'] in jsonDict['processed_dids'] or \
       jobFile['wan_pfn'] in jsonDict['processed_pfns'] or \
       (jobFile['lan_pfn'] and 
        jobFile['lan_pfn'] in jsonDict['processed_pfns']):
      processedFileIDs.append(jobFile['file_id'])

      justin.logEvent(eventTypeID = justin.event_FILE_PROCESSED,
                      workflowID  = workflowID,
                      stageID     = stageID,
                      justinJobID = justinJobID,
                      siteID      = siteID,
                      entryID     = entryID,
                      fileID      = jobFile['file_id']
                     )
    else:
      print('Set unprocessed',jsonDict['jobsub_id'],
            jobFile['file_did'],
            jobFile['wan_pfn'], jobFile['lan_pfn'], 
            str(jsonDict['processed_pfns']), file=sys.stderr)
      unprocessedFileIDs.append(jobFile['file_id'])

  # For processed inputs, we just record they got as far as outputting
  # the corresponding output files, for now 
  try:
    updateFileProcessing(processedFileIDs,
                         'outputting',
                         justinJobID, workflowID, stageID)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed updating file statuses: ' + str(e))

  # For unprocessed inputs, we can put them straight back into the
  # unprocessed states (unallocated or failed)
  try:
    updateFileProcessing(unprocessedFileIDs,
                         'unprocessed',
                         justinJobID, workflowID, stageID)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed updating file statuses: ' + str(e))

  # Get the patterns
  try:
    query = ("SELECT destination,rse_expression,"
             "for_next_stage,pattern_id,number_files,lifetime_seconds "
             "FROM stages_outputs "
             "WHERE stages_outputs.workflow_id=%d "
             "AND stages_outputs.stage_id=%d" 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    patternRows = justin.select(query)
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed getting patterns list: ' + str(e))

  # First pass to count how many files per destination already, allowing for
  # multiple patterns outputting to the same destination
  destinationCounts = {}
  for patternRow in patternRows:
   if patternRow['destination'] not in destinationCounts:
     destinationCounts[patternRow['destination']]  = patternRow['number_files']
   else:
     destinationCounts[patternRow['destination']] += patternRow['number_files']
    
  # Create a list of the output file patterns
  responseDict['patterns'] = {}

  for patternRow in patternRows:
    patternID = int(patternRow['pattern_id'])
    responseDict['patterns'][patternID] = {}
    if patternRow['destination'] in destinationCounts:
      numberFiles = destinationCounts[patternRow['destination']]
    else:
      numberFiles = 0

    if patternRow['destination'].startswith('https://'):
      responseDict['patterns'][patternID]['destination'] = \
        patternRow['destination'] + \
        '/%03d' % (1 + int(numberFiles / justin.filesPerNumberedDestination))

    else:
      responseDict['patterns'][patternID]['destination'] = \
        patternRow['destination'] 

      responseDict['patterns'][patternID]['destination_number'] = \
        'n%03d' % (1 + int(numberFiles / justin.filesPerNumberedDestination))

      responseDict['patterns'][patternID]['for_next_stage'] \
             = patternRow['for_next_stage']

      responseDict['patterns'][patternID]['lifetime_seconds'] \
             = patternRow['lifetime_seconds']

      responseDict['patterns'][patternID]['rse_expression'] \
             = patternRow['rse_expression']

  # List of datasets already created for this workflow/stage
  try:
    query = ("SELECT dataset_did "
             "FROM workflows_datasets "
             "WHERE workflow_id=%d AND stage_id=%d" 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    datasetRows = justin.select(query)
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed getting list of datasets: ' + str(e))

  responseDict['existing_dataset_dids'] = []
  for datasetRow in datasetRows:
    responseDict['existing_dataset_dids'].append(datasetRow['dataset_did'])

  # Record output files

  try:
    processOutputFiles(jsonDict['output_files'], responseDict['patterns'],
                       scopeName, workflowID, stageID, justinJobID)
  except MySQLdb.IntegrityError as e:
    return httpError(startResponse, 
                     '409 Duplicate output file name', 
                     'Duplicate output file name: ' + str(e))
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed recording output files: ' + str(e))

  try:
    jobscriptRealSeconds = int(float(jsonDict['jobscript_real_seconds']))
    jobscriptCpuSeconds  = int(float(jsonDict['jobscript_user_seconds']) +
                               float(jsonDict['jobscript_sys_seconds']))
    jobscriptMaxRssBytes = int(jsonDict['jobscript_max_rss_kb']) * 1024
  except:
    # We can produce an error if this fails in future...
    jobscriptRealSeconds = 0
    jobscriptCpuSeconds  = 0
    jobscriptMaxRssBytes = 0

  try:
    justin.insertUpdate('UPDATE jobs SET job_state="outputting",'
                        'heartbeat_time=NOW(),'
                        'outputting_time=NOW(),'
                        'jobscript_real_seconds=%d,'
                        'jobscript_cpu_seconds=%d,'
                        'jobscript_max_rss_bytes=%d '
                        'WHERE jobsub_id="%s"'
                        % (jobscriptRealSeconds,
                           jobscriptCpuSeconds,
                           jobscriptMaxRssBytes,
                           jsonDict['jobsub_id']))

    justin.logEvent(eventTypeID = justin.event_JOB_OUTPUTTING,
                    workflowID  = workflowID,
                    stageID     = stageID,
                    justinJobID = justinJobID,
                    siteID      = siteID,
                    entryID     = entryID
                   )

  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed updating jobs: ' + str(e))

  return httpOK(startResponse, 
                json.dumps(responseDict))

# Return a list of file DIDs allocated to this job.
# This can be used by jobscripts which don't have a keep of input
# DIDs. For example if running lar in legacy sam-web mode.
def getAllocatedFilesMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing jobsub_id in JSON')
      
  try:
    query = ('SELECT justin_job_id '
             'FROM jobs WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"')

    row = justin.select(query, justOne = True)
    justinJobID  = int(row['justin_job_id'])
  except Exception as e:
    return httpError(startResponse,
                     '404 Not Found',
                     'Failed to find job: ' + str(e))

  try:
    # Use the event records to find the right replicas for this job
    query = ('SELECT file_did FROM events '
             'LEFT JOIN files ON events.file_id=files.file_id '
             'WHERE events.justin_job_id=%d AND events.event_type_id=%d'
             % (justinJobID, justin.event_FILE_ALLOCATED))
             
    jobFiles = justin.select(query)

  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed getting files for the job: ' + str(e))

  outputString = ''
  for jobFile in jobFiles:
    outputString += jobFile['file_did'] + '\n'

  return httpOK(startResponse, outputString)

# Legacy support for samweb's updateFileStatus method 
# Only used for putting "consumed" files into the justin "outputting" state
def samwebUpdateFileStatusMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing jobsub ID in workflow URI')

  try:
   print('==samwebUpdateFileStatusMethod ' + str(jsonDict), file=sys.stderr)
  except:
   pass

  try:
    status = jsonDict['status'][0]
  except:
    return httpOK(startResponse, '')

  if status != 'consumed':
    # Anything other than 'consumed' status just gets an OK
    return httpOK(startResponse, '')

  try:
    pfn = jsonDict['filename'][0]

    if not justin.stringNoQuotes(pfn):
      raise
  except:
    return httpError(startResponse, '400 Bad Request', 'Valid filename missing')
      
  try:
    query = ('SELECT justin_job_id,workflow_id,stage_id '
             'FROM jobs WHERE jobsub_id="%s"' % jsonDict['jobsub_id'])

    row = justin.select(query, justOne = True)

    justinJobID  = int(row['justin_job_id'])
    workflowID = int(row['workflow_id'])
    stageID   = int(row['stage_id'])
  except:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed finding job')

  try:
    query = ('SELECT files.file_id FROM files '
             'LEFT JOIN replicas ON replicas.file_id=files.file_id '
             'WHERE files.workflow_id=%d AND files.stage_id=%d '
             'AND (wan_pfn="%s" OR lan_pfn="%s") '
             'ORDER BY files.file_id LIMIT 1' 
             % (workflowID, stageID, pfn, pfn))

    fileRow = justin.select(query, justOne=True)
    fileID = fileRow['file_id']
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed finding file from PFN: ' + str(e))

  try:
    query = ('UPDATE files SET state="outputting" '
             'WHERE file_id=%d '
             'AND justin_job_id=%d '
             'AND workflow_id=%d '
             'AND stage_id=%d '
             'AND state="allocated"'
             % (fileID, justinJobID, workflowID, stageID))

    justin.insertUpdate(query)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed updating file statuses: ' + str(e))

  return httpOK(startResponse, '')

# After the output files have been successfully uploaded, the wrapper job
# comes back with this method which changes the state of input files from
# outputting to processed, and of output files from recorded to finding.
def confirmResultsMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse,
                     '400 Bad Request',
                     'Missing jobsub_id in JSON')

  try:
    query = ('SELECT justin_job_id,workflow_id,stage_id,site_id,entry_id,'
             'job_state FROM jobs '
             'WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"')

    row = justin.select(query, justOne = True)

    justinJobID  = int(row['justin_job_id'])
    workflowID   = int(row['workflow_id'])
    stageID      = int(row['stage_id'])
    siteID       = int(row['site_id'])
    entryID      = int(row['entry_id'])
    jobState     = row['job_state']
  except:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed finding job')

  if jobState != 'outputting':
    return httpError(startResponse, 
                     '410 Gone',
                     'Cannot confirm results as job already in %s state'
                     % jobState)

  try:
    justin.insertUpdate('UPDATE files '
                        'SET state="processed",processed_time=NOW(),'
                        'processed_hour=FLOOR(UNIX_TIMESTAMP()/3600),'
                        'processed_site_id=%d '
                        'WHERE state="outputting" AND justin_job_id=%d' 
                        % (siteID, justinJobID))
  except:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Updating processed input files')

  try:
    fileRows = justin.select('SELECT file_did,file_id,files.creator_stage_id,'
                             'creator_pattern_id,destination,files.stage_id '
                             'FROM files '
                             'LEFT JOIN stages_outputs '
                             'ON stages_outputs.workflow_id=files.workflow_id '
                             'AND stages_outputs.stage_id=files.creator_stage_id '
                             'AND stages_outputs.pattern_id=creator_pattern_id '
                             'WHERE files.state="recorded" AND '
                             'creator_justin_job_id=%d' % justinJobID)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Getting recorded output files: ' + str(e))

  for fileRow in fileRows:
    if fileRow['file_did'].startswith('https://'):
      fileName = fileRow['file_did'].split('/')[-1]
    else:
      fileName = fileRow['file_did'].split(':')[1]

    if fileName not in jsonDict['output_files']:
      # Recorded but not actually uploaded!
      # SHOULD THIS HAVE AN EVENT? SKIP SILENTLY FOR NOW
      continue
  
    try:
      try:
        rseName = jsonDict['output_files'][fileName]['rse_name']
      except:
        rseName = None

      try:
        sizeBytes = jsonDict['output_files'][fileName]['size_bytes']
        seconds   = jsonDict['output_files'][fileName]['seconds']
      except:
        sizeBytes = 0
        seconds   = 0.0
      
      justin.insertUpdate('UPDATE files SET state="%s",size_bytes=%d '
                          'WHERE file_id=%d' %
                       ('finding' if (fileRow['stage_id'] > 0) else 'output',
                          sizeBytes,
                          fileRow['file_id']))

      justin.insertUpdate('UPDATE stages_outputs '
                          'SET number_files=number_files+1 '
                          'WHERE workflow_id=%d AND stage_id=%d '
                          'AND pattern_id=%d' % 
                          (workflowID, stageID, fileRow['creator_pattern_id']))

      justin.logEvent(eventTypeID = justin.event_FILE_CREATED,
                      workflowID = workflowID,
                      stageID = stageID,
                      fileID = fileRow['file_id'],
                      justinJobID = justinJobID,
                      siteID = siteID,
                      entryID = entryID,
                      rseName = rseName,
                      seconds = seconds
                     )

    except Exception as e:
      return httpError(startResponse,
                       '500 Internal Server Error',
                       'Updating recorded output file: ' + str(e))
  try:
    numProcessedRow = justin.select('SELECT '
             '(SELECT sent_get_file FROM jobs WHERE justin_job_id=%d) '
             'AS sent_get_file,'
             '(SELECT COUNT(*) FROM events WHERE '
             'events.justin_job_id=%d AND '
             'events.event_type_id=%d) AS num_processed,'
             '(SELECT COUNT(*) FROM events WHERE '
             'events.justin_job_id=%d AND '
             'events.event_type_id=%d) AS num_allocated'
             % (justinJobID,
                justinJobID, justin.event_FILE_PROCESSED,
                justinJobID, justin.event_FILE_ALLOCATED),
                                    justOne = True)
    sentGetFile  = bool(numProcessedRow['sent_get_file'])
    numProcessed = numProcessedRow['num_processed']
    numAllocated = numProcessedRow['num_allocated']
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed to count FILE_PROCESSED events: ' + str(e))

  if 'created_dataset_dids' in jsonDict:
    for datasetDID in jsonDict['created_dataset_dids']:
      try:
        justin.insertUpdate('INSERT IGNORE INTO workflows_datasets '
                            'SET workflow_id=%d,stage_id=%d,'
                            'dataset_did="%s"' 
                            % (workflowID, stageID, datasetDID))
      except Exception as e:
        # Carry on if there are errors: jobs will retry creation/reporting
        print('Failed inserting dataset: ' + str(e), file=sys.stderr)

  # Finish up and update job status
  try:
    # Jobs only finish in the none_processed state if any files were allocated
    # or if they never even asked for any, but not for AWT jobs
    justin.insertUpdate('UPDATE jobs SET job_state="%s",'
                        'heartbeat_time=NOW(),'
                        'finished_time=NOW() '
                        'WHERE justin_job_id=%d AND job_state="outputting"'
                        % ('none_processed'
                           if ((numProcessed==0 and numAllocated) 
                           or not sentGetFile) 
                           and (workflowID != justin.awtWorkflowID)
                           else 'finished',
                           justinJobID))

    justin.logEvent(eventTypeID = justin.event_JOB_FINISHED,
                    workflowID = workflowID,
                    stageID = stageID,
                    justinJobID = justinJobID,
                    siteID = siteID,
                    entryID = entryID
                   )
  except:
    return httpError(startResponse, 
                     '500 Internal Server Error',
                     'Updating job state to finished')

  return httpOK(startResponse, '')

def checkSecretHash(environ, jsonDict):
  # Check if the hash provided is based on the job's secret
  # or raise an exception

  try:
    secretHash = jsonDict['secret_hash']
    secretTime = int(jsonDict['secret_time'])

    if not secretHash or not secretTime:
      raise

  except Exception as e:
    print('Valid hash of job secret not provided: ' + str(e), file=sys.stderr)
    raise RuntimeError('Forbidden - valid hash of job secret not provided')

  nowTime = int(time.time())
  if secretTime > nowTime + 60:
    raise RuntimeError('Forbidden - secret hash expired (secret=%d,now=%d)'
                       % (secretTime, nowTime ) )

  try:
    row = justin.select('SELECT justin_job_secret,job_state FROM jobs '
                        'WHERE jobsub_id="%s"' % jsonDict['jobsub_id'],
                        justOne = True)
    justinJobSecret = row['justin_job_secret']
    jobState        = row['job_state']
  except:
    raise RuntimeError('Forbidden - failed to get job secret for %s' 
                       % jsonDict['jobsub_id'])
 
  hash = hmac.new(bytes(justinJobSecret, 'UTF-8'),
                                     (jsonDict['method'] +
                                      str(secretTime) +
                                      jsonDict['jobsub_id']
                                     ).encode(),
                                     hashlib.sha256).hexdigest()

  print('method=%s secret_hash=%s hash=%s secret_time=%d now=%d' 
        % (jsonDict['method'], secretHash, hash, secretTime, nowTime), 
           file=sys.stderr)

  if jsonDict['secret_hash'] != secretHash:
    raise RuntimeError('Forbidden - invalid hash given')

  return jobState

#def getAllocatorLock(timeoutSeconds):
#  # Try to get the workflow allocator service lock or raise an exception if
#  # unable to do this for any reason, including a timeout
#  # No try here since we expose exceptions to the caller!
#  justin.cur.execute('SELECT GET_LOCK("justin_allocator", %d) AS result' %
#                     timeoutSeconds)
#
#  row =  justin.cur.fetchone()
#
#  if not row or 'result' not in row:
#    # Something went wrong!
#    raise RuntimeError('justin_allocator lock query failure')
#  
#  if row['result'] == 0:
#    # Failed to get lock in time so raise exception
#    raise RuntimeError('justin_allocator lock timeout')
#
#  # Success!  
#  return

#
# Entry point from mod_wsgi
#
def application(environ, startResponse):

  justin.wsgiCallsCount += 1
  print('Call count (pid=%d): %d' % (os.getpid(), justin.wsgiCallsCount), 
        file=sys.stderr)

  justin.readConf()

  # Quickly reject random GETs etc (if not handled by Apache already)
  if environ['REQUEST_METHOD'] != 'POST':
    return httpError(startResponse,
                     '405 Method not allowed', 
                     'We only support POST')

  try:
    # True should provoke a reconnection attempt.
    # See https://github.com/farcepest/MySQLdb1/blob/master/_mysql.c#L1978
    # (Not sure if there is a more authoritative source for this API.)
    justin.conn.ping(reconnect = True)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'DB connection lost and cannot reconnect: ' + str(e))

  # Avoid leftovers from partial, failed transactions in this instance
  # and reset autocommit
  justin.conn.rollback()
  justin.conn.autocommit(False)

  justin.checkProxyStrings()

  if environ['REQUEST_URI'].startswith('/api/get-classads'):
    return getClassadsCall(startResponse, environ)

  if environ['REQUEST_URI'].startswith('/api/samweb/'):
    # Legacy samweb support: put values into dictionary
    try:
      inputLength = int(environ.get('CONTENT_LENGTH', '0'))
      inputString = environ['wsgi.input'].read(inputLength).decode()
      jsonDict = urllib.parse.parse_qs(inputString)
      print('inputString ' + inputString, file=sys.stderr)

      # URIs like /api/samweb/JOBSUBJOBID/SECRET/processes/N/METHOD
      uriSplit = environ['REQUEST_URI'].split('/')
      jsonDict['method']           = 'samweb_' + uriSplit[-1].lower()
      jsonDict['jobsub_id']        = str(uriSplit[3])
      jsonDict['jobscript_secret'] = str(uriSplit[4])

    except Exception as e:
      return httpError(startResponse, 
                       '400 Bad Request', 
                       'Failed to read and parse workflow: ' + str(e))
  else:
    # Standard justIN API based on JSON
    try:
      inputLength = int(environ.get('CONTENT_LENGTH', '0'))
      inputString = environ['wsgi.input'].read(inputLength)
      jsonDict = json.loads(inputString)
    except Exception as e:
      return httpError(startResponse, 
                       '400 Bad Request', 
                       'Failed to read and parse JSON')

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse,
              '400 Bad Request', 
              'Missing jobsub_id in JSON')

  # Check jsonDict specifies a method
  if 'method' not in jsonDict:
    return httpError(startResponse,
                     '400 Bad Request',
                     'Missing method in JSON')  

  # Legacy samweb updateFileStatus method (only "consumed" does anything)
  if jsonDict['method'] == 'samweb_updatefilestatus' or \
     jsonDict['method'] == 'samweb_setstatus':
    return samwebUpdateFileStatusMethod(startResponse, jsonDict)

  # Return list of file DIDs allocated to this job
  if jsonDict['method'] == 'get_allocated_files':
    return getAllocatedFilesMethod(startResponse, jsonDict)

  # Get one or more files to process
  # This workflow comes from jobscripts using secrets so we 
  # do not run checkSecretHash() as getFileMethod() checks the secret
  if jsonDict['method'] == 'get_file' or \
     jsonDict['method'] == 'samweb_getnextfile':
    return getFileMethod(startResponse, jsonDict)

  # All other methods are called by the wrapper job using its secret hash
  # which is checked with checkSecretHash()
  try:
    jobState = checkSecretHash(environ, jsonDict)
  except Exception as e:
    return httpError(startResponse, '403 Forbidden', str(e))

  if jobState == 'stalled':
    return httpError(startResponse, '403 Forbidden', 'Cannot update stalled jobs')

  # Get the details of the stage for the job to work on
  if jsonDict['method'] == 'get_jobscript':
    if jobState == 'submitted':
      return getJobscriptMethod(startResponse, jsonDict, environ)
    else:
      print('Duplicate get_jobscript received for job in state %s: %s' 
             % (jobState, str(jsonDict)), file=sys.stderr)
      return httpError(startResponse, 
                 '403 Forbidden', 
                 'Job is no longer in submitted state - duplicate/retry?')

  # Record heartbeats from the wrapper jobs
  if jsonDict['method'] == 'send_heartbeat':
    return sendHeartbeatMethod(startResponse, jsonDict)

  # Job aborted
  if jsonDict['method'] == 'job_aborted':
    return jobAbortedMethod(startResponse, jsonDict)
  
  # Record results of processing files
  if jsonDict['method'] == 'record_results':
    return recordResultsMethod(startResponse, jsonDict)

  # Confirm that uploads went ok
  if jsonDict['method'] == 'confirm_results':
    return confirmResultsMethod(startResponse, jsonDict)

  # Otherwise an error
  return httpError(startResponse, 
                   '400 Bad Request', 
                   'Method in JSON not recognised')

