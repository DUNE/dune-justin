#
# justin-wsgi-allocator - justIN allocator API service
#
# Copyright 2013-24, Andrew McNab for the University of Manchester
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# This WSGI script relies on justin-api-import-script being run by
# the mod_wsgi directive WSGIImportScript

import os
import io
import re
import sys
import time
import stat
import json
import hmac
import secrets
import random
import string
import urllib
import hashlib
import M2Crypto
#import htcondor

import justin

#sitesRankCacheTimeout = 300

def httpError(startResponse, code, message):

  try:
    justin.conn.rollback()
  except:
    pass

  print('justin-wsgi-allocator fails with %s (%s)' % 
        (code, str(message)), file=sys.stderr)

  startResponse(code,
                [('Content-type',   'text/plain'),
                 ('Content-length', str(len(message)))
                ])

  return [message.encode('UTF-8')]

def httpOK(startResponse, outputString):

  try:
    justin.conn.commit()
  except Exception as e:
    code = '500 Internal Server Error'
    outputString = 'Database commit fails: ' + str(e)
  else:
    code = '200 OK'

  startResponse(code,
                [('Content-type',   'text/plain'),
                 ('Content-length', str(len(outputString)))
                ])

  return [outputString.encode('UTF-8')]

# Work out an ordered list of stages suited to this entry
def updateSitesRankCache(siteName):

  # We use this to time queries
  startTime = time.time()

  try:
    siteRow = justin.select('SELECT site_id FROM sites '
                            'WHERE site_name="%s" AND enabled' 
                            % siteName, justOne = True)
  except Exception as e: 
    print('updateSitesRankCache() site query fails: ' + str(e), 
          file=sys.stderr)
    return None

  if not siteRow:
    print('updateSitesRankCache() empty site query', file=sys.stderr)
    return None

  try:
    # for the nearest 10 unprocessed files for each stage, calculate the
    # the average distance, and return that distance for the nearest,
    # highest priority,oldest stages
    query = (
      'SELECT stages.stage_priority,'
      'CONCAT("w",workflows.workflow_id,"s",stages.stage_id) AS ws,'
        '(SELECT AVG(sites_storages.distance) '
        'FROM files '
        'LEFT JOIN replicas ON replicas.file_id=files.file_id '
        'LEFT JOIN sites_storages ON sites_storages.rse_id=replicas.rse_id '
        'AND sites_storages.site_id=%d '
        'WHERE files.state="unallocated" '
        'AND files.workflow_id=stages.workflow_id '
        'AND files.stage_id=stages.stage_id '
        'AND sites_storages.distance IS NOT NULL '
        'AND sites_storages.distance <= stages.max_distance '
        'ORDER BY sites_storages.distance '
        'LIMIT 10) AS avg_distance '
      'FROM workflows '
      'LEFT JOIN stages ON stages.workflow_id=workflows.workflow_id '
      'WHERE workflows.workflow_id<>%d '
      'AND workflows.state="running" '
      'AND stages.stage_priority IS NOT NULL '
      'ORDER BY avg_distance,stages.stage_priority DESC,'
      'workflows.workflow_id LIMIT 100'
      % (siteRow['site_id'], justin.awtWorkflowID))

    print('updateSitesRankCache() query: ' + query, file=sys.stderr)
    stages = justin.select(query)
    print('updateSitesRankCache() stages: ' + str(stages), file=sys.stderr)
  except Exception as e:
    print('updateSitesRankCache() fails: ' + str(e), file=sys.stderr)
    return None

  ranks = []
  for stage in stages:
    # the rank of each stage is 100-distance as the integer plus the 
    # priority/100 where priority is 1-99 as the fraction. these ranks are
    # used unmodified as HTCondor ranks with higher being higher priority
    if stage['avg_distance'] is not None:
      ranks.append('%s=%.2f' %
          (stage['ws'], 
           100.0 - int(stage['avg_distance']) + stage['stage_priority']/100.0))

  rankText = '[' + ';'.join(ranks) + ']'

  try:
    justin.insertUpdate('REPLACE INTO sites_ranks_cache '
       'SET site_id=%d,rank_text="%s",cache_time=NOW(),query_seconds=%f'
       % (siteRow['site_id'], rankText, time.time() - startTime))
  except Exception as e:
    print('updateSitesRankCache() REPLACE fails: ' + str(e), file=sys.stderr)
    return None

  return rankText

# Return Class Ads to for glideins to publish
def getClassadsCall(startResponse, environ):

  try:
    inputLength = int(environ.get('CONTENT_LENGTH', '0'))
    inputString = environ['wsgi.input'].read(inputLength).decode()
  except Exception as e:
    return httpError(startResponse, 
                     '400 Bad Request', 'Failed to get inputString: ' + str(e))

  try:
    i = inputString.index('GLIDEIN_DUNESite ')
    siteName = inputString[i+17:].split('\n')[0]
  except Exception as e:
    return httpError(startResponse, 
                     '400 Bad Request', 'Failed to get siteName: ' + str(e))

  if not justin.stringNoQuotes(siteName):
    raise RuntimeError('Quotes in siteName')

  try:
    # This is only used to create a ClassAd, not in the caching
    i = inputString.rindex('HAS_INNER_APPTAINER=')
    hasInnerApptainer = bool(inputString[i+20:].split('\n')[0])
  except Exception as e:
    return httpError(startResponse, 
               '400 Bad Request', 'Failed to get hasInnerApptainer: ' + str(e))

  try:
    row = justin.select('SELECT rank_text,'
                        'TIMESTAMPDIFF(SECOND, cache_time, NOW()) AS seconds,'
                        'sites.site_id FROM sites '
                        'LEFT JOIN sites_ranks_cache '
                        'ON sites_ranks_cache.site_id=sites.site_id '
                        'WHERE sites.site_name="%s" AND cache_time IS NOT NULL'
                        % siteName, justOne = True)
  except Exception as e:
    return httpError(startResponse, '404 Not Found', 'Not Found: ' + str(e))

  if row is None:
    rankText     = '[]'
    cacheSeconds = justin.sitesRankCacheTimeout + 1
  else:
    rankText     = row['rank_text']
    cacheSeconds = row['seconds']

  if cacheSeconds > justin.sitesRankCacheTimeout:
    try:
      getCacheLock('justin_allocator_ranks_cache', 0)
    except:
      pass
    else:
      rankText = updateSitesRankCache(siteName)
      releaseCacheLock('justin_allocator_ranks_cache')

  if justin.proDev == 'dev':
    justinRank = ('isUndefined(Target.JUSTIN_DEV_Stage)'
                    '?My.JUSTIN_PRO_Rank:My.JUSTIN_DEV_Rank')
  else:
    justinRank = 'My.JUSTIN_PRO_Rank'

   # Return ClassAds to be published by the pilot/glidein
  #Â Non-justIN jobs get rank 0 or 101
  # If a justIN job has a stage not listed in the rankText dictionary
  # then it gets rank Undefined which equals 0
  proDevUpper = justin.proDev.upper()
  return httpOK(startResponse, 
           "JUSTIN_%s_ClassAds_Time %d\n"
           "JUSTIN_%s_Stage_Ranks_Raw %s\n"
           "JUSTIN_%s_Stage_Ranks "
             "isUndefined(My.GLIDEIN_PS_JUSTIN_%s_Stage_Ranks_Raw)"
             "?My.JUSTIN_%s_Stage_Ranks_Raw"
             ":My.GLIDEIN_PS_JUSTIN_%s_Stage_Ranks_Raw\n"
           "JUSTIN_%s_Has_Inner_Apptainer %s\n"
           "JUSTIN_%s_Rank isUndefined(Target.JUSTIN_%s_Stage)"
             "?%d:My.JUSTIN_%s_Stage_Ranks[Target.JUSTIN_%s_Stage]\n"
           "Rank %s\n"
              % (proDevUpper, int(time.time()),
                 proDevUpper, rankText,
                 proDevUpper, proDevUpper, proDevUpper, proDevUpper,
                 proDevUpper, hasInnerApptainer,
                 proDevUpper, proDevUpper, 
                   101 if (random.random() < justin.nonJustinFraction) else 0,
                   proDevUpper, proDevUpper,
                 justinRank)
                )

# Return information about the job from the jobsubID
def makeJobDict(jsonDict, jobscriptSecret = None):

  # Find the job info and the stage's max_distance value
  try:
    query = ('SELECT '
             'jobs.job_state,'
             'jobs.jobscript_secret,'
             'stages.max_distance,'
             'jobs.workflow_id,'
             'jobs.stage_id,'
             'jobs.justin_job_id,'
             'jobs.site_id,'
             'jobs.requested_processors,'
             'jobs.requested_rss_bytes,'
             'jobs.requested_wall_seconds,'
             'jobs.has_inner_apptainer,'
             'sites.site_name,'
             'sites.running_jobs,'
             'sites.enabled,'
             'sites.always_inner_apptainer AS site_a_i_a,'
             'entries.always_inner_apptainer AS entry_a_i_a,'
             'entries.entry_name,'
             'entries.entry_id,'
             'scopes.scope_name,'
             'wlcg_groups.wlcg_group_name '
             'FROM jobs '
             'LEFT JOIN stages ON jobs.workflow_id=stages.workflow_id '
             'AND jobs.stage_id=stages.stage_id '
             'LEFT JOIN sites ON jobs.site_id=sites.site_id '
             'LEFT JOIN entries ON jobs.entry_id=entries.entry_id '
             'LEFT JOIN workflows ON workflows.workflow_id=jobs.workflow_id '
             'LEFT JOIN scopes ON scopes.scope_id=workflows.scope_id '
             'LEFT JOIN wlcg_groups '
             'ON wlcg_groups.wlcg_group_id=scopes.wlcg_group_id '
             'WHERE jobs.jobsub_id="%s"'
             % jsonDict['jobsub_id'])

    job = justin.select(query, justOne = True)
  except Exception as e:
    return { "error_message": "Error finding job from jobsubID " + 
             jsonDict['jobsub_id'] + ": " + str(e) }

  if not job:
    return { "error_message": "Failed to find job from jobsubID " + 
             jsonDict['jobsub_id'] }

  if jobscriptSecret is not None and \
     job['jobscript_secret'] != jobscriptSecret:
    return { "error_message": "Jobscript secret mismatch" }

  return { "error_message"          : None,
           "workflow_id"            : job['workflow_id'],
           "stage_id"               : job['stage_id'],
           "site_id"                : job['site_id'],
           "site_name"              : job['site_name'],
           "site_a_i_a"             : job['site_a_i_a'],
           "entry_id"               : job['entry_id'],
           "entry_name"             : job['entry_name'],
           "entry_a_i_a"            : job['entry_a_i_a'],
           "running_jobs"           : job['running_jobs'],
           "site_enabled"           : job['enabled'],
           "justin_job_id"          : job['justin_job_id'],
           "job_state"              : job['job_state'],
           "jobscript_secret"       : job['jobscript_secret'],
           "max_distance"           : job['max_distance'],
           "requested_processors"   : job['requested_processors'],
           "requested_rss_bytes"    : job['requested_rss_bytes'],
           "requested_wall_seconds" : job['requested_wall_seconds'],
           "has_inner_apptainer"    : job['has_inner_apptainer'],
           "scope_name"             : job['scope_name'] 
                                      if job['scope_name'] else '',
           "wlcg_group_name"        : job['wlcg_group_name']
                                      if job['wlcg_group_name'] else ''
         }

# Make a new certificate from the certificate workflow sent by the 
# wrapper job, signed by the VOMS proxy loaded from local disk
# Exceptions must by caught by the caller
def delegateJobsProxy(newCertRequest, jobsProxyString):

#  print('newCertRequest: ' + newCertRequest, file=sys.stderr)
#  print('jobsProxyString: ' + str(jobsProxyString), file=sys.stderr)
  if not jobsProxyString:
    print('Failed to read proxy file from disk?', file=sys.stderr)

  jobsProxyKey      = M2Crypto.RSA.load_key_string(jobsProxyString)
  jobsProxyCertsBIO = M2Crypto.BIO.MemoryBuffer(jobsProxyString)

  jobsProxyCerts = []
  while True:
    try:
      jobsProxyCerts.append(M2Crypto.X509.load_cert_bio(jobsProxyCertsBIO))
    except:
      jobsProxyCertsBIO.close()
      break

  newPubKey = M2Crypto.X509.load_request_string(newCertRequest, 
                                       M2Crypto.X509.FORMAT_PEM).get_pubkey()

  # Create and populate new certificate   
  newCert = M2Crypto.X509.X509()
  newCert.set_pubkey(newPubKey)
  newCert.set_serial_number(int(time.time() * 100))
  newCert.set_issuer_name(jobsProxyCerts[0].get_subject())
  newCert.set_version(2) # "2" is X.509 for "v3" ...

  # Add CN=CENTISECONDS to existing VOMS proxy DN
  newSubject = jobsProxyCerts[0].get_subject()
  newSubject.add_entry_by_txt(field = "CN",
                              type  = 0x1001,
                              entry = str(int(time.time() * 100)),
                              len   = -1,
                              loc   = -1,
                              set   = 0)

  newCert.set_subject_name(newSubject)

  # Set not before time to now
  newNotBefore = M2Crypto.ASN1.ASN1_UTCTIME()
  newNotBefore.set_time(int(time.time()))
  newCert.set_not_before(newNotBefore)

  # Set not after time to now + 7 days
  newNotAfter = M2Crypto.ASN1.ASN1_UTCTIME()
  newNotAfter.set_time(int(time.time()) + 604800)
  newCert.set_not_after(newNotAfter)

  # Usual extension giving key usage
  newCert.add_ext(M2Crypto.X509.new_extension("keyUsage", 
                  "Digital Signature, Key Encipherment, Key Agreement", 1))

  # Special extension for RFC X.509 proxies 
  newCert.add_ext(M2Crypto.X509.new_extension("proxyCertInfo", 
                  "critical, language:1.3.6.1.4.1.3536.1.1.1.9", 1, 0))

  # Sign this new cert with the VOMS proxy private key
  jobsProxyKeyEVP = M2Crypto.EVP.PKey()
  jobsProxyKeyEVP.assign_rsa(jobsProxyKey)
  newCert.sign(jobsProxyKeyEVP, 'sha256')

  # Return the results as PEM encoded strings
  jobsProxyCertsChain = ''
  for oneCert in jobsProxyCerts:
     jobsProxyCertsChain += oneCert.as_pem().decode()

  return (newCert.as_pem().decode(), jobsProxyCertsChain)

def makeJobscriptDict(jsonDict, jobDict):
  # Collect values to be transmitted to the user jobscript

  jobscriptDict = {}
  scopeName = jobDict['scope_name']

  try:
    query = ("SELECT jobscript,principal_name FROM stages_jobscripts "
             "LEFT JOIN workflows "
             "ON workflows.workflow_id=stages_jobscripts.workflow_id "
             "LEFT JOIN users ON users.user_id=workflows.user_id "
             "LEFT JOIN principal_names "
             "ON principal_names.pn_id=users.main_pn_id "
             "WHERE stages_jobscripts.workflow_id=%d "
             "AND stages_jobscripts.stage_id=%d" 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    row       = justin.select(query, justOne = True)
    jobscript = row['jobscript']
    jobscriptDict['principal_name'] = row['principal_name']
  except Exception as e:
    print('SELECT stages_jobscripts fails: ' + str(e), file=sys.stderr)
    return None

  jobscriptDict['jobscript'] = jobscript

  # Create and add files for VOMS proxies for the job
  try:
    (newCertNoRoles, newChainNoRoles) = \
                 delegateJobsProxy(jsonDict['csr-no-roles'],
                                   justin.jobsNoRolesProxyString)

    (newCertProduction, newChainProduction) = \
                 delegateJobsProxy(jsonDict['csr-production'],
                                   justin.jobsProductionProxyString)

  except Exception as e:
    print('Failed to create certs for delegated proxies: ' + str(e), 
          file=sys.stderr)
    return None

  jobscriptDict['justin-jobs-no-roles.cert.pem']    = newCertNoRoles
  jobscriptDict['justin-jobs-no-roles.chain.pem']   = newChainNoRoles
  jobscriptDict['justin-jobs-production.cert.pem']  = newCertProduction
  jobscriptDict['justin-jobs-production.chain.pem'] = newChainProduction

  # Create a list of the output file patterns
  try:
    query = ("SELECT destination,file_pattern,"
             "for_next_stage "
             "FROM stages_outputs "
             "LEFT JOIN stages "
             "ON stages.workflow_id=stages_outputs.workflow_id "
             "AND stages.stage_id=stages_outputs.stage_id "
             "LEFT JOIN workflows "
             "ON workflows.workflow_id=stages_outputs.workflow_id "
             "WHERE stages_outputs.workflow_id=%d "
             "AND stages_outputs.stage_id=%d" 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    rows = justin.select(query)
  except:
    return None

  jobscriptDict['patterns'] = []

  for row in rows:
    jobscriptDict['patterns'].append((row['for_next_stage'],
                                      row['destination'],
      ('::URL::' if row['destination'].startswith('https://') else scopeName),
                                      row['file_pattern']))

  # Create a list of RSEs for Workflow Test jobs
  if jobDict['workflow_id'] == justin.awtWorkflowID:
    try:
      query = ('SELECT rse_name,lan_write_scheme,wan_write_scheme,distance,'
               '(SELECT wan_pfn FROM replicas WHERE workflow_id=%d AND '
               ' replicas.rse_id=storages.rse_id ORDER BY replica_id DESC LIMIT 1)'
               ' AS wan_pfn,'
               '(SELECT lan_pfn FROM replicas WHERE workflow_id=%d AND '
               ' replicas.rse_id=storages.rse_id ORDER BY replica_id DESC LIMIT 1)'
               ' AS lan_pfn '
               'FROM storages '
               'LEFT JOIN sites_storages '
               'ON sites_storages.rse_id=storages.rse_id '
               'AND sites_storages.site_id=%d '
               'WHERE rse_name<>"MONTECARLO" '
               'AND NOT storages.decommissioned GROUP BY rse_name' % 
               (justin.awtWorkflowID,
                justin.awtWorkflowID,
                jobDict['site_id']) )

      rseRows = justin.select(query)
    except Exception as e:
      print('AWT list of RSEs query fails with: ' + str(e), file=sys.stderr)
      return None

    jobscriptDict['awt_rses'] = []
    for rseRow in rseRows:
      if rseRow['lan_pfn'] and rseRow['distance'] == 0:
        pfn = rseRow['lan_pfn']
      elif rseRow['wan_pfn']:
        pfn = rseRow['wan_pfn']
      else:
        continue

      jobscriptDict['awt_rses'].append((rseRow['rse_name'],
                                        rseRow['lan_write_scheme']
                                        if rseRow['distance'] == 0
                                        else rseRow['wan_write_scheme'],
                                        pfn))
                        
  # Create an ordered output RSE list specific to this stage
  #
  # When the occupancy figures are usable this needs to have 
  #  'storages.occupancy < 1.0 AND ' added to WHERE and 
  #  storages.occupancy added to ORDER BY
  try:
    query = ('SELECT rse_name,lan_write_scheme,wan_write_scheme,distance '
             'FROM storages '
             'LEFT JOIN stages_output_storages '
             'ON (storages.rse_id=stages_output_storages.rse_id '
             'AND stages_output_storages.workflow_id=%d '
             'AND stages_output_storages.stage_id=%d) '
             'LEFT JOIN sites_storages '
             'ON (storages.rse_id=sites_storages.rse_id '
             'AND sites_storages.site_id=%d) '
             'WHERE '
             'sites_storages.distance IS NOT NULL AND '
             'storages.rucio_write AND '
             'storages.justin_write AND '
             'storages.occupancy < 1.0 AND '
             'NOT storages.decommissioned '
             'ORDER BY (stages_output_storages.rse_id IS NULL),'
             'distance,RAND()' 
             % (jsonDict['workflow_id'],
                jsonDict['stage_id'],
                jobDict['site_id']
               )
            )

    rseRows = justin.select(query)
  except:
    return None

  jobscriptDict['output_rses'] = []
  for rseRow in rseRows:
    jobscriptDict['output_rses'].append((rseRow['rse_name'],
                                         rseRow['lan_write_scheme'] 
                                         if rseRow['distance'] == 0 
                                         else rseRow['wan_write_scheme']))
      
  jobscriptDict['site_name']        = jobDict['site_name']
  jobscriptDict['workflow_id']      = jsonDict['workflow_id']
  jobscriptDict['stage_id']         = jsonDict['stage_id']
  jobscriptDict['scope']            = scopeName

  jobscriptDict['requested_processors']   = jobDict['requested_processors']
  jobscriptDict['requested_rss_bytes']    = jobDict['requested_rss_bytes']
  jobscriptDict['requested_wall_seconds'] = jobDict['requested_wall_seconds']

  jobscriptDict['jobsub_id']        = jsonDict['jobsub_id']
  jobscriptDict['jobscript_secret'] = jobDict['jobscript_secret']
  jobscriptDict['sam_web_uri']      = \
    'https://justin.dune.hep.ac.uk/api/samweb/%s/%s' \
     % (jsonDict['jobsub_id'], jobDict['jobscript_secret'])

  # User defined environment variables for the jobscript
  jobscriptDict['stage_env'] = []

  try:
    query = ('SELECT env_name,env_value FROM stages_environment '
             'WHERE workflow_id=%d AND stage_id=%d ORDER BY env_name' 
             % (jsonDict['workflow_id'], jsonDict['stage_id']))

    envRows = justin.select(query)
  except:
    return None
  else:
    # Add the user defined ones
    for envRow in envRows:
      jobscriptDict['stage_env'].append((envRow['env_name'],
                                         envRow['env_value']))

  return jobscriptDict

def updateTerminalJobState(justinJobID = 0, jobState = '', 
                           workflowID = 0, stageID = 0, siteID = 0):

# Update job_state with one of the terminal states
# THIS DOES A COMMIT TOO AND MUST ONLY BE USED IMMEDIATELY BEFORE
# httpError() IN THE CALLING FUNCION

  if jobState == 'finished':
    eventTypeID = justin.event_JOB_FINISHED
  elif jobState == 'notused':
    eventTypeID = justin.event_JOB_NOTUSED
  elif jobState == 'aborted':
    eventTypeID = justin.event_JOB_ABORTED
  elif jobState == 'jobscript_error':
    eventTypeID = justin.event_JOB_SCRIPT_ERROR
  elif jobState == 'outputting_failed':
    eventTypeID = justin.event_JOB_OUTPUTTING_FAILED
  else:
    raise RuntimeError('state %s not handled by updateTerminalJobState()'
                       % jobState)
  try:
    justin.insertUpdate(
             'UPDATE jobs SET job_state="%s",'
             'heartbeat_time=NOW(),'
             'finished_time=NOW() '
             'WHERE justin_job_id=%d' % (jobState, justinJobID))

    justin.logEvent(eventTypeID = eventTypeID,
                    workflowID  = workflowID, 
                    stageID     = stageID,
                    justinJobID = justinJobID,
                    siteID      = siteID
                   )

    justin.conn.commit()
  except: 
    pass

# Get details of this jobs workflow/stage
def getJobscriptMethod(startResponse, jsonDict, environ):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request', 
                     'Missing jobsub_id in JSON')

  # Check jsonDict contains required integer/bool values (eg rss_bytes)
  for name in ['workflow_id', 'stage_id', 'rss_bytes', 'processors',
               'wall_seconds', 'has_inner_apptainer']:
    try:
      n = int(jsonDict[name])
    except:
#      updateTerminalJobState(jsonDict['jobsub_id'], 'aborted')
      return httpError(startResponse,
                       '400 Bad Request', 
                       'Missing/invalid integer value(s) in JSON')

  # Check jsonDict contains required string values
  for name in ['site_name', 'entry_name', 'cpuinfo', 'os_release', 
               'hostname', 'site_job_id']:
    if name not in jsonDict or not justin.stringNoQuotes(jsonDict[name]) \
       or not jsonDict[name]:
#      updateTerminalJobState(jsonDict['jobsub_id'], 'aborted')
      return httpError(startResponse, 
                       '400 Bad Request', 
                       'Missing/invalid value for %s in JSON' % name)

  if jsonDict['site_name'] == 'XX_UNKNOWN' or \
     jsonDict['entry_name'] == 'UNKNOWN':
    print('Job %s has unknown site_name or entry_name - aborting' 
          % jsonDict['jobsub_id'], file=sys.stderr)
#    updateTerminalJobState(jsonDict['jobsub_id'], 'aborted')
    return httpError(startResponse, '400 Bad Request', 'Unknown site')

  # If the job is in the submitted state then update with host details
  # If not in submitted, then some mismatch which we catch and return 
  #Â 410 below
  try:
    justin.insertUpdate('UPDATE jobs SET '
                           'site_id=(SELECT site_id FROM sites '
                           'WHERE site_name="%s"),'
                           'entry_id=(SELECT entry_id FROM entries '
                           'WHERE entry_name="%s"),'
                           'cpuinfo="%s",' 
                           'os_release="%s",' 
                           'hostname="%s",' 
                           'rss_bytes=%d,' 
                           'processors=%d,' 
                           'wall_seconds=%d,'
                           'has_inner_apptainer=%s ' 
                           'WHERE jobsub_id="%s" AND '
                           'workflow_id=%d AND stage_id=%d AND '
                           'job_state="submitted"' % 
                           (jsonDict['site_name'], 
                            jsonDict['entry_name'], 
                            jsonDict['cpuinfo'],
                            jsonDict['os_release'],
                            jsonDict['hostname'],
                            jsonDict['rss_bytes'],
                            jsonDict['processors'],
                            jsonDict['wall_seconds'],
                            jsonDict['has_inner_apptainer'],
                            jsonDict['jobsub_id'],
                            jsonDict['workflow_id'],
                            jsonDict['stage_id']
                           ))
    justin.conn.commit()
  except Exception as e:
#    updateTerminalJobState(jsonDict['jobsub_id'], 'aborted')
    return httpError(startResponse, 
                     '500 Internal Server Error',
                     'justIN allocator service fails to update job with site_id '
                     '(site_name=%s): %s' %
                     (str(jsonDict['site_name']), str(e)))
    
  # Find details of the job
  jobDict = makeJobDict(jsonDict)

  if (jobDict['workflow_id'] != jsonDict['workflow_id']) or \
     (jobDict['stage_id'] != jsonDict['stage_id']):
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'],
                           jobState    = 'aborted')
    return httpError(startResponse,
                     '403 Forbidden',
                     'workflow ID / stage ID do not match jobsub ID')

  # Check that we're not trying to run unprivileged jobscripts 
  # without Inner Apptainer: some kind of out of date matching?
  if jsonDict['workflow_id'] != justin.awtWorkflowID and \
     not jsonDict['has_inner_apptainer'] and \
     (not jobDict['wlcg_group_name'] 
      or jobDict['wlcg_group_name'] != '/dune/production'):
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'notused')
    return httpError(startResponse,
              '404 Not Found',
              'Tried to run an unprivileged workflow without Inner Apptainer')
    
  if jobDict['error_message']:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse,
                     '400 Bad Request', 
                     jobDict['error_message'] + ' in getJobscriptMethod')

  #Â Some kind of mismatch. Maybe the idle job was marked as stalled. Maybe 
  # HTCondor rescheduled the job and it has tried to run again. In all cases 
  # leave the job state as is.
  if jobDict['job_state'] != 'submitted':
    return httpError(startResponse, 
                     '410 Gone', 
                     'Cannot get stage as job already in %s state' 
                     % jobDict['job_state'])

  #Â Site not enabled so no matches allowed 
  if not jobDict['site_enabled'] and \
     jsonDict['workflow_id'] != justin.awtWorkflowID:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'notused')
    return httpError(startResponse, 
                     '404 Not Found',
                     'Cannot start as site %s not enabled' 
                     % jobDict['site_name'])

  #Â Record whether job slots from this entry always support Inner Apptainer
  # 0=not always, 1=not known, 2=always
  if jsonDict['has_inner_apptainer']:
    if jobDict['entry_a_i_a'] == 1:
      entryApptainerSQL = ',always_inner_apptainer=2'
    else:
      entryApptainerSQL = ''

    if jobDict['site_a_i_a'] == 1:
      siteApptainerSQL = ',always_inner_apptainer=2'
    else:
      siteApptainerSQL = ''
  else:
    siteApptainerSQL  = ',always_inner_apptainer=0'
    entryApptainerSQL = ',always_inner_apptainer=0'

  try:
    justin.insertUpdate('UPDATE sites SET last_get_jobscript_time=NOW()%s '
                        'WHERE site_id=%d' 
                        % (siteApptainerSQL, jobDict['site_id']))
    justin.insertUpdate('UPDATE entries SET last_get_jobscript_time=NOW()%s '
                        'WHERE entry_id=%d' 
                        % (entryApptainerSQL, jobDict['entry_id']))
  except:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse, 
                 '500 Internal Server Error',
                 'justIN allocator service fails to record get_jobscript time')

  jobscriptDict = makeJobscriptDict(jsonDict, jobDict)
  if not jobscriptDict:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed to create jobscript dictionary')
           
  try:
    # Unique ID string for this instance that may also help in debugging
    allocatorName = environ['SERVER_NAME']

    justin.insertUpdate(
             'UPDATE jobs SET '
             'site_job_id="%s",'
             'allocation_time=NOW(),' 
             'heartbeat_time=NOW(),'
             'finished_time="%s",' 
             'job_state="%s",'
             'has_allocations=TRUE,'
             'allocator_name="%s",' 
             'workflow_id=%d,'
             'stage_id=%d '
             'WHERE jobsub_id="%s"'
             % (str(jsonDict['site_job_id']), 
                justin.unixEpoch,                
                'processing' if jsonDict['workflow_id']==justin.awtWorkflowID 
                else 'started',
                allocatorName,
                jsonDict['workflow_id'],
                jsonDict['stage_id'],
                jsonDict['jobsub_id']))

    justin.logEvent(eventTypeID = justin.event_JOB_STARTED,
                    workflowID = jsonDict['workflow_id'],
                    stageID = jsonDict['stage_id'],
                    justinJobID = jobDict['justin_job_id'],
                    siteID = jobDict['site_id']
                   )

    if jsonDict['workflow_id']==justin.awtWorkflowID:
      # AWT jobs go straight to processing too
      justin.logEvent(eventTypeID = justin.event_JOB_PROCESSING,
                      workflowID = jsonDict['workflow_id'],
                      stageID = jsonDict['stage_id'],
                      justinJobID = jobDict['justin_job_id'],
                      siteID = jobDict['site_id']
                     )

  except Exception as e:
    updateTerminalJobState(justinJobID = jobDict['justin_job_id'], 
                           jobState    = 'aborted')
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'justIN allocator service failed updating allocated job: ' 
                     + str(e))
  try:
    justin.conn.commit()
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Database commit fails: ' + str(e))

  s = json.dumps(jobscriptDict).encode('UTF-8')
  startResponse('200 OK',
                [('Content-type',   'application/json'),
                 ('Content-length', str(len(s)))
                ])

  return [s]
   
# Record heartbeats from wrapper jobs
def sendHeartbeatMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request', 
                     'Missing jobsub_id in JSON')

  try:
    justin.insertUpdate('UPDATE jobs SET heartbeat_time=NOW() ' 
                        'WHERE jobsub_id="' + str(jsonDict['jobsub_id']) + '"'
                       )
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'justIN allocator service failed recording heartbeat: ' 
                     + str(e))

  return httpOK(startResponse, '')
   
# Job aborted 
def jobAbortedMethod(startResponse, jsonDict):

  try:
    httpCode = int(jsonDict['http_code'])
  except:
    return httpError(startResponse, 
                     '400 Bad Request', 
                     'Missing/invalid http_code in JSON')

  if 'aborted_method' not in jsonDict or \
     not justin.stringNoQuotes(jsonDict['aborted_method']):
    return httpError(startResponse,  
                     '400 Bad Request', 
                     'Missing/invalid aborted_method in JSON')

  jobDict = makeJobDict(jsonDict)

  if jobDict['error_message']:
    return httpError(startResponse,
                     '400 Bad Request', 
                     jobDict['error_message'] + ' in jobAbortedMethod')

  if jobDict['job_state'] not in ['submitted','notused','started',
                                  'processing','outputting','jobscript_error']:
    return httpError(startResponse, 
                     '410 Gone', 
                     'Cannot set job to aborted as job already in %s state' 
                     % jobDict['job_state'])

  # All ok, now process the message

  if httpCode == 404 and jsonDict['aborted_method'] == 'get_jobscript':
    state = 'notused'
    eventTypeID = justin.event_JOB_NOTUSED
  elif jsonDict['aborted_method'] == 'jobscript_error':
    state = 'jobscript_error'
    eventTypeID = justin.event_JOB_SCRIPT_ERROR
  elif jsonDict['aborted_method'] in [ 'rucio_attach', 'rucio_upload',
                                       'metacat_declare' ]:
# WANT TO RESET FILE ALLOCATIONS TOO?
    state = 'outputting_failed'
    eventTypeID = justin.event_JOB_OUTPUTTING_FAILED
  else:
    state = 'aborted'
    eventTypeID = justin.event_JOB_ABORTED

  print('Job %s aborted: %d %s' % (str(jsonDict['jobsub_id']),
                                   eventTypeID,
                                   str(jsonDict['aborted_method'])),
        file=sys.stderr)
 
  try:
    justin.insertUpdate(
             'UPDATE jobs SET job_state="%s",'
             'heartbeat_time=NOW(),'
             'finished_time=NOW() '
             'WHERE jobsub_id="%s"' %
             (state, jsonDict['jobsub_id']))

    if 'rse_name' in jsonDict:
      rseName = jsonDict['rse_name']
    else:
      rseName = None

    justin.logEvent(eventTypeID  = eventTypeID,
                    workflowID   = jobDict['workflow_id'],
                    stageID      = jobDict['stage_id'],
                    justinJobID  = jobDict['justin_job_id'],
                    siteID       = jobDict['site_id'],
                    rseName      = rseName
                   )

  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'justIN allocator service job_aborted failed: ' + str(e))

  return httpOK(startResponse, '')

def cacheFiles(siteID, workflowID, stageID, maxDistance):
  # Cache best replicas info for the best stage for a site, 
  # relative to that site

  # We use this time to remove previous cache entries
  cutoffTime = int(time.time())

  try:
    replicaRows = justin.select('SELECT files.file_id,'
                   'replicas.replica_id,'
                   'replicas.rse_id,'
                   'distance '
                   'FROM files '
                   'LEFT JOIN replicas ON files.file_id=replicas.file_id '
                   'LEFT JOIN storages ON replicas.rse_id=storages.rse_id '
                   'LEFT JOIN sites_storages ON '
                   'replicas.rse_id=sites_storages.rse_id AND '
                   'sites_storages.site_id=%d '
                   'WHERE files.workflow_id=%d AND files.stage_id=%d AND '
                   'files.state="unallocated" AND '
                   'distance<=%f AND '
                   'accessible_until > NOW() AND '
                   'storages.rucio_read AND storages.justin_read AND '
                   'NOT storages.decommissioned '
                   'ORDER by distance,replicas.file_id LIMIT 1000' % 
                   (siteID, workflowID, stageID, maxDistance))
  except Exception as e:
    print('Failed getting replica info to cache: %s' % str(e), file=sys.stderr)
    return

  filesSeen = set()

  for replicaRow in replicaRows:
      #Â Go through the replicas, caching them.
      # Do not add more distant replicas of files already seen.
      # This deduplication in the Python agent reduces the big scans of
      # the database which block other services.
      if len(filesSeen) >= 500:
        break
     
      if replicaRow['file_id'] in filesSeen:
        continue
      
      filesSeen.add(replicaRow['file_id'])
   
      try:
        justin.insertUpdate('INSERT INTO sites_files_cache SET '
                               'site_id=%d,'
                               'workflow_id=%d,'
                               'stage_id=%d,'
                               'distance=%f,'
                               'file_id=%d,'
                               'rse_id=%d,'
                               'replica_id=%d,'
                               'cache_time=FROM_UNIXTIME(%d)' %
                               (siteID,
                                workflowID,
                                stageID,
                                replicaRow['distance'],
                                replicaRow['file_id'],
                                replicaRow['rse_id'],
                                replicaRow['replica_id'],
                                cutoffTime
                               )
                              )
      except Exception as e:
        print('Failed caching replica info: %s' % str(e), file=sys.stderr)
        continue

  try:
    justin.insertUpdate('INSERT INTO sites_files_cache_state SET '
                          'site_id=%d,'
                          'workflow_id=%d,'
                          'stage_id=%d,'
                          'number_found=%d,'
                          'cache_time=FROM_UNIXTIME(%d)' %
                          (siteID,
                           workflowID,
                           stageID,
                           len(filesSeen),
                           cutoffTime
                          )
                         )
  except Exception as e:
    print('Failed creating new sites_files_cache_state row: %s' % str(e),
          file=sys.stderr)

  # Whatever else happened we clear out any old cache entries
  #Â for this site/workflow/stage
  try:
    justin.insertUpdate('DELETE FROM sites_files_cache '
                        'WHERE site_id=%d AND workflow_id=%d AND stage_id=%d '
                        'AND cache_time < FROM_UNIXTIME(%d)'
                        % (siteID, workflowID, stageID, cutoffTime))
  except Exception as e:
    print('Delete previous sites_files_cache fails with: ' + str(e), 
          file=sys.stderr)
 
  try:
    justin.insertUpdate('DELETE FROM sites_files_cache_state '
                        'WHERE site_id=%d AND workflow_id=%d AND stage_id=%d '
                        'AND cache_time < FROM_UNIXTIME(%d)'
                        % (siteID, workflowID, stageID, cutoffTime))
  except Exception as e:
    print('Delete previous sites_files_cache_state fails with: ' + str(e), 
          file=sys.stderr)
 
  justin.conn.commit()

# Try to get the file cache service lock or raise an exception if
# unable to do this for any reason, including a timeout
# No try here since we expose exceptions to the caller!
def getCacheLock(lockName, timeoutSeconds):
  row = justin.select(
          'SELECT GET_LOCK("%s", %d) '
          'AS result' % (lockName, timeoutSeconds), justOne = True)

  if not row or 'result' not in row:
    # Something went wrong!
    raise RuntimeError('justin_allocator lock query failure')
  
  if row['result'] == 0:
    # Failed to get lock in time so raise exception
    raise RuntimeError('justin_allocator lock timeout')

  # Success!  
  return

# Release the file cache lock if we can
def releaseCacheLock(lockName):
  try:
    justin.select('SELECT RELEASE_LOCK("%s")' % lockName)
  except Exception as e:
#Â REMOVE THIS print FOR PROD
    print('Exception releasing lock: ' + str(e))
    pass

#Â Get a list of cached files for this workflow/stage at this site
# Special treatment of file_id=0 which records if no-cacheable-files
def findCachedFiles(siteID, workflowID, stageID, maxFiles):
  query = (
    "SELECT file_did,sites_files_cache.file_id,lan_pfn,wan_pfn,"
    "rse_name,sites_files_cache.rse_id,distance "
    "FROM sites_files_cache "
    "LEFT JOIN files ON files.file_id=sites_files_cache.file_id "
    "LEFT JOIN replicas ON replicas.replica_id=sites_files_cache.replica_id "
    "LEFT JOIN storages ON storages.rse_id=sites_files_cache.rse_id "
    "WHERE "
    "sites_files_cache.site_id=%d AND "
    "sites_files_cache.workflow_id=%d AND "
    "sites_files_cache.stage_id=%d AND "
    "files.state='unallocated' "
    "ORDER BY sites_files_cache.cache_time DESC,"
    "sites_files_cache.distance,sites_files_cache.file_id "
    "LIMIT %d FOR UPDATE" %
    (siteID, workflowID, stageID, maxFiles))

  return justin.select(query)
  
def findBestFile(jobDict):

  # When there are less cached files than this we update the cache
  minCacheSize = 20

  replicaRows = findCachedFiles(jobDict['site_id'], jobDict['workflow_id'], 
                                jobDict['stage_id'], minCacheSize)

#  DEBUGGING print() STATEMENTS BELOW NEED TO BE REMOVED
#  print('For %s/%d/%d got %d rows' 
#        % (jobDict['site_name'], jobDict['workflow_id'], jobDict['stage_id'], 
#           len(replicaRows)))

  if len(replicaRows) < minCacheSize:    
    #Â Too few returned: might be time to update the cache?
    try:
      # Check if there just weren't enough available to be cached
      cacheStateRow = justin.select(
        'SELECT number_found,'
        'TIMESTAMPDIFF(SECOND, cache_time, NOW()) AS seconds '
        'FROM sites_files_cache_state '
        'WHERE site_id=%d AND workflow_id=%d and stage_id=%d'
        % (jobDict['site_id'], jobDict['workflow_id'], jobDict['stage_id']),
        justOne = True)
    except Exception as e:
      print('Failed to get number_found: ' + str(e), file=sys.stderr)
      cacheStateRow = None

    print('%s/%d/%d cacheStateRow=%s, len(replicaRows)=%d'
          % (jobDict['site_name'],
             jobDict['workflow_id'],
             jobDict['stage_id'],
             str(cacheStateRow), len(replicaRows)), file=sys.stderr)
    if cacheStateRow is None or \
       cacheStateRow['seconds'] > 300 or \
       cacheStateRow['number_found'] >= minCacheSize:
      # If there are no cache state entries yet,
      # or plenty of files were found, then we update the cache
      # Cache needs to be updated
      print('%s/%d/%d cache needs to be updated'
            % (jobDict['site_name'],
               jobDict['workflow_id'],
               jobDict['stage_id']), file=sys.stderr)
      try:
        getCacheLock('justin_allocator_file_cache', 0)
      except Exception as e:
# THIS EXCEPTION WILL HAPPEN ALL THE TIME NORMALLY SO print() NOT FOR PROD
        # We didn't get the cache lock so we carry on with what was returned
        print('%s/%d/%d Did not get file cache lock: %s'
              % (jobDict['site_name'],
                 jobDict['workflow_id'],
                 jobDict['stage_id'], str(e)), file=sys.stderr)
      else:
        print('%s/%d/%d Got cache lock, update cache'
              % (jobDict['site_name'],
                 jobDict['workflow_id'],
                 jobDict['stage_id']),
              file=sys.stderr)
        cacheFiles(jobDict['site_id'], jobDict['workflow_id'], 
                   jobDict['stage_id'], jobDict['max_distance'])
        releaseCacheLock('justin_allocator_file_cache')

        # If we had zero results before, rerun the query
        if len(replicaRows) == 0:
          print('Rerun findCachedFiles()', file=sys.stderr)
          replicaRows = findCachedFiles(jobDict['site_id'], 
                                        jobDict['workflow_id'], 
                                        jobDict['stage_id'], 1)

  if len(replicaRows) == 0:
    # No matches found
    print('No matches found', file=sys.stderr)
    return { 'error_message': None,
             'file_did'     : None  }

  try: 
    query = ("UPDATE files SET state='allocated',"
             "allocations=allocations+1,"
             "justin_job_id=" + str(jobDict['justin_job_id']) + " "
             "WHERE state='unallocated' AND "
             "file_id=" + str(replicaRows[0]['file_id'])
            )
    affectedRows = justin.cur.execute(query)
  except Exception as e:
    # If anything goes wrong, we stop straightaway
    return { 'error_message': 'Failed recording state change: ' + str(e) }

  if affectedRows != 1:
    return { 'error_message': 
             'Failed to allocate the chosen file (%s): already allocated???'
             % str(replicaRows[0]['file_did']) }
    
  justin.logEvent(eventTypeID = justin.event_FILE_ALLOCATED,
                     workflowID     = jobDict['workflow_id'],
                     stageID        = jobDict['stage_id'],
                     fileID         = replicaRows[0]['file_id'],
                     justinJobID    = jobDict['justin_job_id'],
                     siteID         = jobDict['site_id'],
                     rseID          = replicaRows[0]['rse_id'])

  # If storage is at the same site and a lan PFN is given, use it
  if replicaRows[0]['distance'] == 0 and replicaRows[0]['lan_pfn']:
    pfn = replicaRows[0]['lan_pfn']
  else:
    pfn = replicaRows[0]['wan_pfn']

  # The dictionary to return, with the highest priority result
  replica = { 'error_message' : None,
              'file_did'      : replicaRows[0]['file_did'],
              'pfn'           : pfn,
              'rse_name'      : replicaRows[0]['rse_name']
            }

  return replica

# Get an unallocated file from the given workflow+stage
def getFileMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing/invalid jobsub_id in JSON')

  if 'jobscript_secret' not in jsonDict or \
     not justin.stringNoQuotes(jsonDict['jobscript_secret']):
    return httpError(startResponse,
                     '400 Bad Request',
                     'Missing/invalid jobscript secret in JSON')

  # Lookup job details
  jobDict = makeJobDict(jsonDict, jsonDict['jobscript_secret'])

  if jobDict['error_message']:
    return httpError(startResponse,
                     '400 Bad Request', 
                     jobDict['error_message'] + ' in getFileMethod')

  if jobDict['job_state'] != 'started' and \
     jobDict['job_state'] != 'processing':
    return httpError(startResponse,
                     '403 Forbidden', 
                     'Job in wrong state to find file (%s)' % 
                     jobDict['job_state'])

  #Â Create a stage dictionary with the next file in this stage  
  oneFile = findBestFile(jobDict)

  if oneFile['error_message']:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed finding one file: ' + oneFile['error_message'])
  
  if oneFile['file_did']:
    try:
      # Update heartbeat_time and make sure job is set to processing now
      justin.insertUpdate('UPDATE jobs SET job_state="processing",'
                          'heartbeat_time=NOW() '
                          'WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"'
                         )
      # Note that AWT jobs go to processing when they get the jobscript
      # as they do not use the get_file method implemented here!

      if jobDict['job_state'] == 'started':
        # If just here to update the hearbeat, do not log a state change
        justin.logEvent(eventTypeID = justin.event_JOB_PROCESSING,
                        workflowID  = jobDict['workflow_id'],
                        stageID     = jobDict['stage_id'],
                        justinJobID = jobDict['justin_job_id'],
                        siteID      = jobDict['site_id']
                       )

    except Exception as e:
       return httpError(startResponse,
                        '500 Internal Server Error',
                        'Unable to update job to processing: ' + str(e))

    if jsonDict['method'] == 'samweb_getnextfile':
      return httpOK(startResponse, oneFile['pfn'])

    else: # method = 'get_file'
      return httpOK(startResponse,
                    oneFile['file_did'] + ' ' +
                    oneFile['pfn'] + ' ' +
                    oneFile['rse_name'])

  # No file eligible to be processed by this job
  return httpError(startResponse, 
                   '404 Not Found', 
                   'No eligible file found')

def updateFileProcessing(fileIDList,
                         state, justinJobID, workflowID, stageID):
  # Exceptions in this function must by handled by the caller!

  testFileIDs = []

  for fileID in fileIDList:
    testFileIDs.append('file_id=%d' % fileID)

  #Â Update the files the job did or did not manage to process
  if testFileIDs:

    if state == 'unprocessed':
      # If already at max allocations, go to failed rather than unallocated!
      query = ('UPDATE files SET '
               'state=IF(allocations < ' + str(justin.maxAllocations) + ','
               '"unallocated","failed") '
               'WHERE (' + ' OR '.join(testFileIDs) + ') '
               'AND justin_job_id=' + str(justinJobID) + ' '
               'AND workflow_id=' + str(workflowID) + ' '
               'AND stage_id=' + str(stageID) + ' '
               'AND state="allocated"')
    else:
      query = ('UPDATE files SET state="outputting" '
               'WHERE (' + ' OR '.join(testFileIDs) + ') '
               'AND justin_job_id=' + str(justinJobID) + ' '
               'AND workflow_id=' + str(workflowID) + ' '
               'AND stage_id=' + str(stageID) + ' '
               'AND state="allocated"')

    justin.insertUpdate(query)
    
# Deal with the output files needed by the next stage
# Caller must handle exceptions!
def processOutputDids(outputDids, workflowID, stageID, justinJobID):

  for outputDid in outputDids:
      
    if '"' in outputDid or "'" in outputDid:
      continue

    justin.insertUpdate('INSERT INTO files SET state="recorded",'
                        'file_did="'          + str(outputDid) + '",'
                        'creator_justin_job_id=' + str(justinJobID)  + ','
                        'workflow_id='         + str(workflowID) + ','
                        'stage_id='           + str(stageID + 1)
                       )

def saveAwtResults(justinJobID, siteID, jobscriptLog, hasInnerApptainer):

# REMOVE prints FOR PRODUCTION
  print('Start saveAwtResults', file=sys.stderr)

  for line in jobscriptLog.splitlines():
    if line.startswith('==awt== '):
      lineSplitted = line.split()
      
      try:
        justin.insertUpdate('UPDATE sites_storages SET '
                            'justin_job_id=%d,read_result=%d,write_result=%d '
                            'WHERE site_id=%d AND '
                            'rse_id=(SELECT rse_id FROM storages '
                            'WHERE rse_name="%s")' 
                            % (justinJobID, 
                               int(lineSplitted[3]),
                               int(lineSplitted[4]),
                               siteID,
                               lineSplitted[2])
                           )

        # non-zero shell error code is True in Python...
        justin.logEvent(eventTypeID = 
                          justin.event_AWT_READ_FAIL if int(lineSplitted[3]) 
                          else justin.event_AWT_READ_OK,
                        workflowID = justin.awtWorkflowID,
                        stageID = 1,
                        justinJobID = justinJobID,
                        siteID = siteID,
                        rseName = lineSplitted[2]
                       )

        # non-zero shell error code is True in Python...
        justin.logEvent(eventTypeID = 
                          justin.event_AWT_WRITE_FAIL if int(lineSplitted[4]) 
                          else justin.event_AWT_WRITE_OK,
                        workflowID = justin.awtWorkflowID,
                        stageID = 1,
                        justinJobID = justinJobID,
                        siteID = siteID,
                        rseName = lineSplitted[2]
                       )

      except Exception as e:
        print('Update awt results fails with %s' % str(e), 
              file=sys.stderr)

  try:
    justin.insertUpdate('UPDATE sites SET '
                        'last_awt_time=NOW(), last_awt_job_id=%d '
                        'WHERE site_id=%d' % (justinJobID, siteID))

  except Exception as e:
    print('Update awt last time fails with %s' % str(e), 
          file=sys.stderr)

# Before uploading output files the wrapper jobs record any results from 
# the jobscript, but just puts them in the "outputting" state
def recordResultsMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing jobsub_id in JSON')

  # Check jsonDict contains required lists (can be empty)
  for name in ['processed_dids', 'processed_pfns', 
               'output_dids', 'output_urls', 'next_stage_dids']:
    if name not in jsonDict:
      return httpError(startResponse,
                       '400 Bad Request',
                       'Missing value (%s) in JSON' % name)

    for fileDID in jsonDict[name]:
      if not justin.stringNoQuotes(fileDID):
        return httpError(startResponse,
                         '400 Bad Request',
                         'Invalid DID or PFN in ' + name)
      
  try:
    query = ('SELECT jobs.justin_job_id, jobs.workflow_id, jobs.stage_id,'
             'jobs.site_id, users.access_token, '
             'jobs.has_inner_apptainer, jobs.job_state '
             'FROM jobs '
             'LEFT JOIN workflows ON workflows.workflow_id=jobs.workflow_id '
             'LEFT JOIN users ON users.user_id=workflows.user_id '
             'WHERE jobs.jobsub_id="' + jsonDict['jobsub_id'] + '"')

    row = justin.select(query, justOne = True)

    justinJobID       = int(row['justin_job_id'])
    workflowID        = int(row['workflow_id'])
    stageID           = int(row['stage_id'])
    siteID            = int(row['site_id'])
    userAccessToken   = row['access_token']
    hasInnerApptainer = row['has_inner_apptainer']
    jobState          = row['job_state']
  except:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed finding job')

  if jobState != 'processing':
    return httpError(startResponse, 
                     '410 Gone', 
                     'Cannot record results as job already in %s state' 
                     % jobState)

  try:
    justin.insertUpdate(
        'INSERT INTO jobs_logs SET justin_job_id=' + str(justinJobID) + ','
        'jobscript_log="%s"' %
        jsonDict['jobscript_log'].replace('\\','\\\\').replace('"','\\"')
                       )                           
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed saving job log: ' + str(e))

#  print('Before call saveAwtResults %d %d' 
#        % (workflowID, justin.awtWorkflowID), file=sys.stderr)
  if workflowID == justin.awtWorkflowID:
    saveAwtResults(justinJobID, 
                   siteID, 
                   jsonDict['jobscript_log'],
                   hasInnerApptainer)

  if 'jobscript_exit' in jsonDict and jsonDict['jobscript_exit'] != 0:
    # Jobscript said it failed by exiting with non-zero value

    # Record job as finished due to jobscript_error
    try:
      justin.insertUpdate('UPDATE jobs SET '
                          'job_state="jobscript_error",'
                          'jobscript_exit=%d,'
                          'heartbeat_time=NOW(),'
                          'finished_time=NOW() '
                          'WHERE jobsub_id="%s"' %
                          (jsonDict['jobscript_exit'],
                           jsonDict['jobsub_id']))

      justin.logEvent(eventTypeID  = justin.event_JOB_SCRIPT_ERROR,
                      workflowID   = workflowID,
                      stageID      = stageID,
                      justinJobID  = justinJobID,
                      siteID       = siteID
                     )

    except Exception as e:
      return httpError(startResponse,
                       '500 Internal Server Error',
                       'Failed updating job: ' + str(e))
     
    # Do file allocation resets
    try:
      justin.insertUpdate('UPDATE files SET '
                          'state=IF(allocations < ' + 
                          str(justin.maxAllocations) + ','
                          '"unallocated","failed") '
                          'WHERE justin_job_id=%d AND '
                          '(state="allocated" OR state="outputting")' 
                          % justinJobID)
    except Exception as e:
      return httpError(startResponse,
                       '500 Internal Server Error',
                       'Failed resetting allocated files: ' + str(e))
     
    return httpOK(startResponse, '')

  try:
    # Use the event records to find the right replicas for this job
    query = ('SELECT events.file_id,file_did,wan_pfn,lan_pfn '
             'FROM events '
             'LEFT JOIN replicas ON (replicas.file_id=events.file_id AND '
             'replicas.rse_id=events.rse_id) '
             'LEFT JOIN files ON events.file_id=files.file_id '
             'WHERE events.justin_job_id=%d AND events.event_type_id=%d'
             % (justinJobID, justin.event_FILE_ALLOCATED))
             
    jobFiles = justin.select(query)

  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed getting files for the job: ' + str(e))

  processedFileIDs   = []
  unprocessedFileIDs = []

  for jobFile in jobFiles:
    if jobFile['file_did'] in jsonDict['processed_dids'] or \
       jobFile['wan_pfn'] in jsonDict['processed_pfns'] or \
       (jobFile['lan_pfn'] and 
        jobFile['lan_pfn'] in jsonDict['processed_pfns']):
      processedFileIDs.append(jobFile['file_id'])
    else:
      print('Set unprocessed',jsonDict['jobsub_id'],
            jobFile['file_did'],
            jobFile['wan_pfn'], jobFile['lan_pfn'], 
            str(jsonDict['processed_pfns']), file=sys.stderr)
      unprocessedFileIDs.append(jobFile['file_id'])

  # For processed inputs, we just record they got as far as outputting
  # the corresponding output files, for now 
  try:
    updateFileProcessing(processedFileIDs,
                         'outputting',
                         justinJobID, workflowID, stageID)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed updating file statuses: ' + str(e))

  # For unprocessed inputs, we can put them straight back into the
  # unprocessed states (unallocated or failed)
  try:
    updateFileProcessing(unprocessedFileIDs,
                         'unprocessed',
                         justinJobID, workflowID, stageID)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed updating file statuses: ' + str(e))
          
  try:
    processOutputDids(jsonDict['next_stage_dids'],
                      workflowID, stageID, justinJobID)
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed recording next stage outputs: ' + str(e))

  try:
    # We set stageID=-1 so that when 1 is added, the output only 
    #Â StageID of 0 is recorded
    processOutputDids(jsonDict['output_dids'], workflowID, -1, justinJobID)
  except Exception as e:
    return httpError(startResponse, 
                     '500 Internal Server Error', 
                     'Failed recording outputs: ' + str(e))

  try:
    jobscriptRealSeconds = int(float(jsonDict['jobscript_real_seconds']))
    jobscriptCpuSeconds  = int(float(jsonDict['jobscript_user_seconds']) +
                               float(jsonDict['jobscript_sys_seconds']))
    jobscriptMaxRssBytes = int(jsonDict['jobscript_max_rss_kb']) * 1024
  except:
    # We can produce an error if this fails in future...
    jobscriptRealSeconds = 0
    jobscriptCpuSeconds  = 0
    jobscriptMaxRssBytes = 0

  try:
    justin.insertUpdate('UPDATE jobs SET job_state="outputting",'
                        'heartbeat_time=NOW(),'
                        'outputting_time=NOW(),'
                        'jobscript_real_seconds=%d,'
                        'jobscript_cpu_seconds=%d,'
                        'jobscript_max_rss_bytes=%d '
                        'WHERE jobsub_id="%s"'
                        % (jobscriptRealSeconds,
                           jobscriptCpuSeconds,
                           jobscriptMaxRssBytes,
                           jsonDict['jobsub_id']))

    justin.logEvent(eventTypeID = justin.event_JOB_OUTPUTTING,
                    workflowID  = workflowID,
                    stageID     = stageID,
                    justinJobID = justinJobID,
                    siteID      = siteID
                   )

  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed updating jobs: ' + str(e))

  return httpOK(startResponse, 
                json.dumps({ 'user_access_token' : userAccessToken }))

# Return a list of file DIDs allocated to this job.
# This can be used by jobscripts which don't have a keep of input
# DIDs. For example if running lar in legacy sam-web mode.
def getAllocatedFilesMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing jobsub_id in JSON')
      
  try:
    query = ('SELECT justin_job_id '
             'FROM jobs WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"')

    row = justin.select(query, justOne = True)
    justinJobID  = int(row['justin_job_id'])
  except Exception as e:
    return httpError(startResponse,
                     '404 Not Found',
                     'Failed to find job: ' + str(e))

  try:
    # Use the event records to find the right replicas for this job
    query = ('SELECT file_did FROM events '
             'LEFT JOIN files ON events.file_id=files.file_id '
             'WHERE events.justin_job_id=%d AND events.event_type_id=%d'
             % (justinJobID, justin.event_FILE_ALLOCATED))
             
    jobFiles = justin.select(query)

  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed getting files for the job: ' + str(e))

  outputString = ''
  for jobFile in jobFiles:
    outputString += jobFile['file_did'] + '\n'

  return httpOK(startResponse, outputString)

# Legacy support for samweb's updateFileStatus method 
# Only used for putting "consumed" files into the justin "outputting" state
def samwebUpdateFileStatusMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse, 
                     '400 Bad Request',
                     'Missing jobsub ID in workflow URI')

  try:
   print('==samwebUpdateFileStatusMethod ' + str(jsonDict), file=sys.stderr)
  except:
   pass

  try:
    status = jsonDict['status'][0]
  except:
    return httpOK(startResponse, '')

  if status != 'consumed':
    #Â Anything other than 'consumed' status just gets an OK
    return httpOK(startResponse, '')

  try:
    pfn = jsonDict['filename'][0]

    if not justin.stringNoQuotes(pfn):
      raise
  except:
    return httpError(startResponse, '400 Bad Request', 'Valid filename missing')
      
  try:
    query = ('SELECT justin_job_id,workflow_id,stage_id '
             'FROM jobs WHERE jobsub_id="%s"' % jsonDict['jobsub_id'])

    row = justin.select(query, justOne = True)

    justinJobID  = int(row['justin_job_id'])
    workflowID = int(row['workflow_id'])
    stageID   = int(row['stage_id'])
  except:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed finding job')

  try:
    query = ('SELECT files.file_id FROM files '
             'LEFT JOIN replicas ON replicas.file_id=files.file_id '
             'WHERE files.workflow_id=%d AND files.stage_id=%d '
             'AND (wan_pfn="%s" OR lan_pfn="%s") '
             'ORDER BY files.file_id LIMIT 1' 
             % (workflowID, stageID, pfn, pfn))

    fileRow = justin.select(query, justOne=True)
    fileID = fileRow['file_id']
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed finding file from PFN: ' + str(e))

  try:
    query = ('UPDATE files SET state="outputting" '
             'WHERE file_id=%d '
             'AND justin_job_id=%d '
             'AND workflow_id=%d '
             'AND stage_id=%d '
             'AND state="allocated"'
             % (fileID, justinJobID, workflowID, stageID))

    justin.insertUpdate(query)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Failed updating file statuses: ' + str(e))

  return httpOK(startResponse, '')

# After the output files have been successfully uploaded, the wrapper job
# comes back with this method which changes the state of input files from
# outputting to processed, and of output files from recorded to finding.
def confirmResultsMethod(startResponse, jsonDict):

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse,
                     '400 Bad Request',
                     'Missing jobsub_id in JSON')

  try:
    query = ('SELECT justin_job_id,workflow_id,stage_id,site_id,job_state '
             'FROM jobs WHERE jobsub_id="' + jsonDict['jobsub_id'] + '"')

    row = justin.select(query, justOne = True)

    justinJobID  = int(row['justin_job_id'])
    workflowID = int(row['workflow_id'])
    stageID   = int(row['stage_id'])
    siteID    = int(row['site_id'])
    jobState  = row['job_state']
  except:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Failed finding job')

  if jobState != 'outputting':
    return httpError(startResponse, 
                     '410 Gone',
                     'Cannot confirm results as job already in %s state'
                     % jobState)

  try:
    justin.insertUpdate('UPDATE files '
                        'SET state="processed",processed_time=NOW(),'
                        'processed_hour=FLOOR(UNIX_TIMESTAMP()/3600),'
                        'processed_site_id=%d '
                        'WHERE state="outputting" AND justin_job_id=%d' 
                        % (siteID, justinJobID))
  except:
    return httpError(startResponse,
                     '500 Internal Server Error', 
                     'Updating processed input files')

  try:
    fileRows = justin.select('SELECT file_did,file_id,stage_id FROM files '
                             'WHERE state="recorded" AND '
                             'creator_justin_job_id=%d' % justinJobID)
  except:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'Getting recorded output files')

  for fileRow in fileRows:
  
    try:
      try:
        rseName   = jsonDict['output_dids'][fileRow['file_did']]['rse_name']
        sizeBytes = jsonDict['output_dids'][fileRow['file_did']]['size_bytes']
        seconds   = jsonDict['output_dids'][fileRow['file_did']]['seconds']
      except:
        sizeBytes = 0
        seconds   = 0.0
        rseName   = None
      
      justin.insertUpdate('UPDATE files SET state="%s",size_bytes=%d '
                          'WHERE file_id=%d' %
                       ('finding' if (fileRow['stage_id'] > 0) else 'output',
                          sizeBytes,
                          fileRow['file_id']))

      justin.logEvent(eventTypeID = justin.event_FILE_CREATED,
                      workflowID = workflowID,
                      stageID = stageID,
                      fileID = fileRow['file_id'],
                      justinJobID = justinJobID,
                      siteID = siteID,
                      rseName = rseName,
                      seconds = seconds
                     )
    except Exception as e:
      return httpError(startResponse,
                       '500 Internal Server Error',
                       'Updating recorded output file: ' + str(e))

  # Record output files uploaded directly to scratch etc
  for url in jsonDict['output_urls']:
    if not justin.stringNoQuotes(url):
      return httpError(startResponse, '400 Bad Request', 'Bad URL for upload')

    try:
      file_id = justin.insertUpdate('INSERT INTO files SET '
                                    'state="output",'
                                    'workflow_id=%d,'
                                    'stage_id=0,'
                                    'file_did="%s",'
                                    'creator_justin_job_id=%d'
                                    % (workflowID, 
                                       url,
                                       justinJobID))

      justin.logEvent(eventTypeID = justin.event_FILE_UPLOADED,
                         workflowID = workflowID,
                         stageID = stageID,
                         fileID = file_id,
                         justinJobID = justinJobID,
                         siteID = siteID
                        )

    except Exception as e:
      return httpError(startResponse,
                       '500 Internal Server Error', 
                       'Recording output file uploaded by url: ' + str(e))

  # Finish up and update job status
  try:
    justin.insertUpdate('UPDATE jobs SET job_state="finished",'
                        'heartbeat_time=NOW(),'
                        'finished_time=NOW() '
                        'WHERE justin_job_id=%d AND job_state="outputting"'
                        % justinJobID)

    justin.logEvent(eventTypeID = justin.event_JOB_FINISHED,
                    workflowID = workflowID,
                    stageID = stageID,
                    justinJobID = justinJobID,
                    siteID = siteID
                   )
  except:
    return httpError(startResponse, 
                     '500 Internal Server Error',
                     'Updating job state to finished')

  return httpOK(startResponse, '')

def checkSecretHash(environ, jsonDict):
  # Check if the hash provided is based on the job's secret
  # or raise an exception

  try:
    secretHash = jsonDict['secret_hash']
    secretTime = int(jsonDict['secret_time'])

    if not secretHash or not secretTime:
      raise

  except Exception as e:
    print('Valid hash of job secret not provided: ' + str(e), file=sys.stderr)
    raise RuntimeError('Forbidden - valid hash of job secret not provided')

  nowTime = int(time.time())
  if secretTime > nowTime + 60:
    raise RuntimeError('Forbidden - secret hash expired (secret=%d,now=%d)'
                       % (secretTime, nowTime ) )

  try:
    row = justin.select('SELECT justin_job_secret,job_state FROM jobs '
                        'WHERE jobsub_id="%s"' % jsonDict['jobsub_id'],
                        justOne = True)
    justinJobSecret = row['justin_job_secret']
    jobState        = row['job_state']
  except:
    raise RuntimeError('Forbidden - failed to get job secret for %s' 
                       % jsonDict['jobsub_id'])
 
  hash = hmac.new(bytes(justinJobSecret, 'UTF-8'),
                                     (jsonDict['method'] +
                                      str(secretTime) +
                                      jsonDict['jobsub_id']
                                     ).encode(),
                                     hashlib.sha256).hexdigest()

  print('method=%s secret_hash=%s hash=%s secret_time=%d now=%d' 
        % (jsonDict['method'], secretHash, hash, secretTime, nowTime), 
           file=sys.stderr)

  if jsonDict['secret_hash'] != secretHash:
    raise RuntimeError('Forbidden - invalid hash given')

  return jobState

#def getAllocatorLock(timeoutSeconds):
#  # Try to get the workflow allocator service lock or raise an exception if
#  # unable to do this for any reason, including a timeout
#  # No try here since we expose exceptions to the caller!
#  justin.cur.execute('SELECT GET_LOCK("justin_allocator", %d) AS result' %
#                     timeoutSeconds)
#
#  row =  justin.cur.fetchone()
#
#  if not row or 'result' not in row:
#    # Something went wrong!
#    raise RuntimeError('justin_allocator lock query failure')
#  
#  if row['result'] == 0:
#    # Failed to get lock in time so raise exception
#    raise RuntimeError('justin_allocator lock timeout')
#
#  # Success!  
#  return

#
# Entry point from mod_wsgi
#
def application(environ, startResponse):

  justin.wsgiCallsCount += 1
  print('Call count (pid=%d): %d' % (os.getpid(), justin.wsgiCallsCount), 
        file=sys.stderr)

  # Quickly reject random GETs etc (if not handled by Apache already)
  if environ['REQUEST_METHOD'] != 'POST':
    return httpError(startResponse,
                     '405 Method not allowed', 
                     'We only support POST')

  try:
    # True should provoke a reconnection attempt.
    # See https://github.com/farcepest/MySQLdb1/blob/master/_mysql.c#L1978
    #Â (Not sure if there is a more authoritative source for this API.)
    justin.conn.ping(reconnect = True)
  except Exception as e:
    return httpError(startResponse,
                     '500 Internal Server Error',
                     'DB connection lost and cannot reconnect: ' + str(e))

  # Avoid leftovers from partial, failed transactions in this instance
  # and reset autocommit
  justin.conn.rollback()
  justin.conn.autocommit(False)
#  row = justin.select('SELECT @@autocommit', justOne = True)
#  print('autocommit=' + str(row), file=sys.stderr)

  if environ['REQUEST_URI'].startswith('/api/get-classads'):
    return getClassadsCall(startResponse, environ)

  if environ['REQUEST_URI'].startswith('/api/samweb/'):
    # Legacy samweb support: put values into dictionary
    try:
      inputLength = int(environ.get('CONTENT_LENGTH', '0'))
      inputString = environ['wsgi.input'].read(inputLength).decode()
      jsonDict = urllib.parse.parse_qs(inputString)
      print('inputString ' + inputString, file=sys.stderr)

      # URIs like /api/samweb/JOBSUBJOBID/SECRET/processes/N/METHOD
      uriSplit = environ['REQUEST_URI'].split('/')
      jsonDict['method']           = 'samweb_' + uriSplit[-1].lower()
      jsonDict['jobsub_id']        = str(uriSplit[3])
      jsonDict['jobscript_secret'] = str(uriSplit[4])

    except Exception as e:
      return httpError(startResponse, 
                       '400 Bad Request', 
                       'Failed to read and parse workflow: ' + str(e))
  else:
    # Standard justIN API based on JSON
    try:
      inputLength = int(environ.get('CONTENT_LENGTH', '0'))
      inputString = environ['wsgi.input'].read(inputLength)
      jsonDict = json.loads(inputString)
    except Exception as e:
      return httpError(startResponse, 
                       '400 Bad Request', 
                       'Failed to read and parse JSON')

  if 'jobsub_id' not in jsonDict or \
     not justin.stringIsJobsubID(jsonDict['jobsub_id']):
    return httpError(startResponse,
              '400 Bad Request', 
              'Missing jobsub_id in JSON')

  # Check jsonDict specifies a method
  if 'method' not in jsonDict:
    return httpError(startResponse,
                     '400 Bad Request',
                     'Missing method in JSON')  

  # Legacy samweb updateFileStatus method (only "consumed" does anything)
  if jsonDict['method'] == 'samweb_updatefilestatus' or \
     jsonDict['method'] == 'samweb_setstatus':
    return samwebUpdateFileStatusMethod(startResponse, jsonDict)

  # Return list of file DIDs allocated to this job
  if jsonDict['method'] == 'get_allocated_files':
    return getAllocatedFilesMethod(startResponse, jsonDict)

  # Get one or more files to process
  # This workflow comes from jobscripts using secrets so we 
  # do not run checkSecretHash() as getFileMethod() checks the secret
  if jsonDict['method'] == 'get_file' or \
     jsonDict['method'] == 'samweb_getnextfile':
    return getFileMethod(startResponse, jsonDict)

  # All other methods are called by the wrapper job using its secret hash
  # which is checked with checkSecretHash()
  try:
    jobState = checkSecretHash(environ, jsonDict)
  except Exception as e:
    return httpError(startResponse, '403 Forbidden', str(e))

  # Get the details of the stage for the job to work on
  if jsonDict['method'] == 'get_jobscript':
    if jobState == 'submitted':
      return getJobscriptMethod(startResponse, jsonDict, environ)
    else:
      print('Duplicate get_jobscript received for job in state %s: %s' 
             % (jobState, str(jsonDict)), file=sys.stderr)
      return httpError(startResponse, 
                 '403 Forbidden', 
                 'Job is no longer in submitted state - duplicate/retry?')

  # Record heartbeats from the wrapper jobs
  if jsonDict['method'] == 'send_heartbeat':
    return sendHeartbeatMethod(startResponse, jsonDict)

  # Job aborted
  if jsonDict['method'] == 'job_aborted':
    return jobAbortedMethod(startResponse, jsonDict)
  
  # Record results of processing files
  if jsonDict['method'] == 'record_results':
    return recordResultsMethod(startResponse, jsonDict)

  # Confirm that uploads went ok
  if jsonDict['method'] == 'confirm_results':
    return confirmResultsMethod(startResponse, jsonDict)

  # Otherwise an error
  return httpError(startResponse, 
                   '400 Bad Request', 
                   'Method in JSON not recognised')

