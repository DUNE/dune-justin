#!/usr/bin/env python3
#
# justin-finder - justIN Finder agent
#
# Copyright 2013-24, Andrew McNab for the University of Manchester
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import sys
import ssl
import pwd
import stat
import json
import time
import base64
import socket
import subprocess
import urllib.request
import pathlib
import xml.dom.minidom

## Needs MySQL-python RPM
#import MySQLdb

# WE NEED TO REMOVE OLD MySQLdb REFERENCES STILL!
import pymysql
pymysql.install_as_MySQLdb()
MySQLdb=pymysql

# Installed by pip3 install of Rucio
import rucio.client

import justin

maxFileFindsPerCycle = 500
sleepSeconds         = 60
updateCondorJobs     = 3600 

class justinError(Exception):
  pass

def logLine(text):
  sys.stdout.write(time.strftime('%b %d %H:%M:%S [') + str(os.getpid()) + ']: ' + text + '\n')
  sys.stdout.flush()

def addFilesMonteCarlo(workflowID, count):
  # Add dummy files to keep track of Monte Carlo processing
  
  for i in range(1, count + 1):
    try:
      query = ('INSERT INTO files SET state="unallocated",'
               'workflow_id=%d,file_did="monte-carlo-%06d-%06d"' 
               % (workflowID, workflowID, i)
              )

      justin.cur.execute(query)
    except Exception as e:
      logLine('Failed inserting dummy Monte Carlo file for workflow %d: %s' 
              % (workflowID, str(e)))
      return

    try:
      query = ('INSERT INTO replicas SET rse_id=%d,file_id=%d,'
               'wan_pfn="%06d",lan_pfn=""'
               % (justin.MonteCarloRseID, 
                  justin.cur.lastrowid,
                  i))

      justin.cur.execute(query)
    except Exception as e:
      logLine('Failed inserting dummy Monte Carlo replica for workflow %d: %s' 
               % (workflowID, str(e)))
      return

    justin.logEvent(eventTypeID = justin.event_FILE_ADDED,
                    workflowID = workflowID,
                    stageID = 1,
                    fileID = justin.cur.lastrowid)

  # All ok, so commit it all to the DB
  justin.conn.commit()

def findFilesRucio(didClient, workflowID, scope, name):
  # Find files to be processed for each workflow in the finding state

  try:
    didsList = didClient.list_files(scope, name)
  except Exception as e:
    logLine("Reading file list from Rucio fails with: " + str(e))
    return

  for did in didsList:  
    try:
      query = ('INSERT INTO files SET '
               'workflow_id=' + str(workflowID) + ',' +
               'file_did="' + did['scope'] + ':' + did['name'] + '" '
               'ON DUPLICATE KEY UPDATE workflow_id=workflow_id'
              )

      justin.cur.execute(query)
    except Exception as e:
      logLine('Failed inserting DID %:%s' % (did['scope'], did['name']))
      return

    justin.logEvent(eventTypeID = justin.event_FILE_ADDED,
                    workflowID = workflowID,
                    stageID = 1,
                    fileID = justin.cur.lastrowid)

  # All ok, so commit it all to the DB
  justin.conn.commit()

def findFilesMetaCat(workflowID, mql):
  # Find files matching the mql query from MetaCat

  for i in range(3,0,-1):
    try:
      response = urllib.request.urlopen(
                    "%s/data/query?with_meta=no&with_provenance=no" 
                     % justin.metacatServerInputsURL, data = mql.encode())

      if response.status != 200:
        logLine("Reading files list from MetaCat fails with HTTP code "
                + str(response.status))
        return

      fileLines = response.readlines()
    except Exception as e:    
      logLine("Reading files list from MetaCat fails with: " + str(e))
      
      if i > 1:
        # Try again as Metacat has transitory errors on large queries
        # which seem to go away when repeated
        time.sleep(1)        
        continue
      else:
        # Give up
        return

  for fileLine in fileLines:
    # Remove any weirdness
    if fileLine[0] < 32:
      fileLine = fileLine[1:]

    try:
      fileDict = json.loads(fileLine)
    except:
      logLine("Parsing line from MetaCat fails with: " + str(e))
      return
        
    try:
      namespace = fileDict['namespace']
      name      = fileDict['name']
      size      = int(fileDict['size'])
    except:
      logLine('Ignore file with invalid SCOPE:NAME/SIZE from MetaCat: '
              + str(fileDict))
      continue
    
    try:
      query = ('INSERT INTO files SET '
               'workflow_id=%d,file_did="%s:%s",size_bytes=%d '
               'ON DUPLICATE KEY UPDATE workflow_id=workflow_id'
               % (workflowID, namespace, name, size)
              )

      justin.cur.execute(query)
      
    except Exception as e:
      logLine('Failed inserting DID %:%s' % 
              ( fileDict['namespace'], fileDict['name'] ))
      return

    justin.logEvent(eventTypeID = justin.event_FILE_ADDED,
                    workflowID = workflowID,
                    stageID = 1,
                    fileID = justin.cur.lastrowid)

  # All ok, so commit it all to the DB
  justin.conn.commit()

def findFiles():
  # Find files to be processed for each workflow in the running state
  # when refind_next_time says finding is due but only if it's set
  # By default we use MetaCat, but MQLs starting rucio-dataset or monte-carlo 
  # are handled directly.

  logLine('Start of finding files')

  query = ('SELECT workflow_id,mql,refind_seconds,'
           'UNIX_TIMESTAMP(refind_end_time) AS refind_end,'
           'UNIX_TIMESTAMP(refind_next_time) AS refind_next '
           'FROM workflows '
           'WHERE state="running" AND '
           'refind_next_time <= NOW() AND '
           'refind_next_time > "%s" '
           'ORDER BY workflow_id' % justin.unixEpoch)

  findingWorkflows = justin.select(query)
  if not findingWorkflows:  
    # Nothing to do
    return

  didClient = None

  for workflow in findingWorkflows:
    refindNextTime = int(workflow['refind_next'])
    refindEndTime  = int(workflow['refind_end'])
    refindSeconds  = int(workflow['refind_seconds'])
  
    logLine('Finding files for Workflow %d with MQL %s (%d,%d,%d)' %
            (workflow['workflow_id'], workflow['mql'],
             refindNextTime, refindEndTime, refindSeconds))
  
    mqlSplit = workflow['mql'].split()

    # Workflow for a Rucio dataset: "rucio-dataset SCOPE:NAME"
    if len(mqlSplit) == 2 and mqlSplit[0] == 'rucio-dataset':

      if not didClient:      
       # Only set up Rucio if we see a rucio-dataset
       try:
         didClient = rucio.client.didclient.DIDClient()
       except Exception as e:
         logLine("Connect to Rucio fails with: " + str(e))
         continue

      datasetSplit = mqlSplit[1].split(':')
      if len(datasetSplit) == 2:
        findFilesRucio(didClient,
                       workflow['workflow_id'], 
                       datasetSplit[0], 
                       datasetSplit[1])
        
    # Monte Carlo workflow with a count
    elif len(mqlSplit) == 2 and mqlSplit[0] == 'monte-carlo':
      
      try:
        count = int(mqlSplit[1])
      except:
        continue
      
      addFilesMonteCarlo(workflow['workflow_id'], count)

    else:    
      findFilesMetaCat(workflow['workflow_id'], workflow['mql'])
    
    if refindEndTime == 0:
      # We are not doing refinding and we have done the only finding we do
      # So unset refind_next_time
      refindNextTime = 0
    elif refindNextTime == refindEndTime:
      # We have been doing the final refind soon after the end time
      # So unset refind_next_time
      refindNextTime = 0
    else:
      # Next finding will be soon after refind_seconds from now
      refindNextTime = int(time.time()) + refindSeconds

      # BUT if that would be after refind_end_time, force a final refind,
      # "soon" after the end time
      if refindNextTime > refindEndTime:
        refindNextTime = refindEndTime

    try:
      justin.insertUpdate('UPDATE workflows SET '
                          'refind_next_time=FROM_UNIXTIME(%d) '
                          'WHERE workflow_id=%d AND state="running"' 
                          % (refindNextTime, workflow['workflow_id']))
    except Exception as e:
      logLine("Update workflow %d refind_next_time fails with: %s" 
              % (workflow['workflow_id'],  str(e)))
      continue

    justin.conn.commit()

def findReplicas():
  # Find replicas of files in the finding state

  logLine('Start findng replicas')

  query = (
    'SELECT workflows.workflow_id,stages.stage_id '
    'FROM workflows '
    'LEFT JOIN stages ON workflows.workflow_id=stages.workflow_id '
    'LEFT JOIN files ON workflows.workflow_id=files.workflow_id AND '
    'stages.stage_id=files.stage_id '
    'WHERE workflows.state="running" AND files.state="finding" '
    'GROUP BY workflows.workflow_id,stages.stage_id '
    'ORDER BY stage_priority DESC,RAND() LIMIT 1')

  try:
    stageRow = justin.select(query, justOne = True)
  except Exception as e:
    logLine('Finding workflow/stage to find replicas for fails: %s' % str(e))
    return

  if not stageRow:
    logLine('No files with replicas to be found')
    return

  workflowID = stageRow['workflow_id']
  stageID   = stageRow['stage_id']
  logLine('Looking for replicas in finding state for workflow=%d stage=%d'
          % (workflowID, stageID))

  # Make a list of up to maxFileFindsPerCycle files to work on 
  # but only for Rucio DIDs in scope:name format
  didsList = []

  try:
    fileRows = justin.select(
           'SELECT files.workflow_id,files.stage_id,file_id,file_did '
           'FROM files '
           'LEFT JOIN workflows ON workflows.workflow_id=files.workflow_id '
           'WHERE files.workflow_id=%d '
           'AND files.stage_id=%d '
           'AND files.state="finding" '
           'AND workflows.state="running" '
           'AND file_did LIKE "%%:%%" '
           'ORDER BY file_id LIMIT %d'
           % (workflowID, stageID, maxFileFindsPerCycle))

  except Exception as e:
    logLine('Finding list of files to find fails: ' + str(e))
    return
  
  # Nothing to do
  if not fileRows:
    logLine('No files found in finding state')
    return

  # Check we can talk to Rucio
  try:
    repClient = rucio.client.replicaclient.ReplicaClient()
  except Exception as e:
    logLine("Connect to Rucio fails with: " + str(e))
    return

  # Cache rse_id of every known RSE into a local dictionary
  # unless it is decommissioned
  storagesDict = {}
  allStorages = justin.select('SELECT rse_name,rse_id,needs_pin '
                              'FROM storages WHERE NOT decommissioned')
  
  for storageRow in allStorages:
    storagesDict[storageRow['rse_name']] = { 
                     'rse_id'    : storageRow['rse_id'],
                     'needs_pin' : bool(storageRow['needs_pin']) }

#  # Receives a list of unique (workflowID,stageID) combinations
#  workflowsStages = set()
  filesList = []

  # Make a list of files we want replicas for, in the Rucio API's format
  for fileRow in fileRows:
    (didScope, didName) = fileRow['file_did'].split(':')
    filesList.append({'scope' : didScope, 'name'  : didName })

  # Use the Rucio API to get big lists of replicas for the files
  try:
    # WE FORCE THE RETURNED SCHEME TO BE root FOR NOW
    # domain='all' does not seem to work for now
    wanRepsList = repClient.list_replicas(filesList,
                                          schemes=['root'], 
                                          domain='wan')

    lanRepsList = repClient.list_replicas(filesList,
                                          schemes=['root'], 
                                          domain='lan')

  except Exception as e:
    logLine("Reading RSE replicas info from Rucio fails with: " + str(e))
    return

  # Make a dictionary with DIDs as keys and lists of replicas from Rucio
  # as the values
  repsDict = {}
  for rep in list(wanRepsList) + list(lanRepsList):
    if rep['scope']+':'+rep['name'] not in repsDict:
      repsDict[rep['scope']+':'+rep['name']] = []
      
    for pfn in rep['pfns']:
      pfnDict = rep['pfns'][pfn]
      pfnDict['pfn'] = pfn
      repsDict[rep['scope']+':'+rep['name']].append(pfnDict)

#  logLine('repsDict: ' + str(repsDict))
                      
  # Go through the files again, pulling out the replica info obtained 
  # for the file's DID
  for fileRow in fileRows:
    if fileRow['file_did'] not in repsDict:
      # No replicas for this file! All deleted?
      logLine('File (%d,%d,%s) has no replicas in Rucio' %
                (fileRow['workflow_id'], 
                 fileRow['stage_id'],
                 fileRow['file_did']))

      query = ('UPDATE files SET state="notfound" WHERE file_id=%d' %
               fileRow['file_id'])    
      try:
        justin.cur.execute(query)
      except Exception as e:
        logLine('Failed setting file (%d,%d,%s) to notfound: %s' %
                (fileRow['workflow_id'], 
                 fileRow['stage_id'],
                 fileRow['file_did'],
                 str(e)))
        # We give up if this happens, without the commit
        return
        
      continue
      
    pfnsList = repsDict[fileRow['file_did']]    
    rses = {}

    # Save the wan and lan PFNs returned for each RSE with a replica
    # These go into the SAME rses dictionary so separate LAN and WAN
    # replica information is merged into one data dictionary
    for pfnDict in pfnsList:
        if pfnDict['type'] != 'DISK':
          continue

        if pfnDict['rse'] not in rses:
            rses[pfnDict['rse']] = {}
      
        if pfnDict['domain'] == 'lan':
            rses[pfnDict['rse']]['lan_pfn'] = justin.fixPfn(pfnDict['pfn'])
        else:
            rses[pfnDict['rse']]['wan_pfn'] = justin.fixPfn(pfnDict['pfn'])

    replicasInserted = 0
    # Now go through the replicas for this file, RSE by RSE
    for rse in rses:

        # Skip if RSE not listed (possibly because decommissioned=True)
        if rse not in storagesDict:
          continue
    
        try:
          lanPFN = rses[rse]['lan_pfn'] 
        except:
          lanPFN = ''

        try:
          wanPFN = rses[rse]['wan_pfn'] 
        except:
          wanPFN = ''
          
        # If PFN wan=lan then set lan PFN to ''        
        if wanPFN == lanPFN:
          lanPFN = ''       
  
        if storagesDict[rse]['needs_pin']:
          accessibleUntil = ',accessible_until="%s" ' % justin.unixEpoch
        else:
          accessibleUntil = ''
        
        if workflowID == justin.awtWorkflowID:
          try:
            logLine('Try to update AWT file: %s %s' % (wanPFN, lanPFN))
          
            # For AWT, if the file is put back into finding then we can
            # update the replicas with change PFNs from Rucio
            query = ('UPDATE replicas SET '
                     'wan_pfn="%s",lan_pfn="%s" '
                     'WHERE ' 
                     'workflow_id=%d AND stage_id=%d AND '
                     'rse_id=%d AND file_id=%s'
                     % (wanPFN,
                        lanPFN,
                        workflowID, stageID,
                        storagesDict[rse]['rse_id'],
                        fileRow['file_id']))
                        
            justin.insertUpdate(query)
          except:
            # For AWT, we just carry on
            pass

          try:
            logLine('Try to insert AWT file: %s %s' 
                    % (wanPFN, lanPFN))

            query = ('INSERT INTO replicas SET '              
                     'workflow_id=%d,stage_id=%d,'
                     'rse_id=%d,file_id=%d,'
                     'wan_pfn="%s",lan_pfn="%s"' %
                     (workflowID, stageID,
                      storagesDict[rse]['rse_id'],
                      fileRow['file_id'],
                      wanPFN,
                      lanPFN))

            justin.select(query)
          except:
            # For AWT, we just carry on
            pass

          # But we treat whatever happened as everything to be done
          replicasInserted += 1

        else:
          try:
            # workflow_id/stage_id are only recorded with the replicas 
            # to make it faster to make charts etc in the Dashboard
            query = ('INSERT INTO replicas SET '
                     'workflow_id=%d,stage_id=%d,'
                     'rse_id=%d,file_id=%d,'
                     'wan_pfn="%s",lan_pfn="%s"%s' %
                     (workflowID, stageID,
                      storagesDict[rse]['rse_id'],
                      fileRow['file_id'],
                      wanPFN,
                      lanPFN,
                      accessibleUntil))

            justin.select(query)
            replicasInserted += 1

            justin.logEvent(eventTypeID = justin.event_REPLICA_ADDED,
                            workflowID = workflowID,
                            stageID = stageID,
                            fileID = fileRow['file_id'],
                            rseID = storagesDict[rse]['rse_id'])
   
          except Exception as e:
            logLine('Failed inserting replica: ' + str(e))
            # We give up if this happens, without the commit
            return
           
#        # Add workflow/stage to the list to have updated stats at the end
#        workflowsStages.add((fileRow['workflow_id'], fileRow['stage_id']))

    if replicasInserted:
      # If we got this far, then we found all the replicas for this 
      # file so we update its state to unallocated (not finding any more)
      try:
        query = ('UPDATE files SET state="unallocated" WHERE file_id=%d' %
                 fileRow['file_id'])

        justin.cur.execute(query)
      except Exception as e:
        logLine('Failed updating file (%d,%d,%s): %s' %
                (fileRow['workflow_id'],
                 fileRow['stage_id'],
                 fileRow['file_did'],
                 str(e)))
        # We give up if this happens, without the commit
        return
    else:
      try:
        query = ('UPDATE files SET state="notfound" WHERE file_id=%d' %
                 fileRow['file_id'])

        justin.cur.execute(query)
      except Exception as e:
        logLine('Failed updating file (%d,%d,%s): %s' %
                (fileRow['workflow_id'], 
                 fileRow['stage_id'],
                 fileRow['file_did'],
                 str(e)))
        # We give up if this happens, without the commit
        return
    
  # All ok, so commit it all to the DB
  justin.conn.commit()

def executeMetaCatCommand(args):

  for i in range(1, 4):  
    ret = os.system(
     'export X509_USER_PROXY=%s ; '
     'export METACAT_AUTH_SERVER_URL=%s ; '
     'export METACAT_SERVER_URL=%s ; '
     '/usr/local/bin/metacat auth login -m x509 dunepro ; '
     '/usr/local/bin/metacat %s' % 
     (justin.metacatAuthServerURL, justin.metacatServerOutputsURL, 
      justin.jobsProductionProxyFile, args))
     
    if ret == 0:
      break

    time.sleep(1) 

  return ret

def findSubmittedWorkflows():
  # Find workflows waiting in the submitted state and set them to running

  logLine('Start of finding submitted workflows')

  datasetsToCreate = set()
  workflowsToStart = []

  try:
    workflows = justin.select('SELECT workflows.workflow_id,scope_name,'
                    'condor_groups.condor_group_id '
                    'FROM workflows '
                    'LEFT JOIN scopes '
                    'ON workflows.scope_id=scopes.scope_id '
                    'LEFT JOIN condor_groups '
                    'ON condor_groups.condor_group_id=scopes.condor_group_id '
                    'WHERE workflows.state="submitted" '
                    'ORDER BY workflows.workflow_id')
  except:
    logLine('Failed to get list of submitted workflows')
    return

  for workflow in workflows:
    try:
      condorGroupID = int(workflow['condor_group_id'])
    except:
      condorGroupID = 0
  
    workflowsToStart.append( (workflow['workflow_id'],condorGroupID) )

    try:
      stagesOutputs = justin.select(
          'SELECT destination,lifetime_seconds,stage_id '
          'FROM stages_outputs '
          'WHERE workflow_id=%d' % workflow['workflow_id'])
    except Exception as e:
      logLine('Failed to get list of outputs of submitted workflows: '
              + str(e))
      return

    for stageOutput in stagesOutputs:
      if not stageOutput['destination'].startswith('https://'):
        # A Rucio dataset
        try:
          stagesOutputStorages = justin.select(
           'SELECT rse_name FROM stages_output_storages '
           'LEFT JOIN storages '
           'ON storages.rse_id=stages_output_storages.rse_id '
           'WHERE stages_output_storages.workflow_id=%d '
           'AND stages_output_storages.stage_id=%s '
           % (workflow['workflow_id'], 
              stageOutput['stage_id']))
        except Exception as e:
          logLine('Failed to get list of output RSE: ' + str(e))
          return
          
        rseNames = []
        for stageOutputStorage in stagesOutputStorages:
          rseNames.append(stageOutputStorage['rse_name'])

        if rseNames:
          rseExpression = '|'.join(rseNames)
        else:
          rseExpression = justin.rseDisksExpression
        
        datasetsToCreate.add( (workflow['scope_name'],
                               stageOutput['destination'],
                               stageOutput['lifetime_seconds'],
                               rseExpression) )

# THIS IS NOW DONE WITH PER-RSE DATASETS IN THE INFO COLLECTOR
#    datasetsToCreate.add( ('justin-logs', 
#                           'workflow_%d' % workflow['workflow_id'],
#                           604800,
#                           justin.rseDisksExpression) )

  if datasetsToCreate:
    try:
      logLine('Try to create DID and Rule Rucio clients')
      didClient  = rucio.client.didclient.DIDClient()
      ruleClient = rucio.client.ruleclient.RuleClient()
    except Exception as e:
      logLine("Connect to Rucio fails with: " + str(e))
      return
      
    for (scopeName,datasetName,lifetimeSeconds,rseExpression) \
      in datasetsToCreate:

      # Create the dataset in MetaCat
      logLine('Try to add MetaCat dataset %s:%s' % (scopeName, datasetName))
      ret = executeMetaCatCommand('dataset create %s:%s' 
                                  % (scopeName, datasetName))
      # Carry on regardless if ret != 0 since we can't tell from the metacat 
      # return code if the dataset just already exists or there was a 
      # genuine problem ...

      # Create the dataset in Rucio if not already there
      try:
        logLine('Try to add Rucio dataset %s:%s' % (scopeName, datasetName))
        ret = didClient.add_dataset(scope    = scopeName,
                                    name     = datasetName)
                                    
      except rucio.common.exception.DataIdentifierAlreadyExists:
        pass

      except Exception as e:
        logLine("Exception adding dataset %s:%s : %s" 
                % (scopeName, datasetName, str(e)))
        return

      else:
        if not ret:
          logLine("Failed adding dataset %s:%s" % (scopeName, datasetName))
          return

        # Create a rule for the new dataset
        try:
          logLine('Try to add rule for %s:%s' % (scopeName, datasetName))
          ret = ruleClient.add_replication_rule(
                     dids = [ {'scope' : scopeName,
                               'name'  : datasetName} ], 
                     copies = 1, 
                     rse_expression = rseExpression, 
                     lifetime = lifetimeSeconds)
        except Exception as e:
          logLine('Exception adding rule for %s:%s : %s'
                  % (scopeName, datasetName, str(e)))
          ret = None

        if not ret:
          logLine("Failed adding rule for %s:%s" % (scopeName, datasetName))
          return

# SHOULD ALSO CHECK FOR EXISTING RULES WE CREATED FOR THIS DATASET THAT
# NEED THEIR LIFETIME EXTENDING? THIS IS NEEDED FOR LONG WORKFLOWS?
      
# We now use dune:all in MetaCat for everything ?!?!
# This is set in the wrapper jobs
#
#      try:
#        executeMetaCatCommand("dataset create '%s:%s'" % (scopeName, datasetName))
#      except:
#        # We assume failures are due to the dataset already being registered
#        pass

  # All successful (or not needed) so mark workflows as running
  # Define refind_next_time as now to force at least one finding cycle
  # even if not refinding repeatedly
  for (workflowID, condorGroupID) in workflowsToStart:
    try:
        query = ('UPDATE workflows SET '
                 'started=NOW(),'
                 'state="running",'
                 'refind_next_time=NOW(),'
                 'condor_group_id=%d '
                 'WHERE workflow_id=%d' % (condorGroupID, workflowID))

        justin.insertUpdate(query)
    except Exception as e:
        logLine("Exception setting workflow running: %s" % str(e))
        return

  justin.conn.commit()

def findFinishedWorkflows():
  # Find workflows with all files in terminal states and set to finished

  logLine('Start of finding finished workflows')

  try:
    query = ('SELECT workflow_id FROM workflows '
             'WHERE state="running" ORDER BY workflow_id')

    justin.cur.execute(query)

    findingWorkflows = justin.cur.fetchall()
  except:
    logLine('Failed to get list of running workflows')
    return
    
  if not findingWorkflows:
    logLine('No running workflows to check')
    return

  for workflow in findingWorkflows:

    try:
      query = ('SELECT COUNT(*) AS count FROM files '
               'WHERE workflow_id=%d AND '
               'state <> "processed" AND '
               'state <> "notfound" AND '
               'state <> "failed" AND ' 
               'state <> "recorded" AND '
               'state <> "output"' %
               int(workflow['workflow_id']))
             
      justin.cur.execute(query)
      count = int(justin.cur.fetchone()['count'])
    except Exception as e:
      logLine('Failed to count non-terminal files for Workflow ID %d: %s' % 
              (workflow['workflow_id'], str(e)))
      continue

    if count > 0:
      # Some files have not reached terminal states
      logLine('Still non-terminal files for Workflow ID %d (%d)' % 
              (workflow['workflow_id'], count))
      continue
    
    try:
      query = ('UPDATE workflows SET state="finished",finished=NOW() '
               'WHERE workflow_id=%d' % int(workflow['workflow_id']))
    
      justin.cur.execute(query)
    except:
      logLine('Failed to update state to finished for Workflow ID %d' % 
              workflow['workflow_id'])
      continue

    logLine('Workflow ID %d set to finished' % workflow['workflow_id'])
  justin.conn.commit()

def resetFilesForJob(justinJobID,
                     jobsubID,
                     workflowID,
                     stageID,
                     siteID):   

    # Then find the file(s) allocated to this stalled or aborted job 
    # (IN THE FUTURE WE SHOULD DO SOMETHING CLEVER WITH Rucio/MetaCat 
    #  CHECKS FOR FILE IN THE outputting STATE)
    try:
      query = ('SELECT file_id,allocations,max_allocations,file_did,state '
               'FROM files '
               'WHERE (state="allocated" OR state="outputting") '
               'AND justin_job_id=%d '
               'ORDER BY file_id' % justinJobID)

      justin.cur.execute(query)

      fileRows = justin.cur.fetchall()
    except:
      logLine('Failed to get list of files of the stalled or aborted job')
      # We should give up at this point rather than commit a mess?
      sys.exit(0)
    
    for fileRow in fileRows: 
    
      if fileRow['allocations'] >= fileRow['max_allocations']:
        # Reached max allocations allowed for this file
        # Mark file as failed in case it is the file causing the problem
        # (eg pathological file causes memory to balloon leading to the site
        # killing the job, and to us the job just appears stalled.)

        try:
          # Only update file state if still allocated to THIS bad job.
          # This safeguard should not be needed for maxAllocations files.
          # Alternatively, we could have done a SELECT ... FOR UPDATE
          # to use a lock.
          query = ('UPDATE files SET state="failed" '
                   'WHERE file_id=%d AND justin_job_id=%d' 
                   % (fileRow['file_id'], justinJobID))

          affectedRows = justin.cur.execute(query)
        except:
          logLine('Failed to set file %s to failed in '
                  'workflow %d, stage %d after job %s' 
                  % (fileRow['file_did'], workflowID, 
                     stageID, jobsubID))
          # Again, just stop for now on errors
          sys.exit(0)
            
        if affectedRows:
          justin.logEvent(eventTypeID = justin.event_FILE_SET_TO_FAILED,
                          workflowID = workflowID,
                          stageID = stageID,
                          fileID = fileRow['file_id'],
                          justinJobID = justinJobID,
                          siteID = siteID)
      else:
      
        try:
          # Only update file state if still allocated to THIS bad job.
          # This safeguard is to avoid a race condition between getting the 
          # list of bad jobs and then resetting their files.
          # Alternatively, we could have done a SELECT ... FOR UPDATE
          # to use a lock but this would slow down the allocation process.
          query = ('UPDATE files SET state="unallocated" '
                   'WHERE file_id=%d AND justin_job_id=%d' 
                   % (fileRow['file_id'], justinJobID))

          affectedRows = justin.cur.execute(query)
        except:
          logLine('Failed to set file %s to unallocated in '
                  'workflow %d, stage %d after job %s' 
                  % (fileRow['file_did'], workflowID, 
                     stageID, jobsubID))
          # Again, just stop for now on errors
          sys.exit(0)

        if affectedRows:
          justin.logEvent(eventTypeID = 
                          justin.event_FILE_OUTPUTTING_RESET 
                          if fileRow['state'] == 'outputting'
                          else justin.event_FILE_ALLOCATED_RESET,
                          workflowID = workflowID,
                          stageID = stageID,
                          fileID = fileRow['file_id'],
                          justinJobID = justinJobID,
                          siteID = siteID)

          logLine('Reset file %s in %d,%d to unallocated for job %s' % 
                  (fileRow['file_did'], workflowID,
                  stageID, jobsubID))

def findStalledAbortedJobs():
  # Find jobs which have 
  # (1) not sent a heartbeat in jobStalledSeconds, or
  # (2) which have reported they have aborted or that outputting failed
  #     and still have has_allocations set
  # and then reset their file allocations
  #
  # We handle bad waiting jobs in findStalledCondorJobs()

  logLine('Start of finding stalled and aborted jobs')

  try:
    query = ('SELECT workflow_id,stage_id,justin_job_id,jobsub_id,site_id,'
             'job_state '
             'FROM jobs '
             'WHERE ((job_state="started" OR '
             'job_state="processing" OR '
             'job_state="outputting") AND '
             '(heartbeat_time < DATE_SUB(NOW(),INTERVAL %d SECOND))) OR '
             '((job_state="aborted" OR '
             '  job_state="jobscript_error" OR '
             '  job_state="outputting_failed") AND has_allocations) '
             'ORDER BY justin_job_id' % justin.jobStallSeconds)

    justin.cur.execute(query)

    jobRows = justin.cur.fetchall()
  except Exception as e:
    logLine('Failed to get list of stalled or aborted jobs: ' + str(e))
    return
    
  if not jobRows:
    logLine('No stalled or aborted jobs to process')
    return

  for jobRow in jobRows:

    if jobRow['job_state'] not in ['aborted', 'jobscript_error', 
                                   'outputting_failed']:
      # If not here due to an abort, we mark the job as stalled
      try:
        query = ('UPDATE jobs SET job_state="stalled",finished_time=NOW(),'
                 'has_allocations=FALSE '
                 'WHERE justin_job_id=%d' % jobRow['justin_job_id'])
    
        justin.cur.execute(query)
      except Exception as e: 
        logLine('Failed to set job %s to stalled: %s' 
              % (jobRow['jobsub_id'], str(e)))
        continue

      logLine('Job %s set to stalled from %s' % (jobRow['jobsub_id'], 
                                                 jobRow['job_state']))
    
      justin.logEvent(eventTypeID = justin.event_JOB_STALLED_HEARTBEAT,
                      workflowID = jobRow['workflow_id'],
                      stageID = jobRow['stage_id'],
                      justinJobID = jobRow['justin_job_id'],
                      siteID = jobRow['site_id'])
    else:
      # An aborted, error, failed state, so just reset has_allocations
      try:
        query = ('UPDATE jobs SET has_allocations=FALSE '
                 'WHERE justin_job_id=%d' % jobRow['justin_job_id'])
    
        justin.cur.execute(query)
      except Exception as e: 
        logLine('Failed to set job %s to has_allocations=FALSE: %s' 
              % (jobRow['jobsub_id'], str(e)))
        continue

    # However we got here, reset the job's files
    resetFilesForJob(justinJobID = jobRow['justin_job_id'],
                     jobsubID    = jobRow['jobsub_id'],
                     workflowID  = jobRow['workflow_id'],
                     stageID     = jobRow['stage_id'],
                     siteID      = jobRow['site_id'])

  # Everything went ok so commit
  justin.conn.commit()

def oidcRefreshRequest(refreshToken):
  # Send an OIDC workflow to CILogon to refresh tokens for a user

  postData = urllib.parse.urlencode(
       { 'grant_type'    : 'refresh_token',
         'client_id'     : justin.cilogonClientID,
         'client_secret' : justin.cilogonSecret,
         'refresh_token' : refreshToken
#         ,
#         'scope'         : 'scope=openid profile org.cilogon.userinfo '
#                           'wlcg.capabilityset:/duneana wlcg.groups:/dune '
#                           'wlcg.groups:/dune/production'
       })

  httpRequest = urllib.request.Request('https://cilogon.org/oauth2/token',
                                       data = postData.encode(),
                                       method = 'POST')

  sslContext = ssl.SSLContext()
  sslContext.load_verify_locations(capath = '/etc/grid-security/certificates')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except Exception as e:
    logLine('OIDC tokens HTTP refresh workflow fails: ' + str(e))
    return None
  else:
    if response.status != 200:
      logLine('OIDC tokens HTTP response not 200, code=%d' % response.status)
      return None

  try:
    responseData = response.read().decode('utf-8')
    responseDict = json.loads(responseData)
  except Exception as e:
    logLine('OIDC tokens refresh workflow failed loading json: '+str(e))
    return None

  logLine('responseDict=' + str(responseDict))

  try:
    tokensDict = {} 
    tokensDict['access_token'] = responseDict['access_token']
    tokensDict['access_dict']  = json.loads(base64.urlsafe_b64decode(
                     responseDict['access_token'].split('.')[1] + '=='))

    tokensDict['id_token'] = responseDict['id_token']
    tokensDict['id_dict']  = json.loads(base64.urlsafe_b64decode(
                     responseDict['id_token'].split('.')[1] + '=='))

    tokensDict['refresh_token'] = responseDict['refresh_token']
  except Exception as e:
    logLine('OIDC tokens parsing fails: '+str(e))
    return None
  else:
    return tokensDict

def findTokensToRefresh():

  logLine('Start of finding tokens to refresh')

  try:
    query = ('SELECT users.user_id,principal_name,refresh_token '
             'FROM users '
             'LEFT JOIN principal_names '
             'ON principal_names.pn_id=users.main_pn_id '
             'WHERE '
             'DATE_ADD(NOW(), INTERVAL 60 MINUTE) > access_token_expires AND '
             'access_token_expires > "%s" AND '
             '(SELECT expires_time FROM sessions '
             ' WHERE sessions.user_id=users.user_id '
             ' ORDER BY expires_time DESC LIMIT 1) > NOW() AND '
             'refresh_token <> ""'
             % (justin.unixEpoch))
             
    rows = justin.select(query)

  except Exception as e:
    logLine('Failed to find tokens: ' + str(e))
    return

  for row in rows:
    logLine(str(row))
    tokensDict = oidcRefreshRequest(row['refresh_token'])
    logLine(str(tokensDict))
    
    try:
      accessToken        = tokensDict['access_token']
      accessTokenCreated = tokensDict['access_dict']['iat']
      accessTokenExpires = tokensDict['access_dict']['exp']
      refreshToken       = tokensDict['refresh_token']
    except:
      accessToken        = ''
      accessTokenCreated = 0
      accessTokenExpires = 0
      refreshToken       = ''

    try:
      query = ('UPDATE users SET '
               'access_token="%s",'
               'access_token_created=FROM_UNIXTIME(%d),'
               'access_token_expires=FROM_UNIXTIME(%d),'
               'refresh_token="%s" '
               'WHERE user_id=%d' 
               % (accessToken,
                  accessTokenCreated,
                  accessTokenExpires,
                  refreshToken,
                  row['user_id']))
                  
      justin.insertUpdate(query)
    
    except Exception as e:
      logLine('Failed storing refreshed tokens for %s: %s' 
              % (row['principal_name'], str(e)))
      
    else:
      logLine('Tokens refreshed for %s' % row['principal_name'])

  # All ok, so commit it all to the DB
  justin.conn.commit()

# Remove stale values from caches
def manageCaches():

  logLine('Start of manage caches')

  # Remove stale rank cache entries
  try:
    justin.insertUpdate('DELETE FROM sites_ranks_cache '
                        'WHERE cache_time < DATE_SUB(NOW(),INTERVAL %d SECOND)'
                        % justin.sitesRankCacheStale)
  except Exception as e:
    logLine('Delete stale sites rank cache rows fails: ' + str(e))
            
  # All ok, so commit it all to the DB
  justin.conn.commit()

def findStalledCondorJobs():

  logLine('Start of finding stalled HTCondor jobs')

  nowTime = int(time.time())

  # Gather details of HTCondor jobs from our list of schedds
  condorJobs = {}  

  for scheddHostname in justin.htcondorSchedds:
    try: 
      outcome = subprocess.run(
               ['/usr/bin/condor_q',
                '-name',
                scheddHostname,
                '-long:json',
                '-attributes',
                'clusterid,procid,jobstatus,enteredcurrentstatus'
               ],
               encoding = 'utf-8',
               stderr   = subprocess.STDOUT,
               stdout   = subprocess.PIPE,
               timeout  = 120
                            ) 
    except Exception as e:
      logLine("condor_q fails with error: " + str(e))
      return

    logLine('condor_q exit code: ' + str(outcome.returncode))
    
    if outcome.returncode != 0:
      logLine("condor_q fails with return code %d" % outcome.returncode)
      return

    try:
      jsonJobs = json.loads(outcome.stdout)
    except:
      logLine('Failed to load JSON output by condor_q')
      return

    for jsonJob in jsonJobs:  
      if 'jobstatus' not in jsonJob or \
         'enteredcurrentstatus' not in jsonJob or \
         'clusterid' not in jsonJob or \
         'procid' not in jsonJob:
        continue
      
      condorJobs['%d.%d@%s' % 
                 (jsonJob['clusterid'], 
                  jsonJob['procid'],
                  scheddHostname)] = \
           (jsonJob['jobstatus'], jsonJob['enteredcurrentstatus'])

  if not condorJobs:
    # If we get nothing at all then something must be going wrong
    logLine('Failed to find any job info from HTCondor')
    return
    
  # Get a list of waiting/running jobs
  query = ('SELECT jobsub_id,job_state,justin_job_id,workflow_id,stage_id,'
           'site_id FROM jobs WHERE job_state IN '
           '("submitted","started","processing","outputting")')

  dbJobRows = justin.select(query)
  for dbJobRow in dbJobRows:
    stalled = False
    
    if dbJobRow['jobsub_id'] not in condorJobs:
      # if Condor is unaware of the job, then it's stalled
      logLine('Job %s is not known to HTCondor - marking as stalled' 
              % dbJobRow['jobsub_id'])
      stalled = True
      
    else:
      (jobStatus, enteredCurrentStatus) = condorJobs[dbJobRow['jobsub_id']]

      # Allow 60 seconds for race conditions, but if DB thinks 
      # job is waiting or running but Condor says something else, 
      # then mark as stalled - but submitted can be in HTCondor HELD
      if ((enteredCurrentStatus < nowTime - 60) and 
          (jobStatus == justin.htcondorREMOVED or 
           jobStatus == justin.htcondorCOMPLETED or 
           (jobStatus == justin.htcondorHELD and 
            dbJobRow['job_state'] != 'submitted'))):
          logLine('Job %s is in HTCondor state %d - marking as stalled' %
                (dbJobRow['jobsub_id'], jobStatus))
          stalled = True
        
    if stalled:
      try:
        justin.insertUpdate('UPDATE jobs '
              'SET job_state="stalled",finished_time=NOW() '
              'WHERE justin_job_id="%s" AND '
              'job_state IN ("submitted","started","processing","outputting")'
              % dbJobRow['justin_job_id'])
      except Exception as e:
        logLine('Failed to update stalled job - exiting')
        sys.exit(0)

      justin.logEvent(eventTypeID = justin.event_JOB_STALLED_HTCONDOR,
                      workflowID = dbJobRow['workflow_id'],
                      stageID = dbJobRow['stage_id'],
                      justinJobID = dbJobRow['justin_job_id'],
                      siteID = dbJobRow['site_id'])

      resetFilesForJob(justinJobID = dbJobRow['justin_job_id'],
                       jobsubID    = dbJobRow['jobsub_id'],
                       workflowID  = dbJobRow['workflow_id'],
                       stageID     = dbJobRow['stage_id'],
                       siteID      = dbJobRow['site_id'])

  # All ok, so commit it all to the DB
  justin.conn.commit()

def oneCycle():

  findTokensToRefresh()
  findSubmittedWorkflows()
  findFiles()
  findReplicas()
  findFinishedWorkflows()
  findStalledAbortedJobs()
  manageCaches()

  # Is it time to update CILogon Jwks
  try:
    lastUpdateCondorJobs = \
              os.stat('/var/run/justin/last-updates/condorjobs').st_mtime
  except:
    lastUpdateCondorJobs = 0

  if lastUpdateCondorJobs + updateCondorJobs < time.time():
    pathlib.Path('/var/run/justin/last-updates/condorjobs').touch(exist_ok=True)
    findStalledCondorJobs()
#
# PROGRAM MAIN
#

if __name__ == '__main__':

  if len(sys.argv) > 1 and sys.argv[1] == '--container':
    notContainer = False
  else:
    notContainer = True

  if notContainer and os.fork() != 0:
    sys.exit() # first parent

  else:
    os.chdir("/")
    os.umask(0)

    if notContainer:
      os.setsid()

    if notContainer and os.fork() != 0:
      sys.exit() # second parent

    else:

      try:
        os.makedirs(justin.justinRunDir + '/last-updates',
                    stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR | 
                    stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)
      except:
        pass
        
      try:
        f = open(justin.justinRunDir + '/finder.pid', 'w')
        f.write(str(os.getpid()) + '\n')
        f.close()
      except:
        print('Failed to create '+justin.justinRunDir+'/finder.pid - exiting')
        sys.exit(1)

      # Close stdin now
      si = open('/dev/null', 'r')
      os.dup2(si.fileno(), sys.stdin.fileno())

      while True:

        # Ensure /var/log/justin directory exists
        try:
          os.makedirs('/var/log/justin', 
                      stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR|stat.S_IRGRP|stat.S_IXGRP|stat.S_IROTH|stat.S_IXOTH)
        except:
          pass

        # Close and reopen stdout->log file, in case of logrotate
        try:
          close(so)
        except:
          pass

        so = open('/var/log/justin/finder', 'a+')
        os.dup2(so.fileno(), sys.stdout.fileno())

        # Close and reopen stderr->log file, in case of logrotate
        try:
          close(se)
        except:
          pass
          
        se = open('/var/log/justin/finder', 'a+')
        os.dup2(se.fileno(), sys.stderr.fileno())

        try:
          pf = open(justin.justinRunDir + '/finder.pid', 'r')
          pid = int(pf.read().strip())
          pf.close()

          if pid != os.getpid():
            print('new ' + justin.justinRunDir + '/finder.pid - exiting')
            break

        except:
          print('no ' + justin.justinRunDir + '/finder.pid - exiting')
          break

        # Fork a subprocess to run each cycle
        cyclePid = os.fork()

        if cyclePid == 0:
          logLine('=============== Start cycle ===============')
          
          justin.readConf()
          
          try:
            justin.conn = MySQLdb.connect(
                             host   = socket.gethostbyname(justin.mysqlHostname),
                             user   = justin.mysqlUsername,
                             passwd = justin.mysqlPassword,
                             db     = justin.mysqlDbName)
            justin.conn.autocommit(False)
            justin.cur = justin.conn.cursor(MySQLdb.cursors.DictCursor)
          except Exception as e:
            logLine('Failed to create database connection (' + 
                    str(e) + ') - skipping cycle')
          else:
            try:
              p = pwd.getpwnam(justin.agentUsername)
              os.chown(justin.justinRunDir + '/last-updates', p[2], p[3])
              os.setgid(p[3])
              os.setuid(p[2])
              oneCycle()
            except Exception as e:
              print('Cycle fails with exception ' + str(e))

            justin.conn.close()

          logLine('================ End cycle ================')
          sys.exit(0)

        # wait for cyclePid subprocess to finish
        os.waitpid(cyclePid, 0)

        # wait the allotted time between cycles
        time.sleep(sleepSeconds)

      sys.exit(0) # if we break out of the while loop then we exit

