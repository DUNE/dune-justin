#!/usr/bin/env python3
#
# justin-finder - justIN Finder agent
#
# Copyright 2013-24, Andrew McNab for the University of Manchester
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import sys
import ssl
import stat
import json
import time
import base64
import hashlib
import tempfile
import subprocess
import urllib.request
import pathlib
import xml.dom.minidom

## Needs MySQL-python RPM
#import MySQLdb

# WE NEED TO REMOVE OLD MySQLdb REFERENCES STILL!
import pymysql
pymysql.install_as_MySQLdb()
MySQLdb=pymysql

# Installed by pip3 install of Rucio
import rucio.client

import justin

maxFileFindsPerCycle = 500
sleepSeconds         = 60
updateCondorJobs     = 3600 

class justinError(Exception):
  pass

def addFilesMonteCarlo(workflowID, count, maxFilesPerWorkflow):
  # Add dummy files to keep track of Monte Carlo processing

  if count > maxFilesPerWorkflow:
    justin.logLine('Too many counter files - workflow failed')

    justin.logEvent(eventTypeID = justin.event_WORKFLOW_FAILED_TOO_MANY_FILES,
                    workflowID = workflowID)

    justin.insertUpdate('UPDATE workflows SET state="finished",'
              'state_message="Failed due to more than %d counter files" '
              'WHERE workflow_id=%d' 
              % (maxFilesPerWorkflow, workflowID))

    return False
  
  for i in range(1, count + 1):
    try:
      query = ('INSERT INTO files SET state="unallocated",'
               'workflow_id=%d,file_did="monte-carlo-%06d-%06d"' 
               % (workflowID, workflowID, i)
              )

      justin.cur.execute(query)
    except Exception as e:
      justin.logLine('Failed inserting dummy Monte Carlo file for workflow %d: %s' 
              % (workflowID, str(e)))
      return False

    fileID = justin.cur.lastrowid

    try:
      query = ('INSERT INTO replicas SET rse_id=%d,file_id=%d,'
               'wan_pfn="%06d",lan_pfn=""'
               % (justin.MonteCarloRseID, 
                  fileID,
                  i))

      justin.cur.execute(query)
    except Exception as e:
      justin.logLine('Failed inserting dummy Monte Carlo replica for workflow %d: %s' 
               % (workflowID, str(e)))
      return False

    justin.logEvent(eventTypeID = justin.event_FILE_ADDED,
                    workflowID = workflowID,
                    stageID = 1,
                    fileID = fileID)

  # All ok, so commit it all to the DB
  justin.conn.commit()
  return True

def findFilesRucio(didClient, workflowID, scope, name, maxFilesPerWorkflow):
  # Find files to be processed for each workflow in the finding state

  try:
    didsList = didClient.list_files(scope, name)
  except Exception as e:
    justin.logLine("Reading file list from Rucio fails with: " + str(e))
    return False

  if len(didsList) > maxFilesPerWorkflow:
    justin.logLine('Too many files found - workflow failed')

    justin.logEvent(eventTypeID = justin.event_WORKFLOW_FAILED_TOO_MANY_FILES,
                    workflowID = workflowID)

    justin.insertUpdate('UPDATE workflows SET state="finished",'
              'state_message="Failed due to more than %d input files" '
              'WHERE workflow_id=%d' 
              % (maxFilesPerWorkflow, workflowID))

    return False

  for did in didsList:  
    try:
      query = ('INSERT INTO files SET '
               'workflow_id=' + str(workflowID) + ',' +
               'file_did="' + did['scope'] + ':' + did['name'] + '" '
               'ON DUPLICATE KEY UPDATE workflow_id=workflow_id'
              )

      justin.cur.execute(query)
    except Exception as e:
      justin.logLine('Failed inserting DID %:%s' % (did['scope'], did['name']))
      return False

    justin.logEvent(eventTypeID = justin.event_FILE_ADDED,
                    workflowID = workflowID,
                    stageID = 1,
                    fileID = justin.cur.lastrowid)

  # All ok, so commit it all to the DB
  justin.conn.commit()
  return True

def findFilesMetaCat(workflowID, mql, maxFilesPerWorkflow):
  # Find files matching the mql query from MetaCat

  for i in range(3,0,-1):
    try:
      response = urllib.request.urlopen(
                    "%s/data/query?with_meta=no&with_provenance=no" 
                     % justin.metacatServerInputsURL, data = mql.encode())

      if response.status != 200:
        justin.logLine("Reading files list from MetaCat fails with HTTP code "
                + str(response.status))
        return False

      fileLines = response.readlines()
    except Exception as e:    
      justin.logLine("Reading files list from MetaCat fails with: " + str(e))

# str(e) is 'HTTP Error 400: Bad Request' if the MQL is badly formed. We could
# abort the workflow if we detect that, but we need to be sure its not 
# something going wrong inside MetaCat sometimes. Also the format of the
# string could change. Maybe ask that MetaCat commit to a specific code or
# message as part of the API. We could just pause the workflow if this goes 
# on too long.
      
      if i > 1:
        # Try again as Metacat has transitory errors on large queries
        # which seem to go away when repeated
        time.sleep(1)        
        continue
      else:
        # Give up
        return False

  if len(fileLines) > maxFilesPerWorkflow:
    justin.logLine('Too many files found - workflow failed')

    justin.logEvent(eventTypeID = justin.event_WORKFLOW_FAILED_TOO_MANY_FILES,
                    workflowID = workflowID)

    justin.insertUpdate('UPDATE workflows SET state="finished",'
              'state_message="Failed due to more than %d input files" '
              'WHERE workflow_id=%d' 
              % (maxFilesPerWorkflow, workflowID))

    return False

  for fileLine in fileLines:
    # Remove any weirdness
    if fileLine[0] < 32:
      fileLine = fileLine[1:]

    try:
      fileDict = json.loads(fileLine)
    except:
      justin.logLine("Parsing line from MetaCat fails with: " + str(e))
      return False
        
    try:
      namespace = fileDict['namespace']
      name      = fileDict['name']
      size      = int(fileDict['size'])
    except:
      justin.logLine('Ignore file with invalid SCOPE:NAME/SIZE from MetaCat: '
              + str(fileDict))
      continue
    
    try:
      query = ('INSERT INTO files SET '
               'workflow_id=%d,file_did="%s:%s",size_bytes=%d '
               'ON DUPLICATE KEY UPDATE workflow_id=workflow_id'
               % (workflowID, namespace, name, size)
              )

      justin.cur.execute(query)
      
    except Exception as e:
      justin.logLine('Failed inserting DID %:%s' % 
              ( fileDict['namespace'], fileDict['name'] ))
      return False

    justin.logEvent(eventTypeID = justin.event_FILE_ADDED,
                    workflowID = workflowID,
                    stageID = 1,
                    fileID = justin.cur.lastrowid)

  # All ok, so commit it all to the DB
  justin.conn.commit()
  return True

def findFiles():
  # Find files to be processed for each workflow in the running state
  # when refind_next_time says finding is due but only if it's set
  # By default we use MetaCat, but MQLs starting rucio-dataset or monte-carlo 
  # are handled directly.

  justin.logLine('Start of finding files')

  query = ('SELECT workflows.workflow_id,mql,refind_seconds,'
           'UNIX_TIMESTAMP(refind_end_time) AS refind_end,'
           'UNIX_TIMESTAMP(refind_next_time) AS refind_next,'
           'scope_name,max_files_per_workflow '
           'FROM workflows '
           'LEFT JOIN scopes ON workflows.scope_id=scopes.scope_id '
           'WHERE state="running" AND '
           'refind_next_time <= NOW() AND '
           'refind_next_time > "%s" '
           'ORDER BY workflows.workflow_id' % justin.unixEpoch)

  findingWorkflows = justin.select(query)
  if not findingWorkflows:  
    # Nothing to do
    return

  didClient = None

  for workflow in findingWorkflows:
    refindNextTime = int(workflow['refind_next'])
    refindEndTime  = int(workflow['refind_end'])
    refindSeconds  = int(workflow['refind_seconds'])
  
    justin.logLine('Finding files for Workflow %d with MQL %s (%d,%d,%d)' %
            (workflow['workflow_id'], workflow['mql'],
             refindNextTime, refindEndTime, refindSeconds))
  
    mqlSplit = workflow['mql'].split()

    # Workflow for a Rucio dataset: "rucio-dataset SCOPE:NAME"
    if len(mqlSplit) == 2 and mqlSplit[0] == 'rucio-dataset':

      if not didClient:      
       # Only set up Rucio if we see a rucio-dataset
       try:
         didClient = rucio.client.didclient.DIDClient()
       except Exception as e:
         justin.logLine("Connect to Rucio fails with: " + str(e))
         continue

      findRet = False
      datasetSplit = mqlSplit[1].split(':')
      if len(datasetSplit) == 2:
        findRet = findFilesRucio(didClient,
                       workflow['workflow_id'], 
                       datasetSplit[0], 
                       datasetSplit[1],
                       workflow['max_files_per_workflow'])
      if not findRet:
        continue
        
    # Monte Carlo workflow with a count
    elif len(mqlSplit) == 2 and mqlSplit[0] == 'monte-carlo':
      
      try:
        count = int(mqlSplit[1])
      except:
        continue
      
      findRet = addFilesMonteCarlo(workflow['workflow_id'], count,
                                   workflow['max_files_per_workflow'])
      if not findRet:
        continue

    else:    
      findRet = findFilesMetaCat(workflow['workflow_id'], workflow['mql'],
                                 workflow['max_files_per_workflow'])

      if not findRet:
        continue
          
    if refindEndTime == 0:
      # We are not doing refinding and we have done the only finding we do
      # So unset refind_next_time
      refindNextTime = 0
    elif refindNextTime == refindEndTime:
      # We have been doing the final refind soon after the end time
      # So unset refind_next_time
      refindNextTime = 0
    else:
      # Next finding will be soon after refind_seconds from now
      refindNextTime = int(time.time()) + refindSeconds

      # BUT if that would be after refind_end_time, force a final refind,
      # "soon" after the end time
      if refindNextTime > refindEndTime:
        refindNextTime = refindEndTime

    try:
      justin.insertUpdate('UPDATE workflows SET '
                          'refind_next_time=FROM_UNIXTIME(%d) '
                          'WHERE workflow_id=%d AND state="running"' 
                          % (refindNextTime, workflow['workflow_id']))
    except Exception as e:
      justin.logLine("Update workflow %d refind_next_time fails with: %s" 
              % (workflow['workflow_id'],  str(e)))
      continue

    justin.conn.commit()

def findReplicas():
  # Find replicas of files in the finding state

  justin.logLine('Start finding replicas')

  query = (
    'SELECT workflows.workflow_id,stages.stage_id '
    'FROM workflows '
    'LEFT JOIN stages ON workflows.workflow_id=stages.workflow_id '
    'LEFT JOIN files ON workflows.workflow_id=files.workflow_id AND '
    'stages.stage_id=files.stage_id '
    'WHERE workflows.state="running" AND files.state="finding" '
    'GROUP BY workflows.workflow_id,stages.stage_id '
    'ORDER BY stage_priority DESC,RAND() LIMIT 1')

  try:
    stageRow = justin.select(query, justOne = True)
  except Exception as e:
    justin.logLine('Finding workflow/stage to find replicas for fails: %s' % str(e))
    return

  if not stageRow:
    justin.logLine('No files with replicas to be found')
    return

  workflowID = stageRow['workflow_id']
  stageID   = stageRow['stage_id']
  justin.logLine('Looking for replicas in finding state for workflow=%d stage=%d'
          % (workflowID, stageID))

  # Make a list of up to maxFileFindsPerCycle files to work on 
  # but only for Rucio DIDs in scope:name format
  didsList = []

  try:
    fileRows = justin.select(
           'SELECT files.workflow_id,files.stage_id,file_id,file_did '
           'FROM files '
           'LEFT JOIN workflows ON workflows.workflow_id=files.workflow_id '
           'WHERE files.workflow_id=%d '
           'AND files.stage_id=%d '
           'AND files.state="finding" '
           'AND workflows.state="running" '
           'AND file_did LIKE "%%:%%" '
           'ORDER BY file_id LIMIT %d'
           % (workflowID, stageID, maxFileFindsPerCycle))

  except Exception as e:
    justin.logLine('Finding list of files to find fails: ' + str(e))
    return
  
  # Nothing to do
  if not fileRows:
    justin.logLine('No files found in finding state')
    return

  # Check we can talk to Rucio
  try:
    repClient = rucio.client.replicaclient.ReplicaClient()
  except Exception as e:
    justin.logLine("Connect to Rucio fails with: " + str(e))
    return

  # Cache rse_id of every known RSE into a local dictionary
  # unless it is decommissioned
  storagesDict = {}
  allStorages = justin.select('SELECT rse_name,rse_id,needs_pin '
                              'FROM storages WHERE NOT decommissioned')
  
  for storageRow in allStorages:
    storagesDict[storageRow['rse_name']] = { 
                     'rse_id'    : storageRow['rse_id'],
                     'needs_pin' : bool(storageRow['needs_pin']) }

#  # Receives a list of unique (workflowID,stageID) combinations
#  workflowsStages = set()
  filesList = []

  # Make a list of files we want replicas for, in the Rucio API's format
  for fileRow in fileRows:
    (didScope, didName) = fileRow['file_did'].split(':')
    filesList.append({'scope' : didScope, 'name'  : didName })

  # Use the Rucio API to get big lists of replicas for the files
  try:
    # WE FORCE THE RETURNED SCHEME TO BE root FOR NOW
    # domain='all' does not seem to work for now
    wanRepsList = repClient.list_replicas(filesList,
                                          schemes=['root'], 
                                          domain='wan')

    lanRepsList = repClient.list_replicas(filesList,
                                          schemes=['root'], 
                                          domain='lan')

  except Exception as e:
    justin.logLine("Reading RSE replicas info from Rucio fails with: " + str(e))
    return

  # Make a dictionary with DIDs as keys and lists of replicas from Rucio
  # as the values
  repsDict = {}
  for rep in list(wanRepsList) + list(lanRepsList):
    if rep['scope']+':'+rep['name'] not in repsDict:
      repsDict[rep['scope']+':'+rep['name']] = []
      
    for pfn in rep['pfns']:
      pfnDict = rep['pfns'][pfn]
      pfnDict['pfn'] = pfn
      repsDict[rep['scope']+':'+rep['name']].append(pfnDict)

  # Go through the files again, pulling out the replica info obtained 
  # for the file's DID
  for fileRow in fileRows:
    if fileRow['file_did'] not in repsDict:
      # No replicas for this file! All deleted?
      justin.logLine('File (%d,%d,%s) has no replicas in Rucio' %
                (fileRow['workflow_id'], 
                 fileRow['stage_id'],
                 fileRow['file_did']))

      query = ('UPDATE files SET state="notfound" WHERE file_id=%d' %
               fileRow['file_id'])    
      try:
        justin.cur.execute(query)
      except Exception as e:
        justin.logLine('Failed setting file (%d,%d,%s) to notfound: %s' %
                (fileRow['workflow_id'], 
                 fileRow['stage_id'],
                 fileRow['file_did'],
                 str(e)))
        # We give up if this happens, without the commit
        justin.conn.rollback()
        return
        
      continue
      
    pfnsList = repsDict[fileRow['file_did']]    
    rses = {}

    # Save the wan and lan PFNs returned for each RSE with a replica
    # These go into the SAME rses dictionary so separate LAN and WAN
    # replica information is merged into one data dictionary
    for pfnDict in pfnsList:
        if pfnDict['type'] != 'DISK' and pfnDict['rse'] != 'FNAL_DCACHE':
          continue

        if pfnDict['rse'] not in rses:
            rses[pfnDict['rse']] = {}
      
        if pfnDict['domain'] == 'lan':
            rses[pfnDict['rse']]['lan_pfn'] = justin.fixPfn(pfnDict['pfn'])
        else:
            rses[pfnDict['rse']]['wan_pfn'] = justin.fixPfn(pfnDict['pfn'])

    replicasInserted = 0
    # Now go through the replicas for this file, RSE by RSE
    for rse in rses:

        # Skip if RSE not listed (possibly because decommissioned=True)
        if rse not in storagesDict:
          continue
    
        try:
          lanPFN = rses[rse]['lan_pfn'] 
        except:
          lanPFN = ''

        try:
          wanPFN = rses[rse]['wan_pfn'] 
        except:
          wanPFN = ''
          
        # If PFN wan=lan then set lan PFN to ''        
        if wanPFN == lanPFN:
          lanPFN = ''       
  
        if storagesDict[rse]['needs_pin']:
          accessibleUntil = ',accessible_until="%s" ' % justin.unixEpoch
        else:
          accessibleUntil = ''
        
        if workflowID == justin.awtWorkflowID:
          try:
            justin.logLine('Try to update AWT file: %s %s' % (wanPFN, lanPFN))
          
            # For AWT, if the file is put back into finding then we can
            # update the replicas with change PFNs from Rucio
            query = ('UPDATE replicas SET '
                     'wan_pfn="%s",lan_pfn="%s" '
                     'WHERE ' 
                     'workflow_id=%d AND stage_id=%d AND '
                     'rse_id=%d AND file_id=%s'
                     % (wanPFN,
                        lanPFN,
                        workflowID, stageID,
                        storagesDict[rse]['rse_id'],
                        fileRow['file_id']))
                        
            justin.insertUpdate(query)
          except:
            # For AWT, we just carry on
            pass

          try:
            justin.logLine('Try to insert AWT file: %s %s' 
                    % (wanPFN, lanPFN))

            query = ('INSERT INTO replicas SET '
                     'workflow_id=%d,stage_id=%d,'
                     'rse_id=%d,file_id=%d,'
                     'wan_pfn="%s",lan_pfn="%s"' %
                     (workflowID, stageID,
                      storagesDict[rse]['rse_id'],
                      fileRow['file_id'],
                      wanPFN,
                      lanPFN))

            justin.select(query)
          except:
            # For AWT, we just carry on
            pass

          # But we treat whatever happened as everything to be done
          replicasInserted += 1

        else:
          try:
            # workflow_id/stage_id are only recorded with the replicas 
            # to make it faster to make charts etc in the Dashboard
            query = ('INSERT INTO replicas SET '
                     'workflow_id=%d,stage_id=%d,'
                     'rse_id=%d,file_id=%d,'
                     'wan_pfn="%s",lan_pfn="%s"%s' %
                     (workflowID, stageID,
                      storagesDict[rse]['rse_id'],
                      fileRow['file_id'],
                      wanPFN,
                      lanPFN,
                      accessibleUntil))

            justin.select(query)
            replicasInserted += 1

            justin.logEvent(eventTypeID = justin.event_REPLICA_ADDED,
                            workflowID = workflowID,
                            stageID = stageID,
                            fileID = fileRow['file_id'],
                            rseID = storagesDict[rse]['rse_id'])
   
          except Exception as e:
            justin.logLine('Failed inserting replica: ' + str(e))
            # We give up if this happens, without the commit
            justin.conn.rollback()
            return

    if replicasInserted:
      # If we got this far, then we found all the replicas for this 
      # file so we update its state to unallocated (not finding any more)
      try:
        query = ('UPDATE files SET state="unallocated" WHERE file_id=%d' %
                 fileRow['file_id'])

        justin.cur.execute(query)
      except Exception as e:
        justin.logLine('Failed updating file (%d,%d,%s): %s' %
                (fileRow['workflow_id'],
                 fileRow['stage_id'],
                 fileRow['file_did'],
                 str(e)))
        # We give up if this happens, without the commit
        justin.conn.rollback()
        return
    else:
      try:
        query = ('UPDATE files SET state="notfound" WHERE file_id=%d' %
                 fileRow['file_id'])

        justin.cur.execute(query)
      except Exception as e:
        justin.logLine('Failed updating file (%d,%d,%s): %s' %
                (fileRow['workflow_id'], 
                 fileRow['stage_id'],
                 fileRow['file_did'],
                 str(e)))
        # We give up if this happens, without the commit
        justin.conn.rollback()
        return
    
  # All ok, so commit it all to the DB
  justin.conn.commit()

def executeMetaCatCommand(args):

  for i in range(1, 4):  
    ret = os.system(
     'export X509_USER_PROXY=%s ; '
     'export METACAT_AUTH_SERVER_URL=%s ; '
     'export METACAT_SERVER_URL=%s ; '
     '/usr/local/bin/metacat auth login -m x509 dunepro ; '
     '/usr/local/bin/metacat %s' % 
     (justin.jobsProductionProxyFile, 
      justin.metacatAuthServerURL, justin.metacatServerOutputsURL, 
      args))
     
    if ret == 0:
      break

    time.sleep(1) 

  return ret

# MOVING TO justin-fnal-agent
def getRcdsCvmfsPath(rcdsServer, hashString):

  httpRequest = urllib.request.Request('https://%s//pubapi/exists?cid=dune/%s'
                                       % (rcdsServer, hashString),
                                       data = hashString,
                                       headers = { 'User-Agent':
                                                   'justin-finder' },
                                       method = 'GET')

  sslContext = ssl.SSLContext()
  sslContext.load_cert_chain('/tmp/x509up_u%d' % os.getuid())
  sslContext.load_verify_locations(capath = '/etc/grid-security/certificates')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
    responseString = response.read().decode('utf-8')
  except:
    return None

  if responseString.startswith('PRESENT:'):
    return responseString[8:]
    
  return None

# MOVING TO justin-fnal-agent
def uploadRcdsTarfile(rcdsServer, hashString, tarFileContents):

  httpRequest = urllib.request.Request('https://%s//pubapi/publish?cid=dune/%s'
                                       % (rcdsServer, hashString),
                                       data = tarFileContents,
                                       headers = { 'User-Agent': 
                                                   'justin-finder' },
                                       method = 'POST')

  sslContext = ssl.SSLContext()
  sslContext.load_cert_chain('/tmp/x509up_u%d' % os.getuid())
  sslContext.load_verify_locations(capath = '/etc/grid-security/certificates')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except Exception as e:
    justin.logLine('RCDS upload fails: ' + str(e))
    return False

  try:
    responseString = response.read().decode('utf-8')
  except Exception as e:
    justin.logLine('Failed getting RCDS upload response string: ' + str(e))
    return False

  justin.logLine('RCDS upload response: ' + responseString)

  if response.status != 200:
    justin.logLine('RCDS upload fails with HTTP code %d' % response.status)
    return False
  
  if responseString[:2] != 'OK':
    justin.logLine('RCDS upload does not respond with OK')
    return False
    
  return True

# MOVING TO justin-fnal-agent
def checkGitRepo(workflowID = None, stageID = None, 
                 rcdsServer = 'rcds01.fnal.gov', rcdsHash = None,
                 gitRepo = None, gitCommit = None):
  # Check, and if necessary, upload Git repo to RCDS

  (nameRepo, tag) = gitRepo.split(':',1)
  (name, repo)    = nameRepo.split('/',1)

  if rcdsHash:
    # check if RCDS still recognises the hash we stored previously
    rcdsPath = getRcdsCvmfsPath(rcdsServer, rcdsHash)
    if rcdsPath:
      return { 'rcds_path'  : rcdsPath, 
               'rcds_hash'  : rcdsHash  }
    
  # We need to upload the tar file to RCDS, so first create it using 
  # the Git commit not the tag!
  try:
    tmpDir = tempfile.TemporaryDirectory()
    waitStatus = os.system('export HOME=/home/dunejustin && cd %s && '
                      '/usr/bin/git clone https://github.com/%s/%s.git && '
                      'cd %s && '
                      '/usr/bin/git checkout %s && '
                      'rm -Rf .git .github && '
                      '/usr/bin/tar cvf ../tmp.tar --sort=name --mtime=@0 .'
                      % (tmpDir.name, name, repo, repo, gitCommit)
                          )

    ret = os.waitstatus_to_exitcode(waitStatus)
    
    if ret:
      raise RuntimeError('git clone / tar returns error %d' % ret)
  except Exception as e:
    justin.logLine('Failed to do git clone / tar creation: ' + str(e))
    return None

  try:
# SHOULD CHECK THIS IS NOT ABSURDLY LARGE BEFORE WE SLURP IT INTO MEMORY!!!
    tarFileContents = open(tmpDir.name + '/tmp.tar', 'rb').read()
    rcdsHash        = hashlib.sha1(tarFileContents).hexdigest()
  except Exception as e:
    justin.logLine('Failed to read and hash tar file tmp.tar: ' + str(e))
    return None

  if not uploadRcdsTarfile(rcdsServer, rcdsHash, tarFileContents):
    justin.logLine('Failed to upload tar file to RCDS for w%ds%d' 
            % (workflowID, stageID))
    return None
    
  for i in range(0,12):
    time.sleep(5)
    
    rcdsPath = getRcdsCvmfsPath(rcdsServer, rcdsHash)

    if rcdsPath:
      return { 'rcds_path'  : rcdsPath, 
               'rcds_hash'  : rcdsHash }

  # Failed to get path despite waiting and after many tries
  return None

def findSubmittedWorkflows():
  # Find workflows waiting in the submitted state and set them to running

  justin.logLine('Start of finding submitted workflows')

  try:
    workflows = justin.select('SELECT workflows.workflow_id,scope_name,'
                    'condor_groups.condor_group_id,principal_name '
                    'FROM workflows '
                    'LEFT JOIN scopes '
                    'ON workflows.scope_id=scopes.scope_id '
                    'LEFT JOIN condor_groups '
                    'ON condor_groups.condor_group_id=scopes.condor_group_id '
                    'LEFT JOIN users ON users.user_id=workflows.user_id '
                    'LEFT JOIN principal_names '
                    'ON principal_names.pn_id=users.main_pn_id '
                    'WHERE workflows.state="submitted" AND '
                    '(SELECT COUNT(*) FROM stages_git_repos '
                    'WHERE stages_git_repos.workflow_id=workflows.workflow_id '
                    'AND stages_git_repos.rcds_path="")=0 '
                    'ORDER BY workflows.workflow_id')
  except:
    justin.logLine('Failed to get list of submitted workflows')
    return

  justin.logLine('Submitted workflows %d' % len(workflows))

  for workflow in workflows:
    justin.logLine('workflow: ' + str(workflow))
    try:
      processSubmittedWorkflow(workflow)
    except Exception as e:
      justin.conn.rollback()
      justin.logLine('Failed to process submitted workflow %d : %s'
              % (workflow['workflow_id'], str(e)))
    else:      
      justin.logLine('Processed submitted workflow %d' % workflow['workflow_id'])
      justin.conn.commit()

def processSubmittedWorkflow(workflow):
  # Process a single workflow, raising exceptions on errors

  workflowID = workflow['workflow_id']
  justin.logLine('Processing submitted w%d' % workflowID)

  try:
    condorGroupID = int(workflow['condor_group_id'])
  except:
    condorGroupID = 0

#  stages = justin.select('SELECT stage_id,processors,jobscript_git,'
#                         'jobscript_image,wall_seconds,rss_bytes '
#                         'FROM stages WHERE workflow_id=%d'
#                         % workflowID)
#
#  for stage in stages:
#    stageID = stage['stage_id']
#    justin.logLine('Processing submitted w%ds%d' % (workflowID, stageID))
#
#    stagesOutputStorages = justin.select(
#        'SELECT rse_name FROM stages_output_storages '
#        'LEFT JOIN storages '
#        'ON storages.rse_id=stages_output_storages.rse_id '
#        'WHERE stages_output_storages.workflow_id=%d '
#        'AND stages_output_storages.stage_id=%s '
#        % (workflowID, stageID))
#          
#    rseNames = []
#    for stageOutputStorage in stagesOutputStorages:
#      rseNames.append(stageOutputStorage['rse_name'])
#
#    stagesOutputs = justin.select(
#      'SELECT destination,lifetime_seconds,pattern_id,file_pattern '
#      'FROM stages_outputs '
#      'WHERE workflow_id=%d AND stage_id=%d' % (workflowID, stageID))
#
#    for stageOutput in stagesOutputs:
#      if not stageOutput['destination'].startswith('https://'):
#        justin.logLine('Processing submitted w%ds%dp%d' 
#                % (workflowID, stageID, stageOutput['pattern_id']))
#        # A Rucio dataset
#
#        # Metadata for the main dataset in MetaCat
#        metadataDict = { "dune.workflow" : 
#            { "workflow_id"     : workflowID,
#              "stage_id"        : stageID,
#              "pattern_id"      : stageOutput['pattern_id'],
#              "file_pattern"    : stageOutput['file_pattern'],
#              "user"            : workflow['principal_name'],
#              "processors"      : stage['processors'],
#              "rss_bytes"       : stage['rss_bytes'],
#              "wall_seconds"    : stage['wall_seconds'],
#              "jobscript_image" : stage['jobscript_image']
#            }
#                       }
# ADD RCDS/Git repos in here too with tags and commit hashes
#                         
#        if stage['jobscript_git']:
#          metadataDict['dune.workflow']['jobscript_git'] = stage['jobscript_git']
#
#        createOutputDatasets(workflowID = workflowID,
#                             stageID = stageID,
#                             patternID = stageOutput['pattern_id'],
#                             metadataDict = metadataDict,
#                             scopeName = workflow['scope_name'],
#                             destination = stageOutput['destination'],
#                             lifetimeSeconds = stageOutput['lifetime_seconds'],
#                             rseNames = rseNames)

  # Successful so mark workflow as running
  # Define refind_next_time as now to force at least one finding cycle
  # even if not refinding repeatedly
  query = ('UPDATE workflows SET '
           'started=NOW(),'
           'state="running",'
           'state_message="Moved to running state by justIN",'
           'refind_next_time=NOW(),'
           'condor_group_id=%d '
           'WHERE workflow_id=%d' % (condorGroupID, workflowID))

  justin.insertUpdate(query)

# THIS FUNCTION IS NO LONGER USED AND CAN BE REMOVED ONCE FINISHED PUTTING
# THIS FUNCIONALITY IN THE WRAPPER JOBS
def createOutputDatasets(workflowID, stageID, patternID, metadataDict,
                         scopeName, destination, lifetimeSeconds, rseNames):
  # Create missing main pattern datasets (per-RSEs done in wrapper job)
  # All exceptions will be caught by the caller
  
  justin.logLine('Try to create DID and Rule Rucio clients')
  didClient  = rucio.client.didclient.DIDClient()
  ruleClient = rucio.client.ruleclient.RuleClient()

  # Try to create the pattern's dataset in MetaCat
  justin.logLine('Try to add MetaCat dataset %s:%s' % (scopeName, destination))

  # Temporary file to pass to metacat command 
  (fd, metadataFile) = tempfile.mkstemp(text = True)
  fp = os.fdopen(fd, 'w')
  json.dump(metadataDict, fp)
  fp.close()

  try:
    ret = executeMetaCatCommand("dataset create --metadata %s %s:%s"
                                % (metadataFile, scopeName, destination))
  # Carry on regardless if ret != 0 since we can't tell from the metacat 
  # return code if the dataset just already exists or there was a 
  # genuine problem ...
  except Exception as e:
    raise RuntimeException('Failed adding dataset %s:%s to Metacat: %s' 
                             % (scopeName, destination, str(e)))
  finally:
    os.remove(metadataFile)
  
  # Try to create the pattern's dataset in Rucio if not already there
  try:
    justin.logLine('Try to add Rucio dataset %s:%s' % (scopeName, destination))
    ret = didClient.add_dataset(scope = scopeName, name = destination)
  except rucio.common.exception.DataIdentifierAlreadyExists:
    pass
  else:
    if not ret:
      raise RuntimeException('Failed adding dataset %s:%s' 
                             % (scopeName, destination))

  # Create the per-RSE datasets and rules
  for rseName in rseNames:
    # Try to create the per-RSE dataset in MetaCat
    datasetForRSE = 'w%ds%dp%d-%s' % (workflowID, stageID, patternID, rseName)
    justin.logLine('Try to add MetaCat dataset %s:%s' % (scopeName, datasetForRSE))
    ret = executeMetaCatCommand('dataset create %s:%s' 
                                % (scopeName, datasetForRSE))
    # Carry on regardless if ret != 0 since we can't tell from the metacat 
    # return code if the dataset just already exists or there was a 
    # genuine problem ...
 
    # Try to create the pattern's dataset in Rucio if not already there
    try:
      justin.logLine('Try to add Rucio dataset %s:%s' % (scopeName, datasetForRSE))
      ret = didClient.add_dataset(scope = scopeName, name = datasetForRSE)
    except rucio.common.exception.DataIdentifierAlreadyExists:
      pass
    else:
      if not ret:
        raise RuntimeException('Failed adding dataset %s:%s' 
                               % (scopeName, datasetForRSE))

    # Create a rule for the new per-RSE dataset
    justin.logLine('Try to add rule for %s:%s' % (scopeName, datasetForRSE))
    try:
      ret = ruleClient.add_replication_rule(
                     dids = [ {'scope' : scopeName,
                               'name'  : datasetForRSE} ], 
                     copies = 1, 
                     rse_expression = rseName,
                     lifetime = lifetimeSeconds)
    except rucio.common.exception.DuplicateRule:
      # Cannot create rule now because already exists - dev/int instance?
      justin.logLine('Failed to create rule for %s as a duplicate - skipping' 
              % datasetForRSE)
    except rucio.common.exception.RSEOverQuota:
      # Cannot create rule now because RSE is currently over quota
      # WHY ON EARTH DOES RUCIO WORK THIS WAY!!!
      # ADD RULE LATER SOMEHOW??? 
      justin.logLine('Failed to create rule for %s as over quota - skipping' 
              % datasetForRSE)
    except rucio.common.exception.InsufficientAccountLimit:
      # Cannot create rule now because RSE is currently over account limit
      # WHY ON EARTH DOES RUCIO WORK THIS WAY!!!
      # ADD RULE LATER SOMEHOW??? 
      logLine('Failed to create rule for %s as over account limit - skipping' 
              % datasetForRSE)

    except rucio.common.exception.RSEWriteBlocked:
      # Cannot create rule now because RSE is currently not writeable
      # WHY ON EARTH DOES RUCIO WORK THIS WAY!!!
      # ADD RULE LATER SOMEHOW??? 
      justin.logLine('Failed to create rule for %s as write disabled - skipping' 
              % datasetForRSE)
    else:
      if not ret:
        raise RuntimeException("Failed adding rule for %s:%s" 
                               % (scopeName, datasetForRSE))

# SHOULD ALSO CHECK FOR EXISTING RULES WE CREATED FOR THIS DATASET THAT
# NEED THEIR LIFETIME EXTENDING? THIS IS NEEDED FOR LONG WORKFLOWS?


# MOVING TO justin-fnal-agent
def createWorkflowGitRepos(workflowID):
  # Create all the git repos for this workflow

  try:
    query = ('SELECT stages_git_repos.stage_id,'
             'git_repo,git_commit FROM workflows '
             'LEFT JOIN stages_git_repos '
             'ON stages_git_repos.workflow_id=workflows.workflow_id '
             'WHERE workflows.state="submitted" AND git_repo IS NOT NULL '
             'AND workflows.workflow_id=%d' % workflowID)

    gitRepoRows = justin.select(query) 
  except Exception as e:
    justin.logLine('Exception finding git_repos: %s' % str(e))
    return

  for gitRepoRow in gitRepoRows:
    retDict = checkGitRepo(workflowID = workflowID,
                           stageID    = gitRepoRow['stage_id'],
                           gitRepo    = gitRepoRow['git_repo'],
                           gitCommit  = gitRepoRow['git_commit'])
    
    if not retDict:
      justin.logLine('Failed to check git repo / RCDS - exit for now')
      return
    
    justin.insertUpdate('UPDATE stages_git_repos SET '
                        'rcds_hash="%s",rcds_path="%s" '
                        'WHERE workflow_id=%d AND stage_id=%d '
                        'AND git_repo="%s"' %
                            (retDict['rcds_hash'],
                             retDict['rcds_path'],
                             workflowID, gitRepoRow['stage_id'], 
                             gitRepoRow['git_repo']))

def findWorkflowsToFinish():
  # Find workflows with all files in terminal states and set to finishing

  justin.logLine('Start of finding workflows to be set to finishing')

  try:
    # workflows have to be still in the running state and to have
    # finished their finding or refinding 
    query = ('SELECT workflow_id FROM workflows '
             'WHERE state="running" AND refind_next_time="%s" '
             'ORDER BY workflow_id' % justin.unixEpoch)

    findingWorkflows = justin.select(query)
  except:
    justin.logLine('Failed to get list of running workflows')
    return
    
  if not findingWorkflows:
    justin.logLine('No running workflows to check')
    return

  for workflow in findingWorkflows:

    try:
      query = ('SELECT COUNT(*) AS count FROM files '
               'WHERE workflow_id=%d AND '
               'state <> "processed" AND '
               'state <> "notfound" AND '
               'state <> "failed" AND ' 
               'state <> "recorded" AND '
               'state <> "output"' %
               int(workflow['workflow_id']))
             
      justin.cur.execute(query)
      count = int(justin.cur.fetchone()['count'])
    except Exception as e:
      justin.logLine('Failed to count non-terminal files for Workflow ID %d: %s' % 
              (workflow['workflow_id'], str(e)))
      continue

    if count > 0:
      # Some files have not reached terminal states
      justin.logLine('Still non-terminal files for Workflow ID %d (%d)' % 
              (workflow['workflow_id'], count))
      continue
    
    try:
      query = ('UPDATE workflows SET state="finishing",'
         'state_message="Finishing as all input files are in terminal states" '
               'WHERE workflow_id=%d' % int(workflow['workflow_id']))
    
      justin.cur.execute(query)
    except:
      justin.logLine('Failed to update state to finishing for Workflow ID %d' % 
              workflow['workflow_id'])
      continue

    justin.logLine('Workflow ID %d set to finishing' % workflow['workflow_id'])
  justin.conn.commit()

# NEED TO ADD finishWorkflows() 
# This will use rucio API to finish 
#
# gen=didClient.list_dids('fardet-vd', 
#  [{'name':'*-w5241s*p*'},{'name':'w5241s*p*'}], 'dataset', True, False)
# returns a generator giving datasets matching the name pattern:
# {'scope': 'fardet-vd', 'name': 'fardet-vd-reco_5241-w5241s1p1', 
# 'did_type': 'DATASET', 'bytes': None, 'length': None}
#
# didClient.get_did('fardet-vd', 'fardet-vd-reco_5241-w5241s1p1')
# returns 
# {'scope': 'fardet-vd', 'name': 'fardet-vd-reco_5241-w5241s1p1', 
# 'type': 'DATASET', 'account': 'dunepro', 'open': True, 'monotonic': False,
# 'expired_at': None, 'length': None, 'bytes': None}
#
# This can be used to find open datasets from this workflow. They can closed
# with didClient.close('fardet-vd', 'fardet-vd-reco_5241-w5241s1p1')
# When a cycle finds no open datasets (ie reads it back from Rucio to make
# sure it really worked) then the workflow can be set to finished.
def finishWorkflows():
  # Find workflows in finishing and set to finished when all their datasets
  # are closed

  justin.logLine('Start of finding workflows to finish')

  try:
    # workflows have to be still in the finishing state 
    query = ('SELECT scope_name,workflow_id FROM workflows '
             'LEFT JOIN scopes ON scopes.scope_id=workflows.scope_id '
             'WHERE workflows.state="finishing" '
             'ORDER BY workflow_id')

    finishingWorkflows = justin.select(query)
  except Exception as e:
    justin.logLine('Failed to get list of finishing workflows: ' + str(e))
    return
    
  if not finishingWorkflows:
    justin.logLine('No finishing workflows to check')
    return

  try:
    didClient = rucio.client.didclient.DIDClient()
  except Exception as e:
    justin.logLine("Connect to Rucio fails with: " + str(e))
    return

  for workflowRow in finishingWorkflows:
    # We set this to True if we see any open datasets and close fails
    # Maybe 
    anyOpenDatasets = False
  
    try:
      gen = didClient.list_dids(workflowRow['scope_name'], 
                  [{'name':'*-w%ds*p*' % int(workflowRow['workflow_id'])},
                   {'name':'w%ds*p*'  % int(workflowRow['workflow_id'])}], 
                  'dataset', True, False)
    except Exception as e:
      justin.logLine("list_dids() fails, skipping workflow %d: %s" 
                     % (int(workflowRow['workflow_id']), str(e)))
      continue

    for dataset in gen:
      try:
        didInfo = didClient.get_did(dataset['scope'], dataset['name'])
      except:
        # Assume the worst - but we carry on and look for others to close
        anyOpenDatasets = True
      else:
        # Ignore datasets created by someone else with our dataset format!!!
        if didInfo['account'] != justin.justinJobsUser:
          justin.logLine('Skipping matching dataset %s:%s created by %s' %
                     (dataset['scope'], dataset['name'], didInfo['account']))
          continue
      
        if didInfo['open']:
          try:
            didClient.close(dataset['scope'], dataset['name'])
            # SHOULD WE DOUBLE CHECK THIS SOMEHOW??? ANOTHER PASS?
          except Exception as e:
            # Set to True if fail to close
            anyOpenDatasets = True
            justin.logLine('Closing dataset %s:%s fails with %' %
                           (dataset['scope'], dataset['name'], str(e)))
          else:
            justin.logLine('Closed dataset %s:%s' %
                           (dataset['scope'], dataset['name']))
    
    # If all ok, we can mark the workflow as finished        
    if not anyOpenDatasets:
      try:
        query = ('UPDATE workflows SET state="finished",finished=NOW(),'
                 'state_message="Set to finished" '
                 'WHERE workflow_id=%d and state="finishing"' 
                 % int(workflowRow['workflow_id']))
    
        justin.cur.execute(query)
      except Exception as e:
        justin.logLine('Failed to update state to finished '
                       'for Workflow ID %d: %s'
                       % (workflowRow['workflow_id'], str(e)))
      else:
        justin.logLine('Workflow ID %d set to finished' 
                       % workflowRow['workflow_id'])
        # Commit as we go along
        justin.conn.commit()

def findProblemWorkflows():
  # Find problem workflows using counts of job states and pause them

  justin.logLine('Start of finding problem workflows')

  try:
    query = ('SELECT jobs.workflow_id,job_state,count(*) AS count FROM jobs '
             'LEFT JOIN workflows ON jobs.workflow_id=workflows.workflow_id '
             'WHERE jobs.workflow_id<>1 AND workflows.state="running" '
             'GROUP BY job_state,jobs.workflow_id')

    runningWorkflows = justin.select(query)
  except:
    justin.logLine('Failed to get list of running workflows')
    return
    
  if not runningWorkflows:
    justin.logLine('No running workflows to check for problems')
    return

  # Assemble job state counts in a dictionary
  d = {}
  for jobStateCount in runningWorkflows:
  
    workflowID = int(jobStateCount['workflow_id'])
    jobState   = jobStateCount['job_state']
    count      = int(jobStateCount['count'])

    if workflowID not in d:
      d[workflowID] = {} 

      for state in justin.jobStatesAll:
        d[workflowID][state] = 0 
    
    d[workflowID][jobState] = count
  
  # Go through the workflows, finding problem ones
  for workflowID in d:

    pauseEvent   = None
    pauseMessage = ''

    # Too many jobscript_error jobs
    if (d[workflowID]['jobscript_error'] > 10 and 
        d[workflowID]['jobscript_error'] > 
            d[workflowID]['started'] +
            d[workflowID]['processing'] +
            d[workflowID]['outputting'] +
            d[workflowID]['finished']
       ):
      pauseEvent   = justin.event_WORKFLOW_PAUSED_JOBSCRIPT_ERROR
      pauseMessage = 'Paused after too many jobs with jobscript errors'

    # Too many none_processed jobs      
    elif (d[workflowID]['none_processed'] > 10 and 
        d[workflowID]['none_processed'] > 
            d[workflowID]['started'] +
            d[workflowID]['processing'] +
            d[workflowID]['outputting'] +
            d[workflowID]['finished']
       ):
      pauseEvent   = justin.event_WORKFLOW_PAUSED_NONE_PROCESSED
      pauseMessage = \
        'Paused after too many jobs with no provided input files processed'
    
    # Too many not used jobs      
    elif (d[workflowID]['notused'] > 10 and 
        d[workflowID]['notused'] > 
            d[workflowID]['started'] +
            d[workflowID]['processing'] +
            d[workflowID]['outputting'] +
            d[workflowID]['finished']
       ):
      pauseEvent   = justin.event_WORKFLOW_PAUSED_NOTUSED
      pauseMessage = 'Paused after too many jobs ending not used'
    
    if pauseEvent:
      try:
        query = ('UPDATE workflows SET state="paused",state_message="%s" '
                 'WHERE workflow_id=%d' % (pauseMessage,workflowID))
    
        justin.insertUpdate(query)
        
        justin.logEvent(eventTypeID = pauseEvent, workflowID = workflowID)
        
      except:
        justin.logLine('Failed to update state to paused for Workflow ID %d' % 
                workflowID)
        continue
      else:
        justin.logLine('Workflow ID %d set to paused' % workflowID)

  justin.conn.commit()

def resetFilesForJob(justinJobID,
                     jobsubID,
                     workflowID,
                     stageID,
                     siteID,
                     entryID):   

    # Then find the file(s) allocated to this stalled or aborted job 
    # (IN THE FUTURE WE SHOULD DO SOMETHING CLEVER WITH Rucio/MetaCat 
    #  CHECKS FOR FILE IN THE outputting STATE)
    try:
      query = ('SELECT file_id,allocations,max_allocations,file_did,state '
               'FROM files '
               'WHERE (state="allocated" OR state="outputting") '
               'AND justin_job_id=%d '
               'ORDER BY file_id' % justinJobID)

      justin.cur.execute(query)

      fileRows = justin.cur.fetchall()
    except:
      justin.logLine('Failed to get list of files of the stalled or aborted job')
      # We should give up at this point rather than commit a mess?
      sys.exit(0)
    
    for fileRow in fileRows: 
    
      if fileRow['allocations'] >= fileRow['max_allocations']:
        # Reached max allocations allowed for this file
        # Mark file as failed in case it is the file causing the problem
        # (eg pathological file causes memory to balloon leading to the site
        # killing the job, and to us the job just appears stalled.)

        try:
          # Only update file state if still allocated to THIS bad job.
          # This safeguard should not be needed for maxAllocations files.
          # Alternatively, we could have done a SELECT ... FOR UPDATE
          # to use a lock.
          query = ('UPDATE files SET state="failed" '
                   'WHERE file_id=%d AND justin_job_id=%d' 
                   % (fileRow['file_id'], justinJobID))

          affectedRows = justin.cur.execute(query)
        except:
          justin.logLine('Failed to set file %s to failed in '
                  'workflow %d, stage %d after job %s' 
                  % (fileRow['file_did'], workflowID, 
                     stageID, jobsubID))
          # Again, just stop for now on errors
          sys.exit(0)
            
        if affectedRows:
          justin.logEvent(eventTypeID = justin.event_FILE_SET_TO_FAILED,
                          workflowID = workflowID,
                          stageID = stageID,
                          fileID = fileRow['file_id'],
                          justinJobID = justinJobID,
                          siteID = siteID,
                          entryID = entryID)
      else:
      
        try:
          # Only update file state if still allocated to THIS bad job.
          # This safeguard is to avoid a race condition between getting the 
          # list of bad jobs and then resetting their files.
          # Alternatively, we could have done a SELECT ... FOR UPDATE
          # to use a lock but this would slow down the allocation process.
          query = ('UPDATE files SET state="unallocated" '
                   'WHERE file_id=%d AND justin_job_id=%d' 
                   % (fileRow['file_id'], justinJobID))

          affectedRows = justin.cur.execute(query)
        except:
          justin.logLine('Failed to set file %s to unallocated in '
                  'workflow %d, stage %d after job %s' 
                  % (fileRow['file_did'], workflowID, 
                     stageID, jobsubID))
          # Again, just stop for now on errors
          sys.exit(0)

        if affectedRows:
          justin.logEvent(eventTypeID = 
                          justin.event_FILE_OUTPUTTING_RESET 
                          if fileRow['state'] == 'outputting'
                          else justin.event_FILE_ALLOCATED_RESET,
                          workflowID = workflowID,
                          stageID = stageID,
                          fileID = fileRow['file_id'],
                          justinJobID = justinJobID,
                          siteID = siteID,
                          entryID = entryID)

          justin.logLine('Reset file %s in %d,%d to unallocated for job %s' % 
                  (fileRow['file_did'], workflowID,
                  stageID, jobsubID))

def findStalledAbortedJobs():
  # Find jobs which have 
  # (1) not sent a heartbeat in jobStalledSeconds, or
  # (2) which have reported they have aborted or that outputting failed
  #     and still have allocated_files
  # and then reset their file allocations
  #
  # We handle bad waiting jobs in findStalledCondorJobs()

  justin.logLine('Start of finding stalled and aborted jobs')

  try:
    query = ('SELECT workflow_id,stage_id,justin_job_id,jobsub_id,site_id,'
             'entry_id,job_state '
             'FROM jobs '
             'WHERE ((job_state="started" OR '
             'job_state="processing" OR '
             'job_state="outputting") AND '
             '(heartbeat_time < DATE_SUB(NOW(),INTERVAL %d SECOND))) OR '
             '((job_state="aborted" OR '
             '  job_state="jobscript_error" OR '
             '  job_state="outputting_failed") AND allocated_files) '
             'ORDER BY justin_job_id' % justin.jobStallSeconds)

    justin.cur.execute(query)

    jobRows = justin.cur.fetchall()
  except Exception as e:
    justin.logLine('Failed to get list of stalled or aborted jobs: ' + str(e))
    return
    
  if not jobRows:
    justin.logLine('No stalled or aborted jobs to process')
    return

  for jobRow in jobRows:

    if jobRow['job_state'] not in ['aborted', 'jobscript_error', 
                                   'outputting_failed']:
      # If not here due to an abort, we mark the job as stalled
      try:
        query = ('UPDATE jobs SET job_state="stalled",finished_time=NOW(),'
                 'allocated_files=0 '
                 'WHERE justin_job_id=%d' % jobRow['justin_job_id'])
    
        justin.cur.execute(query)
      except Exception as e: 
        justin.logLine('Failed to set job %s to stalled: %s' 
              % (jobRow['jobsub_id'], str(e)))
        continue

      justin.logLine('Job %s set to stalled from %s' % (jobRow['jobsub_id'], 
                                                 jobRow['job_state']))
    
      justin.logEvent(eventTypeID = justin.event_JOB_STALLED_HEARTBEAT,
                      workflowID = jobRow['workflow_id'],
                      stageID = jobRow['stage_id'],
                      justinJobID = jobRow['justin_job_id'],
                      siteID = jobRow['site_id'],
                      entryID = jobRow['entry_id'])
    else:
      # An aborted, error, failed state, so just reset allocated_files
      try:
        query = ('UPDATE jobs SET allocated_files=0 '
                 'WHERE justin_job_id=%d' % jobRow['justin_job_id'])
    
        justin.cur.execute(query)
      except Exception as e: 
        justin.logLine('Failed to set job %s to allocated_files=0: %s' 
              % (jobRow['jobsub_id'], str(e)))
        continue

    # However we got here, reset the job's files
    resetFilesForJob(justinJobID = jobRow['justin_job_id'],
                     jobsubID    = jobRow['jobsub_id'],
                     workflowID  = jobRow['workflow_id'],
                     stageID     = jobRow['stage_id'],
                     siteID      = jobRow['site_id'],
                     entryID     = jobRow['entry_id'])

  # Everything went ok so commit
  justin.conn.commit()

def oidcRefreshRequest(refreshToken):
  # Send an OIDC workflow to CILogon to refresh tokens for a user

  postData = urllib.parse.urlencode(
       { 'grant_type'    : 'refresh_token',
         'client_id'     : justin.cilogonClientID,
         'client_secret' : justin.cilogonSecret,
         'refresh_token' : refreshToken
#         ,
#         'scope'         : 'scope=openid profile org.cilogon.userinfo '
#                           'wlcg.capabilityset:/duneana wlcg.groups:/dune '
#                           'wlcg.groups:/dune/production'
       })

  httpRequest = urllib.request.Request('https://cilogon.org/oauth2/token',
                                       data = postData.encode(),
                                       method = 'POST')

  sslContext = ssl.SSLContext()
  sslContext.load_verify_locations(capath = '/etc/grid-security/certificates')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except Exception as e:
    justin.logLine('OIDC tokens HTTP refresh workflow fails: ' + str(e))
    return None
  else:
    if response.status != 200:
      justin.logLine('OIDC tokens HTTP response not 200, code=%d' % response.status)
      return None

  try:
    responseData = response.read().decode('utf-8')
    responseDict = json.loads(responseData)
  except Exception as e:
    justin.logLine('OIDC tokens refresh workflow failed loading json: '+str(e))
    return None

  justin.logLine('responseDict=' + str(responseDict))

  try:
    tokensDict = {} 
    tokensDict['access_token'] = responseDict['access_token']
    tokensDict['access_dict']  = json.loads(base64.urlsafe_b64decode(
                     responseDict['access_token'].split('.')[1] + '=='))

    tokensDict['id_token'] = responseDict['id_token']
    tokensDict['id_dict']  = json.loads(base64.urlsafe_b64decode(
                     responseDict['id_token'].split('.')[1] + '=='))

    tokensDict['refresh_token'] = responseDict['refresh_token']
  except Exception as e:
    justin.logLine('OIDC tokens parsing fails: '+str(e))
    return None
  else:
    return tokensDict

def findTokensToRefresh():

  justin.logLine('Start of finding tokens to refresh')

  try:
    query = ('SELECT users.user_id,principal_name,refresh_token '
             'FROM users '
             'LEFT JOIN principal_names '
             'ON principal_names.pn_id=users.main_pn_id '
             'WHERE '
             'DATE_ADD(NOW(), INTERVAL 60 MINUTE) > access_token_expires AND '
             'access_token_expires > "%s" AND '
             '(SELECT expires_time FROM sessions '
             ' WHERE sessions.user_id=users.user_id '
             ' ORDER BY expires_time DESC LIMIT 1) > NOW() AND '
             'refresh_token <> ""'
             % (justin.unixEpoch))
             
    rows = justin.select(query)

  except Exception as e:
    justin.logLine('Failed to find tokens: ' + str(e))
    return

  for row in rows:
    justin.logLine(str(row))
    tokensDict = oidcRefreshRequest(row['refresh_token'])
    justin.logLine(str(tokensDict))
    
    try:
      accessToken        = tokensDict['access_token']
      accessTokenCreated = tokensDict['access_dict']['iat']
      accessTokenExpires = tokensDict['access_dict']['exp']
      refreshToken       = tokensDict['refresh_token']
    except:
      accessToken        = ''
      accessTokenCreated = 0
      accessTokenExpires = 0
      refreshToken       = ''

    try:
      query = ('UPDATE users SET '
               'access_token="%s",'
               'access_token_created=FROM_UNIXTIME(%d),'
               'access_token_expires=FROM_UNIXTIME(%d),'
               'refresh_token="%s" '
               'WHERE user_id=%d' 
               % (accessToken,
                  accessTokenCreated,
                  accessTokenExpires,
                  refreshToken,
                  row['user_id']))
                  
      justin.insertUpdate(query)
    
    except Exception as e:
      justin.logLine('Failed storing refreshed tokens for %s: %s' 
              % (row['principal_name'], str(e)))
      
    else:
      justin.logLine('Tokens refreshed for %s' % row['principal_name'])

  # All ok, so commit it all to the DB
  justin.conn.commit()

# Remove stale values from caches
def manageCaches():

  justin.logLine('Start of manage caches')

  # Remove stale rank cache entries
  try:
    justin.insertUpdate('DELETE FROM sites_ranks_cache '
                        'WHERE cache_time < DATE_SUB(NOW(),INTERVAL %d SECOND)'
                        % justin.sitesRankCacheStale)
  except Exception as e:
    justin.logLine('Delete stale sites rank cache rows fails: ' + str(e))
            
  # All ok, so commit it all to the DB
  justin.conn.commit()

def findStalledCondorJobs():

  justin.logLine('Start of finding stalled HTCondor jobs')

  nowTime = int(time.time())

  # Get a list of waiting/running jobs
  query = ('SELECT jobsub_id,job_state,justin_job_id,workflow_id,stage_id,'
           'site_id,entry_id FROM jobs WHERE job_state IN '
           '("submitted","started","processing","outputting")')

  dbJobRows = justin.select(query)

  # Make a set of unique HTCondor schedd names from the database
  schedds = set()
  for dbJobRow in dbJobRows:
    schedds.add( dbJobRow['jobsub_id'].split('@',1)[1] )

  # Gather details of HTCondor jobs from our set of schedds
  condorJobs = {}  

  for scheddHostname in schedds:
    try: 
      outcome = subprocess.run(
               ['/usr/bin/condor_q',
                '-name',
                scheddHostname,
                '-long:json',
                '-attributes',
                'clusterid,procid,jobstatus,enteredcurrentstatus'
               ],
               encoding = 'utf-8',
               stderr   = subprocess.STDOUT,
               stdout   = subprocess.PIPE,
               timeout  = 120
                            ) 
    except Exception as e:
      justin.logLine("condor_q fails with error: " + str(e))
      return

    justin.logLine('condor_q exit code: ' + str(outcome.returncode))
    
    if outcome.returncode != 0:
      justin.logLine("condor_q fails with return code %d" % outcome.returncode)
      return

    try:
      jsonJobs = json.loads(outcome.stdout)
    except:
      justin.logLine('Failed to load JSON output by condor_q')
      return

    for jsonJob in jsonJobs:  
      if 'jobstatus' not in jsonJob or \
         'enteredcurrentstatus' not in jsonJob or \
         'clusterid' not in jsonJob or \
         'procid' not in jsonJob:
        continue
      
      condorJobs['%d.%d@%s' % 
                 (jsonJob['clusterid'], 
                  jsonJob['procid'],
                  scheddHostname)] = \
           (jsonJob['jobstatus'], jsonJob['enteredcurrentstatus'])

  if not condorJobs:
    # If we get nothing at all then something must be going wrong
    justin.logLine('Failed to find any job info from HTCondor')
    return

  # Go back to the list of jobs from the database and check each one
  # against its HTCondor status
  for dbJobRow in dbJobRows:
    stalled = False
    
    if dbJobRow['jobsub_id'] not in condorJobs:
      # if Condor is unaware of the job, then it's stalled
      justin.logLine('Job %s is not known to HTCondor - marking as stalled' 
              % dbJobRow['jobsub_id'])
      stalled = True
      
    else:
      (jobStatus, enteredCurrentStatus) = condorJobs[dbJobRow['jobsub_id']]

      # Allow 60 seconds for race conditions, but if DB thinks 
      # job is waiting or running but Condor says something else, 
      # then mark as stalled - but submitted can be in HTCondor HELD
      if ((enteredCurrentStatus < nowTime - 60) and 
          (jobStatus == justin.htcondorREMOVED or 
           jobStatus == justin.htcondorCOMPLETED or 
           (jobStatus == justin.htcondorHELD and 
            dbJobRow['job_state'] != 'submitted'))):
          justin.logLine('Job %s is in HTCondor state %d - marking as stalled' %
                (dbJobRow['jobsub_id'], jobStatus))
          stalled = True
        
    if stalled:
      try:
        justin.insertUpdate('UPDATE jobs '
              'SET job_state="stalled",finished_time=NOW() '
              'WHERE justin_job_id="%s" AND '
              'job_state IN ("submitted","started","processing","outputting")'
              % dbJobRow['justin_job_id'])
      except Exception as e:
        justin.logLine('Failed to update stalled job - exiting')
        sys.exit(0)

      justin.logEvent(eventTypeID = justin.event_JOB_STALLED_HTCONDOR,
                      workflowID  = dbJobRow['workflow_id'],
                      stageID     = dbJobRow['stage_id'],
                      justinJobID = dbJobRow['justin_job_id'],
                      siteID      = dbJobRow['site_id'],
                      entryID     = dbJobRow['entry_id'])

      resetFilesForJob(justinJobID = dbJobRow['justin_job_id'],
                       jobsubID    = dbJobRow['jobsub_id'],
                       workflowID  = dbJobRow['workflow_id'],
                       stageID     = dbJobRow['stage_id'],
                       siteID      = dbJobRow['site_id'],
                       entryID     = dbJobRow['entry_id'])

  # All ok, so commit it all to the DB
  justin.conn.commit()

def oneCycle():

  findTokensToRefresh()
  findSubmittedWorkflows()
  findFiles()
  findReplicas()
  findProblemWorkflows()
  findWorkflowsToFinish()
  finishWorkflows()
  findStalledAbortedJobs()
  manageCaches()

  # Is it time to update CILogon Jwks
  try:
    lastUpdateCondorJobs = \
              os.stat('/var/run/justin/last-updates/condorjobs').st_mtime
  except:
    lastUpdateCondorJobs = 0

  if lastUpdateCondorJobs + updateCondorJobs < time.time():
    pathlib.Path('/var/run/justin/last-updates/condorjobs').touch(exist_ok=True)
    findStalledCondorJobs()
#
# PROGRAM MAIN
#

if __name__ == '__main__':
  justin.agentMainLoop('finder', oneCycle, sleepSeconds)
