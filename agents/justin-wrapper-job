#!/usr/bin/env python3
#
# justin-wrapper-job - HTCondor job submitted by justin-job-factory agent
#
# Copyright 2013-25, Andrew McNab for the University of Manchester
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import os.path
import sys
import ssl
import time
import json
import zlib
import glob
import hmac
import hashlib
import shutil
import base64
import urllib
import urllib.request
import random
import subprocess

# Globals used again and again
siteName        = 'XX_UNKNOWN'
jobsubJobID     = None
justinJobSecret = '###justin_job_secret###'
jobscriptImage  = '###justin_jobscript_image###'
workflowID      = ###justin_workflow_id###
stageID         = ###justin_stage_id###

def logLine(text):
  print(time.strftime('%b %d %H:%M:%S ' + text + '\n'))

def sendJsonURL(url, sendDict, timeout = 60):

  sendDict['jobsub_id']   = jobsubJobID
  sendDict['workflow_id'] = workflowID
  sendDict['stage_id']    = stageID
  sendDict['secret_time'] = str(int(time.time()))
  sendDict['secret_hash'] = hmac.new(bytes(justinJobSecret, 'UTF-8'),
                                     (sendDict['method'] +
                                      sendDict['secret_time'] +
                                      jobsubJobID
                                     ).encode(),
                                     hashlib.sha256).hexdigest()

  httpRequest = urllib.request.Request(url,
                   headers = { 'User-Agent'       : 'justin-wrapper-job',
                               'X-Jobid'          : jobsubJobID,
                               'X-DUNE-Site-Name' : siteName,
                               'Expect'           : ''
                             },
                   data = json.dumps(sendDict).encode(),
                   method = 'POST')
  sslContext = ssl.SSLContext()
  sslContext.verify_mode = ssl.CERT_REQUIRED
  sslContext.check_hostname = True
  sslContext.load_verify_locations(capath = 
                       '/cvmfs/grid.cern.ch/etc/grid-security/certificates')

  try:
    response = urllib.request.urlopen(httpRequest, timeout = timeout, 
                                      context = sslContext)
  except urllib.error.URLError as e:
    try:
      httpCode = e.code
    except:
      httpCode = 0

    logLine('Get JSON URL returns HTTP code %d' % httpCode)
    logLine(str(e))
    return { 'status' : httpCode, 'response' : '' }

  except Exception as e:
    logLine('Get JSON URL workflow fails: ' + str(e))
    return { 'status' : 0, 'response' : '' }

  try:
    responseString = response.read().decode('utf-8')
  except:
    logLine('Failed reading response: ' + str(e))
    return { 'status' : 1, 'response' : '' }
  
  if len(responseString) == 0:
    receivedDict = {}
  else:
    try:
      receivedDict = json.loads(responseString)
    except Exception as e:  
      logLine('Failed loading json: ' + str(e))
      return { 'status' : 1, 'response' : responseString }

  receivedDict['status']   = response.status
  receivedDict['response'] = responseString
  return receivedDict

def jobAborted(abortCode, abortedMethod, rseName):

  jobAbortedDict = { "method"         : "job_aborted",
                     "abort_code"     : abortCode,
                     "aborted_method" : abortedMethod,
                     "rse_name"       : rseName
                   }

  sendJsonURL(
    'https://justin-allocator-###justin_instance###.dune.hep.ac.uk'
    '/api/allocator/job_aborted_'
     + str(abortCode),
    jobAbortedDict)

  time.sleep(10)    
  sys.exit(0)

def findGPU(getJobscriptDict):

  # Fall back null values
  getJobscriptDict['gpu_uuid']              = ''
  getJobscriptDict['gpu_nonreserved_bytes'] = 0
  getJobscriptDict['gpu_name']              = ''
  getJobscriptDict['gpu_compute_cap']       = ''
  getJobscriptDict['gpu_driver_version']    = ''
  getJobscriptDict['gpu_vbios_version']     = ''

  cudaVisibleDevices = os.environ.get('CUDA_VISIBLE_DEVICES','')
  if not cudaVisibleDevices:
    # Take the first we will find later, if none explicitly allocated
    cudaVisibleDevices = 'GPU-'

  firstAllocatedNumber = None
  firstAllocatedUUID   = None
  try:
    # Try to handle  CUDA_VISIBLE_DEVICES=1  style
    firstAllocatedNumber = int(cudaVisibleDevices.split(',')[0].strip())
  except:
    # Try to handle  CUDA_VISIBLE_DEVICES=GPU-abcdef  style
    firstAllocatedUUID = cudaVisibleDevices.split(',')[0].strip()
    if not firstAllocatedUUID.startswith('GPU-'):
      firstAllocatedUUID = None

  # Do not understand what is going on, so give up
  if not firstAllocatedNumber and not firstAllocatedUUID:
    return

  try:
    nvidiaSmiOutcome = subprocess.run('nvidia-smi '
                     '--query-gpu=uuid,memory.total,memory.reserved,name,'
                     'compute_cap,driver_version,vbios_version '
                     '--format=csv,noheader,nounits',
                     stdout=subprocess.PIPE,
                     shell=True)

    outcomeLines = nvidiaSmiOutcome.stdout.decode().split('\n')
  except:
    return

  gpuDeviceNumber = 0 
  for outcomeLine in outcomeLines:
      gpuUUID = outcomeLine.split(',')[0]
      
      if ((firstAllocatedUUID is not None 
            and gpuUUID.startswith(firstAllocatedUUID)) or 
          (firstAllocatedNumber is not None 
            and gpuDeviceNumber == firstAllocatedNumber)):
        # Found a device we were allocated - take the first one        
        outcomeList = outcomeLine.split(',')
        getJobscriptDict['gpu_uuid']              = outcomeList[0].strip()
        getJobscriptDict['gpu_nonreserved_bytes'] = \
          1048576 * (int(outcomeList[1].strip()) - int(outcomeList[2].strip()))
        getJobscriptDict['gpu_name']              = outcomeList[3].strip()
        getJobscriptDict['gpu_compute_cap']       = outcomeList[4].strip()
        getJobscriptDict['gpu_driver_version']    = outcomeList[5].strip()
        getJobscriptDict['gpu_vbios_version']     = outcomeList[6].strip()
        return

      gpuDeviceNumber += 1

def executeMetaCatCommand(args):

  for i in range(1, 4):  
    cmd = (
     'source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh ; '
     'setup python v3_9_13 ; setup metacat ; '
     'export X509_USER_PROXY=justin-jobs-production.proxy.pem ; '
     'export METACAT_AUTH_SERVER_URL=###justin_metacat_auth_server_url### ; '
     'export METACAT_SERVER_URL=###justin_metacat_server_outputs_url### ; '
     'export SSL_CERT_DIR=/cvmfs/grid.cern.ch/etc/grid-security/certificates ; '
     'metacat auth login -m x509 dunepro ; '
     'metacat %s 2>&1' % args)
     
    logLine('Try %d/3 of executing these commands in a subshell: %s' % (i, cmd))

    outcome = subprocess.run(cmd, stdout=subprocess.PIPE, shell=True)

    if outcome.returncode == 0:
      logLine('metacat returns %d - success' % outcome.returncode)
      break

    logLine('metacat returns %d - try %d failed'  
            % (outcome.returncode, i))
    time.sleep(1)

  return (outcome.returncode, outcome.stdout)
  
def executeJustinRucioUpload(args):

  cmd = (
     'source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh ; '
     'setup python v3_9_13 ; setup rucio ; setup justin ; '
     'export RUCIO_ACCOUNT=dunepro ; '
     'export X509_USER_PROXY=justin-jobs-production.proxy.pem ; '
     'export SSL_CERT_DIR=/cvmfs/grid.cern.ch/etc/grid-security/certificates ; '
     'unset GFAL_CONFIG_DIR GFAL_PLUGIN_DIR GFAL2_DIR ; '
     '/cvmfs/dune.opensciencegrid.org/products/dune/justin/'
     '###justin_instance###/NULL/bin/justin-rucio-upload '
     '--tries 3 %s 2>&1' % args)

  ret = os.system(cmd)
  logLine('justin-rucio-upload returns %d - %s' 
          % (ret, 'failed' if ret else 'ok'))

  return ret

#
# Start of WebDAV uploads: this code is shared between justin-webdav-upload 
#Â and justin-wrapper-job to make them both self contained
#

def webdavCheckRemoteDirectories(sslContext, token, destinationDir):

  dirSplit = destinationDir.split('/')

  # Go down through the directories looking for the highest one that exists
  for highest in range(len(dirSplit) - 1, 1, -1): 
  
    # Don't do the check once we get down to the bare hostname 
    # But otherwise do stop if the directory exists already
    if highest > 2 and \
       webdavRemoteExists(sslContext, token, '/'.join(dirSplit[:highest + 1])):
      break
      
  for i in range(highest+1, len(dirSplit)):
    webdavRemoteDirCreate(sslContext, token, '/'.join(dirSplit[:i+1]))

def webdavRemoteExists(sslContext, token, url):

  httpRequest = urllib.request.Request(url,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token
                            },
                  method = 'HEAD')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
    httpCode = response.status 

  except urllib.error.URLError as e:
    httpCode = e.code

  except Exception as e:
    print('Unknown error checking if %s exists: %s' % (url, str(e)), 
          file=sys.stderr)
    raise

  if httpCode == 404:
    print('%s does not exist' % url, file=sys.stderr)
    return False
    
  if httpCode == 200:
    print('%s exists' % url, file=sys.stderr)
    return True

  print('Unexpected HTTP code %d when checking for %s' % (httpCode, url),
        file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % httpCode)
  
def webdavRemoteDirCreate(sslContext, token, url):

  httpRequest = urllib.request.Request(url,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token
                            },
                  method = 'MKCOL')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)    
    httpCode = response.status 

  except urllib.error.URLError as e:
    # For MKCOL, HTTP code 405 means directory already exists 
    # http://www.webdav.org/specs/rfc2518.html#rfc.section.8.3.2.p.4
    # Presumably another job created it ...
    if e.code == 405:
      print('%s already exists now!' % url, file=sys.stderr)
      return

    print('HTTP error: ' + str(e), file=sys.stderr)
    raise
    
  except Exception as e:
    print('Unknown error: ' + str(e), file=sys.stderr)
    raise
  
  if httpCode == 201:
    print('%s created, HTTP code %d' % (url, httpCode), file=sys.stderr)
    return

  print('Unexpected HTTP code %d when creating %s' % (httpCode, url),
        file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % httpCode)
  
def webdavPutFile(token, source, destinationDir):

#  destDirSplit = destinationDir.split('/')
#
#  # Remove any final "/" and "//" after the one in "https://"
#  for i in range(2, len(destDirSplit)):
#    if destDirSplit[i] == '':
#      destDirSplit.pop(i)
#      
#  destinationDir = '/'.join(destDirSplit)

  sslContext = ssl.SSLContext()
  sslContext.verify_mode = ssl.CERT_REQUIRED
  sslContext.check_hostname = True
  sslContext.load_verify_locations(capath = 
                       '/cvmfs/grid.cern.ch/etc/grid-security/certificates')

  # Check the directory tree already exists and create if necessary
  webdavCheckRemoteDirectories(sslContext, token, destinationDir)

  sourceFile = source.split('/')[-1]
  destinationURL = destinationDir + '/' + sourceFile
    
  if webdavRemoteExists(sslContext, token, destinationURL):
    print('File already exists at %s' % destinationURL, file=sys.stderr)
    raise RuntimeError('File already exists')
  
  # Do a PUT to the desired destination, but expect a redirect
  httpRequest = urllib.request.Request(destinationURL,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token,
                              'Expect'         : '100-continue'
                            },
                  method = 'PUT')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
    # Success is failure as we want a 307 redirect
    raise RuntimeError('No redirect received')

  except urllib.error.HTTPError as e:

    if e.code == 307:
      # And failure is success
      location = e.headers['Location']
    else:
      print('Unexpected HTTP code %d when creating %s' 
            % (e.code, destinationURL), file=sys.stderr)
      raise
    
  except Exception as e:
    print('Unexpected error when creating %s: %s' 
          % (str(e), destinationURL), file=sys.stderr)
    message = e.read().decode('utf-8')
    print(message, file=sys.stderr)
    raise

  print('Redirected to %s' % location, file=sys.stderr)

  #
  # Now use the URL we got from the redirection
  # 
  httpRequest = urllib.request.Request(location,
                  data = open(source, 'rb'),
                  headers = { 'User-Agent' : 'justin-webdav-upload' },
                  method = 'PUT')

  # Replace .has_header to prevent the default Content-Type 
  # IMO dCache should ignore Content-Type rather than reject the workflow
  httpRequest.has_header \
   = lambda header_name: (header_name == 'Content-type' or
                urllib.request.Request.has_header(httpRequest, header_name))

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except Exception as e:
    print('Error when creating %s: %s'
          % (destinationURL, str(e)), file=sys.stderr)
    raise

  if response.status == 201:
    # Created successfully
    print('Created ' + destinationURL)
    return

  print('Unexpected HTTP code %d when creating %s' 
        % (response.status, destinationURL), file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % response.status)
#
# End of WebDAV uploads code
#

def calculateAdler32(fileName):

  checksum = zlib.adler32(b"")
 
  with open(fileName, "rb") as f:
   block = f.read(16 * 1024)
   while block:
    checksum = zlib.adler32(block, checksum)
    block = f.read(16 * 1024)
    
  return ("%08x" % checksum)

def updateMetadataTmp(fileName, 
                      getJobscriptDict, jobscriptDict, recordResultsDict):
  try:
    metadata = json.load(open('home/workspace/' + fileName + '.json', 'r'))
  except FileNotFoundError:
    metadata = { "metadata" : {} }
  except Exception as e:
    logLine("File home/workspace/%s.json exists but JSON load fails: %s" % 
            (fileName, str(e)))
    raise

  metadata['namespace'] = jobscriptDict['scope']
  metadata['name']      = fileName
  metadata['metadata']['dune.workflow'] = {}

  metadata['metadata']['dune.workflow']['site_name'] \
   = getJobscriptDict['site_name'] 
  metadata['metadata']['dune.workflow']['workflow_id'] \
   = workflowID
  metadata['metadata']['dune.workflow']['stage_id'] \
   = stageID
  metadata['metadata']['dune.workflow']['user'] \
   = jobscriptDict['principal_name']
  metadata['metadata']['dune.workflow']['hostname'] \
   = getJobscriptDict['hostname']

  metadata['metadata']['dune.workflow']['jobscript_start'] \
   = recordResultsDict['jobscript_start']
  metadata['metadata']['dune.workflow']['jobscript_finish'] \
   = recordResultsDict['jobscript_finish']

  metadata['metadata']['dune.workflow']['jobscript_real_seconds'] \
   = recordResultsDict['jobscript_real_seconds']
  metadata['metadata']['dune.workflow']['jobscript_cpu_seconds'] \
   = recordResultsDict['jobscript_user_seconds'] + \
     recordResultsDict['jobscript_sys_seconds']
  metadata['metadata']['dune.workflow']['jobscript_max_rss_bytes'] \
   = recordResultsDict['jobscript_max_rss_kb'] * 1024

  metadata['metadata']['dune.workflow']['cpuinfo'] \
   = getJobscriptDict['cpuinfo']
  metadata['metadata']['dune.workflow']['os_release'] \
   = getJobscriptDict['os_release']
  metadata['metadata']['dune.workflow']['job_id'] \
   = jobsubJobID

  # This gets updated to confirmed if the confirm step after MetaCat/Rucio
  # is successful for ALL output files in this job
  metadata['metadata']['dune.output_status'] = 'recorded'

  # Force lowercase in top level metadata keys names
  for keyName in metadata['metadata']:
    if keyName != keyName.lower():
      metadata['metadata'][keyName.lower()] = metadata['metadata'].pop(keyName)

  with open('tmp.json', 'w') as f:
    f.write(json.dumps(metadata, indent = 4, sort_keys = True))

def createDatasets(datasets):

  with open('datasets.json','w') as f:
    json.dump(datasets, f)

  for i in range(1, 4):  
    cmd = (
     'source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh ; '
     'setup python v3_9_13 ; setup metacat ; setup rucio ; '
     'export X509_USER_PROXY=justin-jobs-production.proxy.pem ; '
     'export METACAT_AUTH_SERVER_URL=###justin_metacat_auth_server_url### ; '
     'export METACAT_SERVER_URL=###justin_metacat_server_outputs_url### ; '
     'metacat auth login -m x509 dunepro ; '
     '/cvmfs/dune.opensciencegrid.org/products/dune/justin/'
     '###justin_instance###/NULL/jobutils/justin-job-datasets')
     
    logLine('Try %d/3 of executing these commands in a subshell: %s' % (i, cmd))

    outcome = subprocess.run(cmd, stdout=subprocess.PIPE, shell=True)

    if outcome.returncode == 0:
      logLine('metacat returns %d - success' % outcome.returncode)
      break

    logLine('metacat returns %d - try %d failed'  
            % (outcome.returncode, i))
    time.sleep(1)

  return (outcome.returncode, outcome.stdout)
  
########################################################################
os.environ['TZ'] = 'UTC0'
logLine('====Start of justin-wrapper-job====')

for i in sorted(os.environ):
  print('%s=%s' % (i, os.environ[i]))

jobsubJobID = os.environ['JUSTIN_JOBSUB_ID']
siteName    = os.environ.get('GLIDEIN_DUNESite', 'XX_UNKNOWN')
entryName   = os.environ.get('GLIDEIN_Entry_Name', 'XX_UNKNOWN')
jobPID      = os.getpid()

justinWorkdir = os.environ['PWD']

# Check our cvmfs paths are mountable
cvmfsPaths = [ '/cvmfs/grid.cern.ch',
               '/cvmfs/dune.opensciencegrid.org',
               '/cvmfs/larsoft.opensciencegrid.org' ]

# REMOVE THIS ASAP!!!! Nasty, nasty, hardy, codey!
if not siteName.startswith('US_NERSC'):
  cvmfsPaths += [ '/cvmfs/dune.osgstorage.org',
                  '/cvmfs/fifeuser1.opensciencegrid.org',
                  '/cvmfs/fifeuser2.opensciencegrid.org',
                  '/cvmfs/fifeuser3.opensciencegrid.org',
                  '/cvmfs/fifeuser4.opensciencegrid.org' ]

for i in cvmfsPaths:
  try:
    s = os.stat(i)
  except Exception as e:
    logLine('Failed checking %s - %s' % (i, str(e)))
    jobAborted(323, 'Failed checking ' + i, '')

  logLine('Check passed: %s' % i)

# Make $HOME directory and workspace subdirectory for Apptainer/Singularity
os.makedirs('home/workspace')

# Make jobutils scripts available to jobscripts in a known location
for i in ['justin-get-file', 'justin-allocated-files', 'pdjson2metadata']:
  os.symlink('/cvmfs/dune.opensciencegrid.org/products/dune/justin/'
             '###justin_instance###/NULL/jobutils/' + i, 'home/' + i)

# Log the visible IP addresses and interfaces
os.system('/usr/sbin/ip addr')

# Assemble values to send to allocator
getJobscriptDict = { 'method' : 'get_jobscript' }

for line in open('/proc/cpuinfo','r').readlines():
  if line.startswith('model name'):
    getJobscriptDict['cpuinfo'] = line.split(':')[1].strip()
    break

getJobscriptDict['os_release'] \
  = open('/etc/redhat-release','r').readlines()[0].strip()
getJobscriptDict['hostname']    = os.environ.get('HOSTNAME', os.uname()[1])

# Always check for a GPU
findGPU(getJobscriptDict)

getJobscriptDict['site_name']   = siteName
getJobscriptDict['entry_name']  = entryName

getJobscriptDict['site_job_id'] = 'XXX'
##  "site_job_id"     : "${JOB_GLIDEIN_SiteWMS_JobId:-unknown}",

for line in open(os.environ['_CONDOR_JOB_AD'],'r').readlines():
  if line.startswith('RequestCpus = '):
    getJobscriptDict['processors'] = int(line.split('=')[1].strip())

  if line.startswith('RequestMemory = '):
    getJobscriptDict['rss_bytes'] = 1048576 * int(line.split('=')[1].strip())

for line in open(os.environ['_CONDOR_MACHINE_AD'],'r').readlines():
  if line.startswith('GLIDEIN_Max_Walltime = '):
    getJobscriptDict['wall_seconds'] = int(line.split('=')[1].strip())
    break

logLine('==== Before try apptainer ====')
getJobscriptDict['has_inner_apptainer'] = (0 == os.system(
   '/cvmfs/oasis.opensciencegrid.org/mis/apptainer/1.2/bin/apptainer '
   'shell --shell /usr/bin/hostname %s' % jobscriptImage
   ))
logLine('==== After try apptainer ====')

# Make no roles CSR 
os.system('openssl req -batch -nodes -newkey rsa:2048 '
          '-keyout justin-jobs-no-roles.key.pem '
          '-out justin-jobs-no-roles.csr.pem')

getJobscriptDict['csr-no-roles'] = \
  open('justin-jobs-no-roles.csr.pem', 'r').read()

justinJobsNoRolesKeyPem = \
  open('justin-jobs-no-roles.key.pem', 'r').read()

# Make production CSR
os.system('openssl req -batch -nodes -newkey rsa:2048 '
          '-keyout justin-jobs-production.key.pem '
          '-out justin-jobs-production.csr.pem')

getJobscriptDict['csr-production'] = \
  open('justin-jobs-production.csr.pem', 'r').read()

justinJobsProductionKeyPem = \
  open('justin-jobs-production.key.pem', 'r').read()

for i in range(1,6):

  # Sleep for up to 60 seconds to spread out job start storms
  logLine('Random sleep ...')
  time.sleep(random.randrange(61))

  logLine('====start justin-get-stage.json====')
  print(json.dumps(getJobscriptDict))
  logLine('====end justin-get-stage.json====')

  jobscriptDict = sendJsonURL(
    'https://justin-allocator-###justin_instance###.dune.hep.ac.uk'
    '/api/allocator/get_jobscript',
    getJobscriptDict)

  logLine('get_jobscript returns HTTP code %d' % jobscriptDict['status'])

  if jobscriptDict['status'] != 503 and jobscriptDict['status'] != 0:
    break
    
if jobscriptDict['status'] != 200:
  logLine('Exiting due to code %d. Received: \n%s'
          % (jobscriptDict['status'], jobscriptDict['response']))
  time.sleep(10)
  sys.exit(0)

# Start of subprocess to send regular heartbeats
sys.stdout.flush()
sys.stderr.flush()
if os.fork() == 0:
  # REMOVE FOR NOW: time.sleep(600)
  # Redirect stdout/stderr
  so = open('heartbeats.log', 'a+')
  os.dup2(so.fileno(), sys.stdout.fileno())
  se = open('heartbeats.log', 'a+')
  os.dup2(se.fileno(), sys.stderr.fileno())
  
  # Send heartbeats until parent exits
  while os.getppid() == jobPID:
    logLine('--- Sending heartbeat ---')
    heartbeatDict = \
      sendJsonURL('https://justin-allocator-###justin_instance###.dune.hep.ac.uk'
                '/api/allocator/send_heartbeat',
                { "method" : "send_heartbeat" },
                timeout = 600
               )
    print(str(heartbeatDict))
    sys.stdout.flush()
    sys.stderr.flush()
    time.sleep(600)
             
  sys.exit(0)    
# End of subprocess to send regular heartbeats

# Stage-defined and standard environment variables for the jobscript
with open('home/justin-jobscript-env.sh','w') as f:
  for (n,v) in jobscriptDict['stage_env']:
    f.write('export %s="%s"\n' % (n,v))
    
  f.write('export RUCIO_ACCOUNT=justinreadonly\n')

  f.write('export JUSTIN_ALLOCATOR='
   'https://justin-allocator-###justin_instance###.dune.hep.ac.uk/api/allocator/\n')
  f.write('export JUSTIN_SITE_NAME=%s\n' % jobscriptDict['site_name'])
  f.write('export JUSTIN_WORKFLOW_ID=%d\n' % workflowID)
  f.write('export JUSTIN_STAGE_ID=%d\n' % stageID)
  f.write('export JUSTIN_SCOPE=%s\n' % jobscriptDict['scope'])
  f.write('export JUSTIN_MQL="%s"\n' % jobscriptDict['mql'])
  f.write('export JUSTIN_PROCESSORS=%d\n' 
          % jobscriptDict['requested_processors'])
  f.write('export JUSTIN_RSS_MIB=%d\n' % 
          int(jobscriptDict['requested_rss_bytes'] / 1048576))
  f.write('export JUSTIN_WALL_SECONDS=%d\n' 
          % jobscriptDict['requested_wall_seconds'])
  f.write('export JUSTIN_JOBSUB_ID=%s\n' % jobsubJobID)
  f.write('export JUSTIN_TIMESTAMP=%d\n' % int(time.time()))
  f.write('export JUSTIN_JOBSCRIPT_SECRET=%s\n' 
          % jobscriptDict['jobscript_secret'])
  f.write('export JUSTIN_SAM_WEB_URI='
          '"https://justin.dune.hep.ac.uk/api/samweb/%s/%s"\n' 
          % (jobscriptDict['jobsub_id'], jobscriptDict['jobscript_secret']))


  if getJobscriptDict['gpu_uuid']:
    f.write('export CUDA_VISIBLE_DEVICES=%s\n' % getJobscriptDict['gpu_uuid'])
    f.write('export LD_LIBRARY_PATH=/.singularity.d/libs\n')

# JSON for justin-get-file command to use
with open('home/justin-get-file.json', 'w') as f:
  f.write(json.dumps({ 'method' : 'get_file',
                       'jobsub_id' : jobscriptDict['jobsub_id'],
                       'jobscript_secret' : jobscriptDict['jobscript_secret']
                     }
                    )
         )

# JSON for justin-allocated-files command to use
with open('home/justin-allocated-files.json', 'w') as f:
  f.write(json.dumps({ 'method' : 'get_allocated_files',
                       'jobsub_id' : jobscriptDict['jobsub_id'],
                       'jobscript_secret' : jobscriptDict['jobscript_secret']
                     }
                    )
         )

# The jobscript in any scripting language supported by the container
with open('home/justin-jobscript','wb') as f:
  f.write(jobscriptDict['jobscript'].encode())

os.chmod('home/justin-jobscript', 0o755)

# Assemble proxy to be used by jobscript
with open('home/justin-jobs-no-roles.proxy.pem', 'w') as f:
  f.write(jobscriptDict['justin-jobs-no-roles.cert.pem'])
  f.write(justinJobsNoRolesKeyPem)
  f.write(jobscriptDict['justin-jobs-no-roles.chain.pem'])

os.chmod('home/justin-jobs-no-roles.proxy.pem', 0o400)

# Assemble proxy to be used by wrapper job itself
with open('justin-jobs-production.proxy.pem', 'w') as f:
  f.write(jobscriptDict['justin-jobs-production.cert.pem'])
  f.write(justinJobsProductionKeyPem)
  f.write(jobscriptDict['justin-jobs-production.chain.pem'])

os.chmod('justin-jobs-production.proxy.pem', 0o400)

# AWT jobs get production proxy too
if 'awt_rses' in jobscriptDict:

  with open('home/justin-awt-rse-list.txt', 'w') as f:
    for (rseName, scheme, pfn) in jobscriptDict['awt_rses']:
      f.write('%s %s %s\n' % (rseName, scheme, pfn))

  with open('home/awt-proxy.pem', 'w') as f:
    f.write(jobscriptDict['justin-jobs-production.cert.pem'])
    f.write(justinJobsProductionKeyPem)
    f.write(jobscriptDict['justin-jobs-production.chain.pem'])

  os.chmod('home/awt-proxy.pem', 0o400)

if getJobscriptDict['has_inner_apptainer']:
  # Wrapper to be run inside the container
  with open('home/jobscript-wrapper.sh','w') as f:
    f.write('''#!/bin/bash
export JUSTIN_PATH="$HOME"
export X509_USER_PROXY="$HOME/justin-jobs-no-roles.proxy.pem"
export SSL_CERT_DIR=/cvmfs/grid.cern.ch/etc/grid-security/certificates
cd workspace
. ../justin-jobscript-env.sh
stdbuf -oL -eL ../justin-jobscript >jobscript.log 2>&1
''')

  os.chmod('home/jobscript-wrapper.sh', 0o755)

  logLine('Inner apptainer available. Run jobscript inside container')
  logLine('Jobscript image is %s' % jobscriptImage)
  logLine('====Start of jobscript execution====')
  jobscriptStartTime=int(time.time())

  jobscriptOutcome = subprocess.run(
  '/usr/bin/time -o time.txt -f "%%e %%U %%S %%M" '
  '/cvmfs/oasis.opensciencegrid.org/mis/apptainer/1.2/bin/apptainer shell '
  '%s '
  '--shell /home/jobscript-wrapper.sh '
  '--containall '
  '--bind /cvmfs '
  '--workdir %s '
  '--home %s/home:/home %s'
  % ('--nv' if getJobscriptDict['gpu_uuid'] else '',
     justinWorkdir, justinWorkdir, jobscriptImage),
  stdout=subprocess.PIPE,
  shell=True,
  timeout=jobscriptDict['requested_wall_seconds']
                                   )
else:
  # Wrapper to be run outside a container
  with open('home/jobscript-wrapper.sh','w') as f:
    f.write('''#!/bin/bash
cd home
export HOME="$PWD"
export JUSTIN_PATH="$HOME"
export X509_USER_PROXY="$HOME/justin-jobs-no-roles.proxy.pem"
export SSL_CERT_DIR=/cvmfs/grid.cern.ch/etc/grid-security/certificates
cd workspace
. ../justin-jobscript-env.sh
stdbuf -oL -eL ../justin-jobscript >jobscript.log 2>&1
''')

  os.chmod('home/jobscript-wrapper.sh', 0o755)

  logLine('Inner apptainer not available. Run jobscript in wrapper job context')
  logLine('====Start of jobscript execution====')
  jobscriptStartTime=int(time.time())

  jobscriptOutcome = subprocess.run('/usr/bin/time -o time.txt '
                                    '-f "%%e %%U %%S %%M" '
                                    'home/jobscript-wrapper.sh ',
                          stdout=subprocess.PIPE,
                          shell=True,
                          timeout=jobscriptDict['requested_wall_seconds']
                                   )

jobscriptFinishTime=int(time.time())
logLine('====End of jobscript execution====')

logLine('#### start subprocess log ####')
print(jobscriptOutcome.stdout)
logLine('#### end subprocess log ####')

try:
  jobscriptLog = open('home/workspace/jobscript.log','r').read()
except:
  jobscriptLog = ''

logLine('Jobscript exit code: %d' % jobscriptOutcome.returncode)
logLine('#### start jobscript log (last 10,000 characters) ####')
print(jobscriptLog[-10000:])
logLine('#### end jobscript log ####')

try:
  t = open('time.txt','r').read()
  logLine('time output: ' + str(t))
  (t1, t2, t3, t4) = t.split()
  logLine('t1=%s,t2=%s,t3=%s,t4=%s,' % (t1,t2,t3,t4))

  jobscriptRealSeconds = int(float(t1.strip()))
  jobscriptUserSeconds = int(float(t2.strip()))
  jobscriptSysSeconds  = int(float(t3.strip()))
  jobscriptMaxRssKB    = int(t4.strip())
except Exception as e:
  logLine('Failed to get time output: ' + str(e))
  jobscriptRealSeconds = 0
  jobscriptUserSeconds = 0
  jobscriptSysSeconds  = 0
  jobscriptMaxRssKB    = 0

recordResultsDict = {
  'method'                 : 'record_results',
  'output_files'           : [],
  'jobscript_log'          : jobscriptLog[-10000:],
  'jobscript_exit'         : jobscriptOutcome.returncode,
  'jobscript_start'        : jobscriptStartTime,
  'jobscript_finish'       : jobscriptFinishTime,
  'jobscript_real_seconds' : jobscriptRealSeconds,
  'jobscript_user_seconds' : jobscriptUserSeconds,
  'jobscript_sys_seconds'  : jobscriptSysSeconds,
  'jobscript_max_rss_kb'   : jobscriptMaxRssKB
}

outputFiles = []

# Find files matching output patterns specified for this stage
for (patternType, pattern, patternID) in jobscriptDict['patterns']:
  try:
    matches = glob.glob('home/workspace/' + pattern)
  except Exception as e:
    logLine('Got exception from glob matching of patterns: ' + str(e))
    continue
    
  for match in matches:
    matchingFile = match.split('/')[-1]
    try:
      fileSize = os.path.getsize(match)
    except:
      logLine('Got exception from os.path.getsize: ' + str(e))
      continue
      
    outputFiles.append((matchingFile, fileSize, patternID, pattern))
    recordResultsDict['output_files'].append((matchingFile, patternID))

recordResultsDict['processed_dids'] = []
try:
  for i in open('home/workspace/justin-processed-dids.txt','r').readlines():
    recordResultsDict['processed_dids'].append(i.strip())
except:
  pass

recordResultsDict['processed_pfns'] = []
try:
  for i in open('home/workspace/justin-processed-pfns.txt','r').readlines():
    recordResultsDict['processed_pfns'].append(i.strip())
except:
  pass

logLine('record results: ' + str(recordResultsDict))
resultsResponseDict = sendJsonURL(
 'https://justin-allocator-###justin_instance###.dune.hep.ac.uk'
 '/api/allocator/record_results', 
 recordResultsDict)

logLine('record_results returns HTTP code %d' % resultsResponseDict['status'])

if resultsResponseDict['status'] == 410:
  logLine('Exiting as job is already marked as gone!')
  sys.exit(0)

if resultsResponseDict['status'] != 200:
  jobAborted(312, 'record_results', '')

logLine('Output RSEs: ' + str(resultsResponseDict['output_rses']))

# Create a logs.tgz file and upload with rucio
# At the very least jobscript.log and ClassAds logs will be there
try:
  shutil.copyfile(os.environ['_CONDOR_JOB_AD'],
                  justinWorkdir + '/home/workspace/condor.job.ad.log')
  shutil.copyfile(os.environ['_CONDOR_MACHINE_AD'],
                  justinWorkdir + '/home/workspace/condor.machine.ad.log')
  shutil.copyfile('heartbeats.log',
                  justinWorkdir + '/home/workspace/heartbeats.log')
except:
  pass

tgzName = jobsubJobID.replace('@','-') + '.logs.tgz'
try:
  ret = os.system('cd home/workspace ; '
                  'tar zcf ../../%s --transform="s,^,%s/," *.log' 
                  % (tgzName, jobsubJobID.replace('@','-')))
except Exception as e:
  logLine('tar zcf %s *.log fails %s' % (tgzName, str(e)))
  ret = 1
  
if ret:
  jobAborted(313, 'create_logs_tgz', '')

# First try to register logs with MetaCat
logLine('Register justin-logs:%s with MetaCat' % tgzName)
try:
  open('tmp.json','w').write('{"namespace":"justin-logs","name":"' 
                             + tgzName + '","size":0}')
except:
  logLine('Failed to create tmp.json for justin-logs:%s' % tgzName)
  jobAborted(314, 'metacat_logs_creation', '')

# The registration with MetaCat itself  
(ret, out) = \
  executeMetaCatCommand('file declare --json -f tmp.json "dune:all"')
if ret:
  logLine('Failed to register justin-logs:%s in MetaCat' % tgzName)
  jobAborted(315, 'metacat_logs_registration', '')

# Check it really was registered with MetaCat
(ret, out) = \
  executeMetaCatCommand('file show --json --metadata justin-logs:%s' 
                        % tgzName)
if ret:
  logLine('Had failed to register justin-logs:%s in MetaCat' % tgzName)
  jobAborted(315, 'metacat_logs_registration', '')

for (rse,scheme) in resultsResponseDict['output_rses'][:3]:
  logLine('Upload justin-logs:%s to %s/%s' % (tgzName, rse, scheme))

  ret = executeJustinRucioUpload('--rse %s '
                                 '--protocol %s '
                                 '--scope justin-logs '
                                 '--dataset logstgz-%d-%s '
                                 '--timeout 1200 '
                                 '%s' 
                                 % (rse, 
                                    scheme, 
                                    1000000 * int(time.time() / 1000000),
                                    rse,
                                    tgzName))
  if ret == 0:
    logLine('Uploaded justin-logs:%s to %s' % (tgzName, rse))
    break
  else:
    logLine('Failed to upload justin-logs:%s to %s' % (tgzName, rse))

if ret:
  logLine('Failed to upload justin-logs:%s' % tgzName)
  jobAborted(316, 'rucio_upload_logs_tgz', '')

# No other uploads if the jobscript returned an error 
if jobscriptOutcome.returncode != 0:
  jobAborted(324, 'jobscript_error', '')

# Now try to upload the output files we recorded as created by the job

# TO DO: DROP THIS FILE WRITE?
# Write out the token for uploading to user's scratch
#with open('user-upload-token', 'w') as f:
#  f.write(resultsResponseDict['user_access_token'])

confirmResultsDict = { 'method'       : 'confirm_results',
                       'output_files' : {}  }

rucioUploadedDIDs = []

# Go through the list of output files
for (fileName, fileSize, intPatternID, pattern) in outputFiles:
  strPatternID = str(intPatternID)

  # Metadata to be declared for all datasets from this pattern
  datasetMetadata = { "dune.workflow" : 
            { "workflow_id"     : workflowID,
              "stage_id"        : stageID,
              "pattern_id"      : intPatternID,
              "file_pattern"    : pattern,
              "user"            : jobscriptDict['principal_name'],
              "processors"      : jobscriptDict['requested_processors'],
              "rss_bytes"       : jobscriptDict['requested_rss_bytes'],
              "wall_seconds"    : jobscriptDict['requested_wall_seconds'],
              "jobscript_image" : jobscriptDict['jobscript_image']
            }
                    }

  if stageID == 1:
    datasetMetadata['dune.workflow']['mql'] = jobscriptDict['mql']

  # Find the destination for this pattern in the resultsResponseDict
  destination = resultsResponseDict['patterns'][strPatternID]['destination']

  if destination.startswith('https://'):
    # Uploading to user scratch

    try:
      uploadStartTime = time.time()
      webdavPutFile(resultsResponseDict['user_access_token'], 
                    'home/workspace/' + fileName,
                    destination)
      uploadEndTime = time.time()
    except:
      jobAborted(317, 'webdav_upload', '')

    confirmResultsDict['output_files'][fileName] = {
                         'pattern_id' : intPatternID,
                         'seconds'    : uploadEndTime - uploadStartTime,
                         'size_bytes' : fileSize }
                                             
  elif resultsResponseDict['output_rses']:
    # Uploading file to Rucio managed storage        

    # Create tmp.json metadata file for this output file
    try:
      updateMetadataTmp(fileName, 
                        getJobscriptDict, jobscriptDict, recordResultsDict)

      fileAdler32 = calculateAdler32('home/workspace/' + fileName)
    except Exception as e:
      logLine('updateMetadataTmp() fails ' + str(e))
      jobAborted(318, 'create_metadata', '')

    # First try to register with MetaCat
    (ret, out) = executeMetaCatCommand('file declare --json -f tmp.json '
                                       '-s %d -c "adler32:%s" '
                                       '"%s:%s"' 
                                       % (fileSize, fileAdler32,
                                          'dune', 'all'
                                      ))

    if ret:
      logLine('Failed to register %s:%s in MetaCat' % (jobscriptDict['scope'], 
                                                       fileName))
      jobAborted(319, 'metacat_registration', '')

    # If that succeeds, then try to register/upload with rucio
    destinationNumber = \
         resultsResponseDict['patterns'][strPatternID]['destination_number']

    createdDatasetDIDs = []

    for (rse,scheme) in resultsResponseDict['output_rses'][:3]:
      datasets = []

      datasetName = '###justin_instance###-w%ds%dp%d-%s' \
                    % (workflowID, stageID, intPatternID, rse)
      if ('%s:%s' % (jobscriptDict['scope'], datasetName)) not in \
            resultsResponseDict['existing_dataset_dids']:
        datasets.append({ 'metadata'         : datasetMetadata,
                          'dataset_scope'    : jobscriptDict['scope'],
                          'dataset_name'     : datasetName,
                          'rse_expression'   : rse,
                          'lifetime_seconds' : 
         resultsResponseDict['patterns'][strPatternID]['lifetime_seconds']})


      if ('%s:%s' % (jobscriptDict['scope'], destination+destinationNumber)) \
         not in resultsResponseDict['existing_dataset_dids']:           
        if resultsResponseDict['patterns'][strPatternID]['rse_expression']:
          datasets.append({ 'metadata'         : datasetMetadata,
                            'dataset_scope'    : jobscriptDict['scope'],
                            'dataset_name'     : destination+destinationNumber,
                            'rse_expression'   : 
         resultsResponseDict['patterns'][strPatternID]['rse_expression'],
                          'lifetime_seconds' : 
         resultsResponseDict['patterns'][strPatternID]['lifetime_seconds']
                        })      
        else:      
          datasets.append({ 'metadata'         : datasetMetadata,
                            'dataset_scope'    : jobscriptDict['scope'],
                            'dataset_name'     : destination+destinationNumber,
                            'lifetime_seconds' : 
         resultsResponseDict['patterns'][strPatternID]['lifetime_seconds']
                        })

      if ('%s:%s' % (jobscriptDict['scope'], destination)) \
          not in resultsResponseDict['existing_dataset_dids']:
        datasets.append({ 'metadata'         : datasetMetadata,
                          'dataset_scope'    : jobscriptDict['scope'],
                          'dataset_name'     : destination,
                          'lifetime_seconds' : 
         resultsResponseDict['patterns'][strPatternID]['lifetime_seconds']
                      })

      # Try to create the per-RSE and destination datasets
      try:
        createDatasets(datasets)
      except Exception as e:
        logLine('createDatasets fails "%s" - try next RSE ' % str(e))
        continue

      # Save created datasets for sending back to allocator
      for dataset in datasets:
        createdDatasets.append(dataset['dataset_scope'] + ':' +
                               dataset['dataset_name'])

      logLine('Try %s:%s to %s/%s' 
              % (jobscriptDict['scope'], fileName, rse, scheme))
 
      uploadStartTime = time.time()
      ret = executeJustinRucioUpload('--rse %s '
                                     '--protocol %s '
                                     '--scope %s '
                                     '--dataset w%ds%dp%d-%s '
                                     '--dataset %s '
                                     '--dataset %s '
                                     '--timeout 1200 '
                                     'home/workspace/%s' %
                                     (rse,
                                      scheme,
                                      jobscriptDict['scope'],
                                      workflowID, stageID, intPatternID, rse,
                                      destination + destinationNumber,
                                      destination,
                                      fileName))
      uploadEndTime = time.time()
      if ret == 0:
        logLine('Uploaded %s:%s to %s in %.3fs' %
                (jobscriptDict['scope'], fileName, rse, 
                 uploadEndTime - uploadStartTime))
        break
      else:
        logLine('Failed to upload %s:%s to %s' 
                % (jobscriptDict['scope'], fileName, rse))

    if ret:
      logLine('Failed to upload %s:%s' % (jobscriptDict['scope'], fileName))
      jobAborted(320, 'rucio_upload', '')

    # Add the file to the per-RSE dataset
    (ret, out) = executeMetaCatCommand('dataset add-files --files "%s:%s" '
                                       '"%s:w%ds%dp%d-%s"' 
                                % ( jobscriptDict['scope'], fileName,
                                    jobscriptDict['scope'], 
                                    workflowID, stageID, intPatternID, rse
                                  )
                                      )

    if ret:
      logLine('Failed to add %s:%s to %s:w%ds%dp%d-%s' 
          % (jobscriptDict['scope'], fileName, 
             jobscriptDict['scope'], workflowID, stageID, intPatternID, rse))
      jobAborted(319, 'metacat_registration', '')

    # Add the file to the numbered dataset
    (ret, out) = executeMetaCatCommand('dataset add-files --files "%s:%s" '
                                       '"%s:%s"' 
                                       % ( jobscriptDict['scope'], fileName,
                                           jobscriptDict['scope'], 
                                           destination + destinationNumber
                                            )
                                      )

    if ret:
      logLine('Failed to add %s:%s to %s' 
              % (jobscriptDict['scope'], fileName, 
                 destination + destinationNumber))
      jobAborted(319, 'metacat_registration', '')

    # Add the file to the destination dataset
    (ret, out) = executeMetaCatCommand('dataset add-files --files "%s:%s" '
                                       '"%s:%s"' 
                                       % ( jobscriptDict['scope'], fileName,
                                           jobscriptDict['scope'], destination
                                         )
                                      )

    if ret:
      logLine('Failed to add %s:%s to %s' 
              % (jobscriptDict['scope'], fileName, destination))
      jobAborted(319, 'metacat_registration', '')

    # Update MetaCat output_status for this file to uploaded
    try:
      (ret, out) = executeMetaCatCommand("file update --metadata "
         "'{\"dune.output_status\":\"uploaded\"}' '%s:%s'" 
         % (jobscriptDict['scope'], fileName))
    except Exception as e:
      logLine('Failed to set %s:%s to output_status=uploaded' 
              % (jobscriptDict['scope'], fileName))
      jobAborted(321, 'set_uploaded', '')

    rucioUploadedDIDs.append(jobscriptDict['scope'] + ':' + fileName)

    confirmResultsDict['created_dataset_dids'] = createdDatasetDIDs

    # Add to list of uploaded files for confirm results
    confirmResultsDict['output_files'][fileName] = {
           'pattern_id' : intPatternID,
           'rse_name'   : rse,
           'seconds'    : uploadEndTime - uploadStartTime,
           'size_bytes' : fileSize }
                                             
                                             
# If all ok, then confirm that to the Workflow Allocator

logLine('confirm results: ' + str(confirmResultsDict))
confirmDict = sendJsonURL(
  'https://justin-allocator-###justin_instance###.dune.hep.ac.uk'
  '/api/allocator/confirm_results',
  confirmResultsDict)

logLine('confirm_results returns HTTP code %d' % confirmDict['status'])

if confirmDict['status'] == 410:
  # Server side, nothing is confirmed. Presumably this was already sorted
  # when the job was marked as stalled or whatever.
  # In this script, we exit now, before the outputs are marked as confirmed 
  # in MetaCat.
  logLine('Exiting as job is already marked as gone!')
  sys.exit(0)

if confirmDict['status'] != 200:
  jobAborted(322, 'confirm_results', '')

## Update MetaCat output_status for all files to confirmed
## THIS SHOULD WORK with update-meta IN ONE GO BUT DOES NOT!
##try: 
#  (ret, out) = executeMetaCatCommand("file update-meta --metadata "
#         "'{\"dune.output_status\":\"confirmed\"}' "
#         "--files %s" % ','.join(rucioUploadedDIDs))
logLine('Using metacat file update instead of update-meta to set all '
        'output files to confirmed!')

try:
  for did in rucioUploadedDIDs:
    (ret, out) = executeMetaCatCommand("file update --metadata "
                                "'{\"dune.output_status\":\"confirmed\"}' "
                                "'%s'" % did)
except Exception as e:
  # We do not abort here, because everything has been already been 
  # confirmed as ok to justIN
  logLine('Failed to set files to output_status=confirmed in MetaCat: '
          + str(e)) 

try:
  heartbeatsLog = open('heartbeats.log','r').read()
except Exception as e:
  logLine('Failed to open heartbeats.log: ' + str(e))
  heartbeatsLog = ''
else:
  logLine('#### start heartbeats log (last 1000 characters) ####')
  print(heartbeatsLog[-1000:])
  logLine('#### end heartbeats log ####')

logLine('====End of justin-wrapper-job====')
time.sleep(10)
sys.exit(0)
########################################################################
