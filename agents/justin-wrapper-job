#!/usr/bin/env python3
#
# justin-wrapper-job - HTCondor job submitted by justin-job-factory agent
#
# Copyright 2013-24, Andrew McNab for the University of Manchester
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import os.path
import sys
import ssl
import time
import json
import zlib
import glob
import hmac
import hashlib
import shutil
import base64
import urllib
import urllib.request
import random
import subprocess

awtWorkflowID   = 1

# Three globals used again and again
siteName        = 'XX_UNKNOWN'
jobsubJobID     = None
justinJobSecret = '###justin_job_secret###'
workflowID      = ###justin_workflow_id###
stageID         = ###justin_stage_id###

def logLine(text):
  print(time.strftime('%b %d %H:%M:%S ' + text + '\n'))

def sendJsonURL(url, sendDict):

  sendDict['jobsub_id']   = jobsubJobID
  sendDict['workflow_id'] = workflowID
  sendDict['stage_id']    = stageID
  sendDict['secret_time'] = str(int(time.time()))
  sendDict['secret_hash'] = hmac.new(bytes(justinJobSecret, 'UTF-8'),
                                     (sendDict['method'] +
                                      sendDict['secret_time'] +
                                      jobsubJobID
                                     ).encode(),
                                     hashlib.sha256).hexdigest()

  httpRequest = urllib.request.Request(url,
                   headers = { 'User-Agent'       : 'justin-wrapper-job',
                               'X-Jobid'          : jobsubJobID,
                               'X-DUNE-Site-Name' : siteName,
                               'Expect'           : ''
                             },
                   data = json.dumps(sendDict).encode(),
                   method = 'POST')
  sslContext = ssl.SSLContext()
  sslContext.verify_mode = ssl.CERT_REQUIRED
  sslContext.check_hostname = True
  sslContext.load_verify_locations(capath = 
                       '/cvmfs/grid.cern.ch/etc/grid-security/certificates')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except urllib.error.URLError as e:
    try:
      httpCode = e.code
    except:
      httpCode = 0

    logLine('Get JSON URL returns HTTP code %d' % httpCode)
    logLine(e.read().decode('utf-8'))
    return { 'status' : httpCode, 'response' : '' }

  except Exception as e:
    logLine('Get JSON URL workflow fails: ' + str(e))
    return { 'status' : 0, 'response' : '' }

  try:
    responseString = response.read().decode('utf-8')
  except:
    logLine('Failed reading response: ' + str(e))
    return { 'status' : 1, 'response' : '' }
  
  if len(responseString) == 0:
    receivedDict = {}
  else:
    try:
      receivedDict = json.loads(responseString)
    except Exception as e:  
      logLine('Failed loading json: ' + str(e))
      return { 'status' : 1, 'response' : responseString }

  receivedDict['status']   = response.status
  receivedDict['response'] = responseString
  return receivedDict

def jobAborted(httpCode, abortedMethod, rseName):

  jobAbortedDict = { "method"         : "job_aborted",
                     "http_code"      : httpCode,
                     "aborted_method" : abortedMethod,
                     "rse_name"       : rseName
                   }

  sendJsonURL(
    'https://justin-allocator-###justin_prodev###.dune.hep.ac.uk'
    '/api/allocator/job_aborted_'
     + str(httpCode),
    jobAbortedDict)

  time.sleep(10)    
  sys.exit(0)

def executeMetaCatCommand(args):

  for i in range(1, 4):  
    cmd = (
     'source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh ; '
     'setup python v3_9_13 ; setup metacat ; '
     'export X509_USER_PROXY=justin-jobs-production.proxy.pem ; '
     'export METACAT_AUTH_SERVER_URL=https://metacat.fnal.gov:8143/auth/dune ; '
     'export METACAT_SERVER_URL=https://metacat.fnal.gov:9443/dune_meta_prod/app ; '
     'metacat auth login -m x509 dunepro ; '
     'metacat %s 2>&1' % args)
     
    logLine('Try %d/3 of executing these commands in a subshell: %s' % (i, cmd))
    ret = os.system(cmd)

    if ret == 0:
      logLine('metacat returns %d - success' % ret)
      break

    logLine('metacat returns %d - try %d failed' % (ret, i))
    time.sleep(1)

  return ret
  
def executeJustinRucioUpload(args):

  for i in range(1, 4):
    cmd = (
     'source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh ; '
     'setup python v3_9_13 ; setup rucio ; setup justin ; '
     'export X509_USER_PROXY=justin-jobs-production.proxy.pem ; '
     'unset GFAL_CONFIG_DIR GFAL_PLUGIN_DIR GFAL2_DIR ; '
     'justin-rucio-upload %s 2>&1' % args)

    logLine('Try %d/3 of executing these commands in a subshell: %s' % (i, cmd))
    ret = os.system(cmd)

    if ret == 0:
      logLine('justin-rucio-upload returns %d - success' % ret)
      break

    logLine('justin-rucio-upload returns %d - try %d failed' % (ret, i))
    time.sleep(1)

  return ret

#
# Start of WebDAV uploads: this code is shared between justin-webdav-upload 
#Â and justin-wrapper-job to make them both self contained
#

def webdavCheckRemoteDirectories(sslContext, token, dirSplit):

  # Go down through the directories looking for the highest one that exists
  for highest in range(len(dirSplit) - 1, 1, -1): 
  
    # Don't do the check once we get down to the bare hostname 
    # But otherwise do stop if the directory exists already
    if highest > 2 and \
       webdavRemoteExists(sslContext, token, '/'.join(dirSplit[:highest + 1])):
      break
      
  for i in range(highest+1, len(dirSplit)):
    webdavRemoteDirCreate(sslContext, token, '/'.join(dirSplit[:i+1]))

def webdavRemoteExists(sslContext, token, url):

  httpRequest = urllib.request.Request(url,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token
                            },
                  method = 'HEAD')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
    httpCode = response.status 

  except urllib.error.URLError as e:
    httpCode = e.code

  except Exception as e:
    print('Unknown error checking if %s exists: %s' % (url, str(e)), 
          file=sys.stderr)
    raise

  if httpCode == 404:
    print('%s does not exist' % url, file=sys.stderr)
    return False
    
  if httpCode == 200:
    print('%s exists' % url, file=sys.stderr)
    return True

  print('Unexpected HTTP code %d when checking for %s' % (httpCode, url),
        file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % httpCode)
  
def webdavRemoteDirCreate(sslContext, token, url):

  httpRequest = urllib.request.Request(url,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token
                            },
                  method = 'MKCOL')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)    
    httpCode = response.status 

  except urllib.error.URLError as e:
    # For MKCOL, HTTP code 405 means directory already exists 
    # http://www.webdav.org/specs/rfc2518.html#rfc.section.8.3.2.p.4
    # Presumably another job created it ...
    if e.code == 405:
      print('%s already exists now!' % url, file=sys.stderr)
      return

    print('HTTP error: ' + str(e), file=sys.stderr)
    raise
    
  except Exception as e:
    print('Unknown error: ' + str(e), file=sys.stderr)
    raise
  
  if httpCode == 201:
    print('%s created, HTTP code %d' % (url, httpCode), file=sys.stderr)
    return

  print('Unexpected HTTP code %d when creating %s' % (httpCode, url),
        file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % httpCode)
  
def webdavPutFile(token, source, destinationDir):

  destDirSplit = destinationDir.split('/')

  # Remove any final "/" and "//" after the one in "https://"
  for i in range(2, len(destDirSplit)):
    if destDirSplit[i] == '':
      destDirSplit.pop(i)
      
  destinationDir = '/'.join(destDirSplit)

  sslContext = ssl.SSLContext()
  sslContext.verify_mode = ssl.CERT_REQUIRED
  sslContext.check_hostname = True
  sslContext.load_verify_locations(capath = 
                       '/cvmfs/grid.cern.ch/etc/grid-security/certificates')

  # Check the directory tree already exists and create if necessary
  webdavCheckRemoteDirectories(sslContext, token, destDirSplit)

  sourceFile = source.split('/')[-1]
  destinationURL = destinationDir + '/' + sourceFile
    
  if webdavRemoteExists(sslContext, token, destinationURL):
    print('File already exists at %s' % destinationURL, file=sys.stderr)
    raise RuntimeError('File already exists')
  
  # Do a PUT to the desired destination, but expect a redirect
  httpRequest = urllib.request.Request(destinationURL,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token,
                              'Expect'         : '100-continue'
                            },
                  method = 'PUT')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
    # Success is failure as we want a 307 redirect
    raise RuntimeError('No redirect received')

  except urllib.error.HTTPError as e:

    if e.code == 307:
      # And failure is success
      location = e.headers['Location']
    else:
      print('Unexpected HTTP code %d when creating %s' 
            % (e.code, destinationURL), file=sys.stderr)
      raise
    
  except Exception as e:
    print('Unexpected error when creating %s: %s' 
          % (str(e), destinationURL), file=sys.stderr)
    message = e.read().decode('utf-8')
    print(message, file=sys.stderr)
    raise

  print('Redirected to %s' % location, file=sys.stderr)

  #
  # Now use the URL we got from the redirection
  # 
  httpRequest = urllib.request.Request(location,
                  data = open(source, 'rb'),
                  headers = { 'User-Agent' : 'justin-webdav-upload' },
                  method = 'PUT')

  # Replace .has_header to prevent the default Content-Type 
  # IMO dCache should ignore Content-Type rather than reject the workflow
  httpRequest.has_header \
   = lambda header_name: (header_name == 'Content-type' or
                urllib.request.Request.has_header(httpRequest, header_name))

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except Exception as e:
    print('Error when creating %s: %s'
          % (destinationURL, str(e)), file=sys.stderr)
    raise

  if response.status == 201:
    # Created successfully
    print('Created ' + destinationURL)
    return

  print('Unexpected HTTP code %d when creating %s' 
        % (response.status, destinationURL), file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % response.status)
#
# End of WebDAV uploads code
#

def calculateAdler32(fileName):

  checksum = zlib.adler32(b"")
 
  with open(fileName, "rb") as f:
   block = f.read(16 * 1024)
   while block:
    checksum = zlib.adler32(block, checksum)
    block = f.read(16 * 1024)
    
  return ("%08x" % checksum)

def updateMetadataTmp(fileScope, fileName, 
                      getJobscriptDict, jobscriptDict, recordResultsDict):
  try:
    metadata = json.load(open('home/workspace/' + fileName + '.json', 'r'))
  except FileNotFoundError:
    metadata = { "metadata" : {} }
  except Exception as e:
    logLine("File home/workspace/%s.json exists but JSON load fails: %s" % 
            (fileName, str(e)))
    raise

  metadata['namespace'] = fileScope
  metadata['name']      = fileName
  metadata['metadata']['dune.workflow'] = {}

  metadata['metadata']['dune.workflow']['site_name'] \
   = getJobscriptDict['site_name'] 
  metadata['metadata']['dune.workflow']['workflow_id'] \
   = workflowID
  metadata['metadata']['dune.workflow']['stage_id'] \
   = stageID
  metadata['metadata']['dune.workflow']['user'] \
   = jobscriptDict['principal_name']
  metadata['metadata']['dune.workflow']['hostname'] \
   = getJobscriptDict['hostname']

  metadata['metadata']['dune.workflow']['jobscript_start'] \
   = recordResultsDict['jobscript_start']
  metadata['metadata']['dune.workflow']['jobscript_finish'] \
   = recordResultsDict['jobscript_finish']

  metadata['metadata']['dune.workflow']['jobscript_real_seconds'] \
   = recordResultsDict['jobscript_real_seconds']
  metadata['metadata']['dune.workflow']['jobscript_cpu_seconds'] \
   = recordResultsDict['jobscript_user_seconds'] + \
     recordResultsDict['jobscript_sys_seconds']

  metadata['metadata']['dune.workflow']['cpuinfo'] \
   = getJobscriptDict['cpuinfo']
  metadata['metadata']['dune.workflow']['os_release'] \
   = getJobscriptDict['os_release']
  metadata['metadata']['dune.workflow']['job_id'] \
   = jobsubJobID

  # This gets updated to confirmed if the confirm step after MetaCat/Rucio
  # is successful for ALL output files in this job
  metadata['metadata']['dune.output_status'] = 'recorded'

  # Force lowercase in top level metadata keys names
  for keyName in metadata['metadata']:
    if keyName != keyName.lower():
      metadata['metadata'][keyName.lower()] = metadata['metadata'].pop(keyName)

  with open('tmp.json', 'w') as f:
    f.write(json.dumps(metadata, indent = 4, sort_keys = True))
  
########################################################################
logLine('====Start of justin-wrapper-job====')

for i in sorted(os.environ):
  print('%s=%s' % (i, os.environ[i]))

jobsubJobID = os.environ['JUSTIN_JOBSUB_ID']
siteName    = os.environ.get('GLIDEIN_DUNESite', 'XX_UNKNOWN')
entryName   = os.environ.get('GLIDEIN_Entry_Name', 'XX_UNKNOWN')
jobPID      = os.getpid()

justinWorkdir = os.environ['PWD']

# Make $HOME directory and workspace subdirectory for Apptainer/Singularity
os.makedirs('home/workspace')

# Make jobutils scripts available to jobscripts in a known location
for i in ['justin-get-file', 'justin-allocated-files', 'pdjson2metadata']:
  os.symlink('/cvmfs/dune.opensciencegrid.org/products/dune/justin/'
             '###justin_prodev###/NULL/jobutils/' + i, 'home/' + i)

# Assemble values to send to allocator
getJobscriptDict = { 'method' : 'get_jobscript' }

for line in open('/proc/cpuinfo','r').readlines():
  if line.startswith('model name'):
    getJobscriptDict['cpuinfo'] = line.split(':')[1].strip()
    break

getJobscriptDict['os_release'] \
  = open('/etc/redhat-release','r').readlines()[0].strip()
getJobscriptDict['hostname']    = os.environ.get('HOSTNAME', os.uname()[1])

#os.system('nvidia-smi')

getJobscriptDict['site_name']   = siteName
getJobscriptDict['entry_name']  = entryName

getJobscriptDict['site_job_id'] = 'XXX'
##  "site_job_id"     : "${JOB_GLIDEIN_SiteWMS_JobId:-unknown}",

for line in open(os.environ['_CONDOR_JOB_AD'],'r').readlines():
  if line.startswith('RequestCpus = '):
    getJobscriptDict['processors'] = int(line.split('=')[1].strip())

  if line.startswith('RequestMemory = '):
    getJobscriptDict['rss_bytes'] = 1048576 * int(line.split('=')[1].strip())

for line in open(os.environ['_CONDOR_MACHINE_AD'],'r').readlines():
  if line.startswith('GLIDEIN_Max_Walltime = '):
    getJobscriptDict['wall_seconds'] = int(line.split('=')[1].strip())
    break

logLine('==== Before try apptainer ====')
getJobscriptDict['has_inner_apptainer'] = (0 == os.system(
   '/cvmfs/oasis.opensciencegrid.org/mis/apptainer/current/bin/apptainer '
   'shell --shell /usr/bin/hostname '
   '/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:latest'
   ))
logLine('==== After try apptainer ====')

# Make no roles CSR 
os.system('openssl req -batch -nodes -newkey rsa:2048 '
          '-keyout justin-jobs-no-roles.key.pem '
          '-out justin-jobs-no-roles.csr.pem')

getJobscriptDict['csr-no-roles'] = \
  open('justin-jobs-no-roles.csr.pem', 'r').read()

justinJobsNoRolesKeyPem = \
  open('justin-jobs-no-roles.key.pem', 'r').read()

# Make production CSR
os.system('openssl req -batch -nodes -newkey rsa:2048 '
          '-keyout justin-jobs-production.key.pem '
          '-out justin-jobs-production.csr.pem')

getJobscriptDict['csr-production'] = \
  open('justin-jobs-production.csr.pem', 'r').read()

justinJobsProductionKeyPem = \
  open('justin-jobs-production.key.pem', 'r').read()

for i in range(1,6):

  # Sleep for up to 60 seconds to spread out job start storms
  logLine('Random sleep ...')
  time.sleep(random.randrange(61))

  logLine('====start justin-get-stage.json====')
  print(json.dumps(getJobscriptDict))
  logLine('====end justin-get-stage.json====')

  jobscriptDict = sendJsonURL(
    'https://justin-allocator-###justin_prodev###.dune.hep.ac.uk'
    '/api/allocator/get_jobscript',
    getJobscriptDict)

  logLine('get_jobscript returns HTTP code %d' % jobscriptDict['status'])

  if jobscriptDict['status'] != 503 and jobscriptDict['status'] != 0:
    break
    
if jobscriptDict['status'] != 200:
  logLine('Exiting due to code %d. Received: \n%s'
          % (jobscriptDict['status'], jobscriptDict['response']))
  time.sleep(10)
  sys.exit(0)

# Start of subprocess to send regular heartbeats
if os.fork() == 0:
  time.sleep(600)
  # Redirect stdout/stderr
  so = open('heartbeats.log', 'a+')
  os.dup2(so.fileno(), sys.stdout.fileno())
  se = open('heartbeats.log', 'a+')
  os.dup2(se.fileno(), sys.stderr.fileno())
  
  # Send heartbeats until parent exits
  while os.getppid() == jobPID:
    sendJsonURL('https://justin-allocator-###justin_prodev###.dune.hep.ac.uk'
                '/api/allocator/send_heartbeat',
                { "method" : "send_heartbeat" }
               )
    time.sleep(600)
             
  sys.exit(0)    
# End of subprocess to send regular heartbeats

# Stage-defined and standard environment variables for the jobscript
with open('home/justin-jobscript-env.sh','w') as f:
  for (n,v) in jobscriptDict['stage_env']:
    f.write('export %s="%s"\n' % (n,v))

# REMOVE THIS
  f.write('export JUSTIN_PRO_DEV=###justin_prodev###\n')
# ^^^^^^^^^^^

  f.write('export JUSTIN_ALLOCATOR='
   'https://justin-allocator-###justin_prodev###.dune.hep.ac.uk/api/allocator/\n')
  f.write('export JUSTIN_SITE_NAME=%s\n' % jobscriptDict['site_name'])
  f.write('export JUSTIN_WORKFLOW_ID=%d\n' % workflowID)
  f.write('export JUSTIN_STAGE_ID=%d\n' % stageID)
  f.write('export JUSTIN_SCOPE=%s\n' % jobscriptDict['scope'])
  f.write('export JUSTIN_PROCESSORS=%d\n' 
          % jobscriptDict['requested_processors'])
  f.write('export JUSTIN_RSS_MB=%d\n' % 
          int(jobscriptDict['requested_rss_bytes'] / 1048576))
  f.write('export JUSTIN_WALL_SECONDS=%d\n' 
          % jobscriptDict['requested_wall_seconds'])
  f.write('export JUSTIN_JOBSUB_ID=%s\n' % jobsubJobID)
  f.write('export JUSTIN_JOBSCRIPT_SECRET=%s\n' 
          % jobscriptDict['jobscript_secret'])
  f.write('export JUSTIN_SAM_WEB_URI='
          '"https://justin.dune.hep.ac.uk/api/samweb/%s/%s"\n' 
          % (jobscriptDict['jobsub_id'], jobscriptDict['jobscript_secret']))

# JSON for justin-get-file command to use
with open('home/justin-get-file.json', 'w') as f:
  f.write(json.dumps({ 'method' : 'get_file',
                       'jobsub_id' : jobscriptDict['jobsub_id'],
                       'jobscript_secret' : jobscriptDict['jobscript_secret']
                     }
                    )
         )

# JSON for justin-allocated-files command to use
with open('home/justin-allocated-files.json', 'w') as f:
  f.write(json.dumps({ 'method' : 'get_allocated_files',
                       'jobsub_id' : jobscriptDict['jobsub_id'],
                       'jobscript_secret' : jobscriptDict['jobscript_secret']
                     }
                    )
         )

# The jobscript in any scripting language supported by the container
with open('home/justin-jobscript','wb') as f:
  f.write(jobscriptDict['jobscript'].encode())

os.chmod('home/justin-jobscript', 0o755)

# Assemble proxy to be used by jobscript
with open('home/justin-jobs-no-roles.proxy.pem', 'w') as f:
  f.write(jobscriptDict['justin-jobs-no-roles.cert.pem'])
  f.write(justinJobsNoRolesKeyPem)
  f.write(jobscriptDict['justin-jobs-no-roles.chain.pem'])

os.chmod('home/justin-jobs-no-roles.proxy.pem', 0o400)

# Assemble proxy to be used by wrapper job itself
with open('justin-jobs-production.proxy.pem', 'w') as f:
  f.write(jobscriptDict['justin-jobs-production.cert.pem'])
  f.write(justinJobsProductionKeyPem)
  f.write(jobscriptDict['justin-jobs-production.chain.pem'])

os.chmod('justin-jobs-production.proxy.pem', 0o400)

# AWT jobs get production proxy too
if workflowID == awtWorkflowID:

  with open('home/justin-awt-rse-list.txt', 'w') as f:
    for (rseName, scheme, pfn) in jobscriptDict['awt_rses']:
      f.write('%s %s %s\n' % (rseName, scheme, pfn))

  with open('home/awt-proxy.pem', 'w') as f:
    f.write(jobscriptDict['justin-jobs-production.cert.pem'])
    f.write(justinJobsProductionKeyPem)
    f.write(jobscriptDict['justin-jobs-production.chain.pem'])

  os.chmod('home/awt-proxy.pem', 0o400)

if getJobscriptDict['has_inner_apptainer']:
  # Wrapper to be run inside the container
  with open('home/jobscript-wrapper.sh','w') as f:
    f.write('''#!/bin/sh
export JUSTIN_PATH="$HOME"
export X509_USER_PROXY="$HOME/justin-jobs-no-roles.proxy.pem"
cd workspace
. ../justin-jobscript-env.sh
stdbuf -oL -eL ../justin-jobscript >jobscript.log 2>&1
''')

  os.chmod('home/jobscript-wrapper.sh', 0o755)

  logLine('Inner apptainer available. Run jobscript inside container')
  logLine('====Start of jobscript execution====')
  jobscriptStartTime=int(time.time())

  jobscriptOutcome = subprocess.run(
  '/usr/bin/time -o time.txt -f "%%e %%U %%S %%M" '
  '/cvmfs/oasis.opensciencegrid.org/mis/apptainer/current/bin/apptainer shell '
  '--shell /home/jobscript-wrapper.sh '
  '--containall '
  '--bind /cvmfs '
  '--workdir %s '
  '--home %s/home:/home '
  '/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:latest '
  % (justinWorkdir, justinWorkdir),
  stdout=subprocess.PIPE,
  shell=True 
  )
  
else:
  # Wrapper to be run outside a container
  with open('home/jobscript-wrapper.sh','w') as f:
    f.write('''#!/bin/sh
cd home
export HOME="$PWD"
export JUSTIN_PATH="$HOME"
export X509_USER_PROXY="$HOME/justin-jobs-no-roles.proxy.pem"
cd workspace
. ../justin-jobscript-env.sh
stdbuf -oL -eL ../justin-jobscript >jobscript.log 2>&1
''')

  os.chmod('home/jobscript-wrapper.sh', 0o755)

  logLine('Inner apptainer not available. Run jobscript in wrapper job context')
  logLine('====Start of jobscript execution====')
  jobscriptStartTime=int(time.time())

  jobscriptOutcome = subprocess.run('/usr/bin/time -o time.txt '
                                    '-f "%%e %%U %%S %%M" '
                                    'home/jobscript-wrapper.sh ',
                                    stdout=subprocess.PIPE,
                                    shell=True 
                                   )

jobscriptFinishTime=int(time.time())
logLine('====End of jobscript execution====')

logLine('#### start subprocess log ####')
print(jobscriptOutcome.stdout)
logLine('#### end subprocess log ####')

try:
  jobscriptLog = open('home/workspace/jobscript.log','r').read()
except:
  jobscriptLog = ''

logLine('Jobscript exit code: %d' % jobscriptOutcome.returncode)
logLine('#### start jobscript log ####')
print(jobscriptLog)
logLine('#### end jobscript log ####')

try:
  t = open('time.txt','r').read()
  logLine('time output: ' + str(t))
  (t1, t2, t3, t4) = t.split()
  logLine('t1=%s,t2=%s,t3=%s,t4=%s,' % (t1,t2,t3,t4))

  jobscriptRealSeconds = int(float(t1))
  jobscriptUserSeconds = int(float(t2))
  jobscriptSysSeconds  = int(float(t3))
  jobscriptMaxRssKB    = int(t4)
except Exception as e:
  logLine('Failed to get time output: ' + str(e))
  jobscriptRealSeconds = 0
  jobscriptUserSeconds = 0
  jobscriptSysSeconds  = 0
  jobscriptMaxRssKB    = 0

justinRecordResultsDict = {
  'method'                 : 'record_results',
  'output_urls'            : [],
  'output_dids'            : [],
  'next_stage_dids'        : [],
  'jobscript_log'          : jobscriptLog[-10000:],
  'jobscript_exit'         : jobscriptOutcome.returncode,
  'jobscript_start'        : jobscriptStartTime,
  'jobscript_finish'       : jobscriptFinishTime,
  'jobscript_real_seconds' : jobscriptRealSeconds,
  'jobscript_user_seconds' : jobscriptUserSeconds,
  'jobscript_sys_seconds'  : jobscriptSysSeconds,
  'jobscript_max_rss_kb'   : jobscriptMaxRssKB
}

outputFiles = []

# Find files matching output patterns specified for this stage
for (forNextStage, destination, scope, pattern) in \
    jobscriptDict['patterns']:
  
  matches = glob.glob('home/workspace/' + pattern)
  for match in matches:
    matchingFile = match.split('/')[-1]
    fileSize     = os.path.getsize(match)
    outputFiles.append((destination, scope, matchingFile, fileSize))
    
    if scope == '::URL::':
      justinRecordResultsDict['output_urls'].append(destination + '/' +
                                                        matchingFile)
    elif forNextStage:
      justinRecordResultsDict['next_stage_dids'].append(scope + ':' + 
                                                        matchingFile)    
    else:
      justinRecordResultsDict['output_dids'].append(scope + ':' + matchingFile)

justinRecordResultsDict['processed_dids'] = []
try:
  for i in open('home/workspace/justin-processed-dids.txt','r').readlines():
    justinRecordResultsDict['processed_dids'].append(i.strip())
except:
  pass

justinRecordResultsDict['processed_pfns'] = []
try:
  for i in open('home/workspace/justin-processed-pfns.txt','r').readlines():
    justinRecordResultsDict['processed_pfns'].append(i.strip())
except:
  pass

logLine('record results: ' + str(justinRecordResultsDict))
resultsResponseDict = sendJsonURL(
 'https://justin-allocator-###justin_prodev###.dune.hep.ac.uk'
 '/api/allocator/record_results', 
 justinRecordResultsDict)

logLine('record_results returns HTTP code %d' % resultsResponseDict['status'])

if resultsResponseDict['status'] == 410:
  logLine('Exiting as job is already marked as gone!')
  sys.exit(0)

if resultsResponseDict['status'] != 200:
  jobAborted(resultsResponseDict['status'], 'record_results', '')

# Create a logs.tgz file and upload with rucio
# At the very least jobscript.log and ClassAds logs will be there
try:
  shutil.copyfile(os.environ['_CONDOR_JOB_AD'],
                  justinWorkdir + '/home/workspace/condor.job.ad.log')
  shutil.copyfile(os.environ['_CONDOR_MACHINE_AD'],
                  justinWorkdir + '/home/workspace/condor.machine.ad.log')
except:
  pass

tgzName = jobsubJobID.replace('@','-') + '.logs.tgz'
try:
  ret = os.system('cd home/workspace ; '
                  'tar zcf ../../%s --transform="s,^,%s/," *.log' 
                  % (tgzName, jobsubJobID.replace('@','-')))
except Exception as e:
  logLine('tar zcf %s *.log fails %s' % (tgzName, str(e)))
  ret = 1
  
if ret:
  jobAborted(900, 'create_logs_tgz', '')

# First try to register logs with MetaCat
logLine('Register justin-logs:%s with MetaCat' % tgzName)
try:
  open('tmp.json','w').write('{"namespace":"justin-logs","name":"' 
                             + tgzName + '","size":0}')
except:
  logLine('Failed to create tmp.json for justin-logs:%s' % tgzName)
  jobAborted(900, 'metacat_logs_creation', '')
  
ret = executeMetaCatCommand('file declare --json -f tmp.json "dune:all"')
if ret:
  logLine('Failed to register justin-logs:%s in MetaCat' % tgzName)
  jobAborted(900, 'metacat_logs_registration', '')

for (rse,scheme) in jobscriptDict['output_rses'][:3]:
  logLine('Upload justin-logs:%s to %s/%s' % (tgzName, rse, scheme))

  ret = executeJustinRucioUpload('--rse %s '
                                 '--protocol %s '
                                 '--scope justin-logs '
                                 '--dataset logstgz-%d-%s '
                                 '--timeout 1200 '
                                 '%s' 
                                 % (rse, 
                                    scheme, 
                                    1000000 * int(time.time() / 1000000),
                                    rse,
                                    tgzName))
  if ret == 0:
    logLine('Uploaded justin-logs:%s to %s' % (tgzName, rse))
    break
  else:
    logLine('Failed to upload justin-logs:%s to %s' % (tgzName, rse))

if ret:
  logLine('Failed to upload justin-logs:%s' % tgzName)
  jobAborted(900, 'rucio_upload_logs_tgz', '')

# No other uploads if the jobscript returned an error 
if jobscriptOutcome.returncode != 0:
  jobAborted(900, 'jobscript_error', '')

# Now try to upload the output files we recorded as created by the job
logLine('Output RSEs: ' + str(jobscriptDict['output_rses']))

# TO DO: DROP THIS FILE WRITE?
# Write out the token for uploading to user's scratch
#with open('user-upload-token', 'w') as f:
#  f.write(resultsResponseDict['user_access_token'])

confirmResultsDict = { 'method'      : 'confirm_results',
                       'output_dids' : {},
                       'output_urls' : []  }

rucioUploadedDIDs = []

# Go through the list of output files
for (destination, fileScope, fileName, fileSize) in outputFiles:
  
  if fileScope == '::URL::': 
    # Uploading to user scratch

    try:
      webdavPutFile(resultsResponseDict['user_access_token'], 
                    'home/workspace/' + fileName,
                    destination)
    except:
      jobAborted(900, 'webdav_upload', '')
    
    confirmResultsDict['output_urls'].append(destination + '/' + fileName)
  elif jobscriptDict['output_rses']:
    # Uploading to Rucio managed storage
    
    # Create tmp.json metadata file for this output file
    try:
      updateMetadataTmp(fileScope, fileName, 
                     getJobscriptDict, jobscriptDict, justinRecordResultsDict)

      fileAdler32 = calculateAdler32('home/workspace/' + fileName)
    except Exception as e:
      logLine('updateMetadataTmp() fails ' + str(e))
      jobAborted(900, 'update_metadata', '')

    # First try to register with MetaCat
    ret = executeMetaCatCommand('file declare --json -f tmp.json '
                                '-s %d -c "adler32:%s" '
                                '"%s:%s"' 
                                  % (fileSize, fileAdler32,
                                     'dune', 'all'
                                     ))

    if ret:
      logLine('Failed to register %s:%s in MetaCat' % (fileScope, fileName))
      jobAborted(900, 'metacat_registration', '')

    # If that succeeds, then try to register/upload with rucio    
    for (rse,scheme) in jobscriptDict['output_rses'][:3]:
      logLine('Try %s:%s to %s/%s' 
              % (fileScope, fileName, rse, scheme))
 
      uploadStartTime = time.time()
      ret = executeJustinRucioUpload('--rse %s '
                                     '--protocol %s '
                                     '--scope %s '
                                     '--dataset %s '
                                     '--timeout 1200 '
                                     'home/workspace/%s' %
                                     (rse,
                                      scheme,
                                      fileScope,
                                      destination,
                                      fileName))
      uploadEndTime = time.time()
      if ret == 0:
        logLine('Uploaded %s:%s to %s in %.3fs' %
                (fileScope, fileName, rse, uploadEndTime - uploadStartTime))
        break
      else:
        logLine('Failed to upload %s:%s to %s' % (fileScope, fileName, rse))

    if ret:
      logLine('Failed to upload %s:%s' % (fileScope, fileName))
      jobAborted(900, 'rucio_upload', '')

    # Update MetaCat output_status for this file to uploaded
    try:
      ret = executeMetaCatCommand("file update --metadata "
         "'{\"dune.output_status\":\"uploaded\"}' '%s:%s'" 
         % (fileScope, fileName))
    except Exception as e:
      logLine('Failed to set %s:%s to output_status=uploaded' 
              % (fileScope, fileName))
      jobAborted(900, 'set_uploaded', '')

    rucioUploadedDIDs.append(fileScope + ':' + fileName)

    # Add to list of uploaded files for confirm results
    confirmResultsDict['output_dids'][fileScope + ':' + fileName] = \
      { "rse_name"   : rse, 
        "seconds"    : uploadEndTime - uploadStartTime,
        "size_bytes" : fileSize }

# If all ok, then confirm that to the Workflow Allocator

logLine('confirm results: ' + str(confirmResultsDict))
confirmDict = sendJsonURL(
  'https://justin-allocator-###justin_prodev###.dune.hep.ac.uk'
  '/api/allocator/confirm_results',
  confirmResultsDict)

logLine('confirm_results returns HTTP code %d' % confirmDict['status'])

if confirmDict['status'] == 410:
  # Server side, nothing is confirmed. Presumably this was already sorted
  # when the job was marked as stalled or whatever.
  # In this script, we exit now, before the outputs are marked as confirmed 
  # in MetaCat.
  logLine('Exiting as job is already marked as gone!')
  sys.exit(0)

if confirmDict['status'] != 200:
  jobAborted(900, 'confirm_results', '')

## Update MetaCat output_status for all files to confirmed
## THIS SHOULD WORK with update-meta IN ONE GO BUT DOES NOT!
##try: 
#  ret = executeMetaCatCommand("file update-meta --metadata "
#         "'{\"dune.output_status\":\"confirmed\"}' "
#         "--files %s" % ','.join(rucioUploadedDIDs))
logLine('Using metacat file update intead of update-meta to set all '
        'output files to confirmed!')

try:
  for did in rucioUploadedDIDs:
    ret = executeMetaCatCommand("file update --metadata "
                                "'{\"dune.output_status\":\"confirmed\"}' "
                                "'%s'" % did)
except Exception as e:
  # We do not abort here, because everything has been already been 
  # confirmed as ok to justIN
  logLine('Failed to set files to output_status=confirmed in MetaCat: '
          + str(e)) 

logLine('====End of justin-wrapper-job====')
time.sleep(10)
sys.exit(0)
########################################################################
