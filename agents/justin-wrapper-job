#!/usr/bin/env python3
#
# justin-wrapper-job - HTCondor job submitted by justin-job-factory agent
#
# Copyright 2013-23, Andrew McNab for the University of Manchester
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import os.path
import sys
import ssl
import time
import json
import zlib
import glob
import hmac
import hashlib
import base64
import urllib
import urllib.request
import subprocess

# Three globals used again and again
siteName        = 'XX_UNKNOWN'
jobsubJobID     = None
justinJobSecret = '###justin_job_secret###'

def logLine(text):
  print(time.strftime('%b %d %H:%M:%S ' + text + '\n'))

def sendJsonURL(url, sendDict):

  sendDict['jobsub_id']   = jobsubJobID
  sendDict['secret_time'] = str(int(time.time()))
  sendDict['secret_hash'] = hmac.new(bytes(justinJobSecret, 'UTF-8'),
                                     (sendDict['method'] +
                                      sendDict['secret_time'] +
                                      jobsubJobID
                                     ).encode(),
                                     hashlib.sha256).hexdigest()

  httpRequest = urllib.request.Request(url,
                   headers = { 'User-Agent'       : 'justin-wrapper-job',
                               'X-Jobid'          : jobsubJobID,
                               'X-DUNE-Site-Name' : siteName,
                               'Expect'           : ''
                             },
                   data = json.dumps(sendDict).encode(),
                   method = 'POST')
  sslContext = ssl.SSLContext()
  sslContext.verify_mode = ssl.CERT_REQUIRED
  sslContext.check_hostname = True
  sslContext.load_verify_locations(capath = 
                       '/cvmfs/grid.cern.ch/etc/grid-security/certificates')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except urllib.error.URLError as e:
    try:
      httpCode = e.code
    except:
      httpCode = 0

    logLine('Get JSON URL returns HTTP code %d' % httpCode)
    logLine(e.read().decode('utf-8'))
    return { 'status' : httpCode, 'response' : '' }

  except Exception as e:
    logLine('Get JSON URL workflow fails: ' + str(e))
    return { 'status' : 0, 'response' : '' }

  try:
    responseString = response.read().decode('utf-8')
  except:
    logLine('Failed reading response: ' + str(e))
    return { 'status' : 1, 'response' : '' }
  
  if len(responseString) == 0:
    receivedDict = {}
  else:
    try:
      receivedDict = json.loads(responseString)
    except Exception as e:  
      logLine('Failed loading json: ' + str(e))
      return { 'status' : 1, 'response' : responseString }

  receivedDict['status']   = response.status
  receivedDict['response'] = responseString
  return receivedDict

def jobAborted(httpCode, abortedMethod, rseName):

  jobAbortedDict = { "method"         : "job_aborted",
                     "http_code"      : httpCode,
                     "aborted_method" : abortedMethod,
                     "rse_name"       : rseName
                   }

  sendJsonURL(
    'https://justin-allocator-###prodev###.dune.hep.ac.uk'
    '/api/allocator/job_aborted_'
     + str(httpCode),
    jobAbortedDict)

  time.sleep(10)    
  sys.exit(0)

def executeMetaCatCommand(args):

  for i in range(1, 4):  
    ret = os.system(
     'source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh ; '
     'setup metacat ; '
     'export X509_USER_PROXY=justin-jobs-production.proxy.pem ; '
     'export METACAT_AUTH_SERVER_URL=https://metacat.fnal.gov:8143/auth/dune ; '
     'export METACAT_SERVER_URL=https://metacat.fnal.gov:9443/dune_meta_prod/app ; '
     'metacat auth login -m x509 dunepro ; '
     'metacat %s' % args)
     
    if ret == 0:
      break

    time.sleep(1)

  return ret
  
def executeJustinRucioUpload(args):

  for i in range(1, 4):
    ret = os.system(
     'source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh ; '
     'setup rucio ; setup justin ; '
     'export X509_USER_PROXY=justin-jobs-production.proxy.pem ; '
     'unset GFAL_CONFIG_DIR GFAL_PLUGIN_DIR GFAL2_DIR ; '
     'justin-rucio-upload %s' % args)

    if ret == 0:
      break

    time.sleep(1)

  return ret

#
# Start of WebDAV uploads: this code is shared between justin-webdav-upload 
#Â and justin-wrapper-job to make them both self contained
#

def webdavCheckRemoteDirectories(sslContext, token, dirSplit):

  # Go down through the directories looking for the highest one that exists
  for highest in range(len(dirSplit) - 1, 1, -1): 
  
    # Don't do the check once we get down to the bare hostname 
    # But otherwise do stop if the directory exists already
    if highest > 2 and \
       webdavRemoteExists(sslContext, token, '/'.join(dirSplit[:highest + 1])):
      break
      
  for i in range(highest+1, len(dirSplit)):
    webdavRemoteDirCreate(sslContext, token, '/'.join(dirSplit[:i+1]))

def webdavRemoteExists(sslContext, token, url):

  httpRequest = urllib.request.Request(url,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token
                            },
                  method = 'HEAD')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
    httpCode = response.status 

  except urllib.error.URLError as e:
    httpCode = e.code

  except Exception as e:
    print('Unknown error checking if %s exists: %s' % (url, str(e)), 
          file=sys.stderr)
    raise

  if httpCode == 404:
    print('%s does not exist' % url, file=sys.stderr)
    return False
    
  if httpCode == 200:
    print('%s exists' % url, file=sys.stderr)
    return True

  print('Unexpected HTTP code %d when checking for %s' % (httpCode, url),
        file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % httpCode)
  
def webdavRemoteDirCreate(sslContext, token, url):

  httpRequest = urllib.request.Request(url,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token
                            },
                  method = 'MKCOL')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)    
    httpCode = response.status 

  except urllib.error.URLError as e:
    # For MKCOL, HTTP code 405 means directory already exists 
    # http://www.webdav.org/specs/rfc2518.html#rfc.section.8.3.2.p.4
    # Presumably another job created it ...
    if e.code == 405:
      print('%s already exists now!' % url, file=sys.stderr)
      return

    print('HTTP error: ' + str(e), file=sys.stderr)
    raise
    
  except Exception as e:
    print('Unknown error: ' + str(e), file=sys.stderr)
    raise
  
  if httpCode == 201:
    print('%s created, HTTP code %d' % (url, httpCode), file=sys.stderr)
    return

  print('Unexpected HTTP code %d when creating %s' % (httpCode, url),
        file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % httpCode)
  
def webdavPutFile(token, source, destinationDir):

  destDirSplit = destinationDir.split('/')

  # Remove any final "/" and "//" after the one in "https://"
  for i in range(2, len(destDirSplit)):
    if destDirSplit[i] == '':
      destDirSplit.pop(i)
      
  destinationDir = '/'.join(destDirSplit)

  sslContext = ssl.SSLContext()
  sslContext.verify_mode = ssl.CERT_REQUIRED
  sslContext.check_hostname = True
  sslContext.load_verify_locations(capath = 
                       '/cvmfs/grid.cern.ch/etc/grid-security/certificates')

  # Check the directory tree already exists and create if necessary
  webdavCheckRemoteDirectories(sslContext, token, destDirSplit)

  sourceFile = source.split('/')[-1]
  destinationURL = destinationDir + '/' + sourceFile
    
  if webdavRemoteExists(sslContext, token, destinationURL):
    print('File already exists at %s' % destinationURL, file=sys.stderr)
    raise RuntimeError('File already exists')
  
  # Do a PUT to the desired destination, but expect a redirect
  httpRequest = urllib.request.Request(destinationURL,
                  headers = { 'User-Agent'     : 'justin-webdav-upload',
                              'Authorization'  : 'Bearer ' + token,
                              'Expect'         : '100-continue'
                            },
                  method = 'PUT')

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
    # Success is failure as we want a 307 redirect
    raise RuntimeError('No redirect received')

  except urllib.error.HTTPError as e:

    if e.code == 307:
      # And failure is success
      location = e.headers['Location']
    else:
      print('Unexpected HTTP code %d when creating %s' 
            % (e.code, destinationURL), file=sys.stderr)
      raise
    
  except Exception as e:
    print('Unexpected error when creating %s: %s' 
          % (str(e), destinationURL), file=sys.stderr)
    message = e.read().decode('utf-8')
    print(message, file=sys.stderr)
    raise

  print('Redirected to %s' % location, file=sys.stderr)

  #
  # Now use the URL we got from the redirection
  # 
  httpRequest = urllib.request.Request(location,
                  data = open(source, 'rb'),
                  headers = { 'User-Agent' : 'justin-webdav-upload' },
                  method = 'PUT')

  # Replace .has_header to prevent the default Content-Type 
  # IMO dCache should ignore Content-Type rather than reject the workflow
  httpRequest.has_header \
   = lambda header_name: (header_name == 'Content-type' or
                urllib.request.Request.has_header(httpRequest, header_name))

  try:
    response = urllib.request.urlopen(httpRequest, context = sslContext)
  except Exception as e:
    print('Error when creating %s: %s'
          % (destinationURL, str(e)), file=sys.stderr)
    raise

  if response.status == 201:
    # Created successfully
    print('Created ' + destinationURL)
    return

  print('Unexpected HTTP code %d when creating %s' 
        % (response.status, destinationURL), file=sys.stderr)
  raise RuntimeError('Unexpected HTTP code %d' % response.status)
#
# End of WebDAV uploads code
#

def calculateAdler32(fileName):

  checksum = zlib.adler32(b"")
 
  with open(fileName, "rb") as f:
   block = f.read(16 * 1024)
   while block:
    checksum = zlib.adler32(block, checksum)
    block = f.read(16 * 1024)
    
  return ("%08x" % checksum)

def updateMetadataTmp(fileScope, fileName, 
                      getStageDict, receivedDict, recordResultsDict):
  try:
    metadata = json.load(open('home/workspace/' + fileName + '.json', 'r'))
  except FileNotFoundError:
    metadata = { "metadata" : {} }
  except Exception as e:
    logLine("File home/workspace/%s.json exists but JSON load fails: %s" % 
            (fileName, str(e)))
    raise

  metadata['namespace'] = fileScope
  metadata['name']      = fileName
  metadata['metadata']['dune.workflow'] = {}

  metadata['metadata']['dune.workflow']['site_name'] \
   = getStageDict['site_name'] 
  metadata['metadata']['dune.workflow']['workflow_id'] \
   = receivedDict['workflow_id']
  metadata['metadata']['dune.workflow']['stage_id'] \
   = receivedDict['stage_id']
  metadata['metadata']['dune.workflow']['user'] \
   = receivedDict['principal_name']
  metadata['metadata']['dune.workflow']['hostname'] \
   = getStageDict['hostname']

  metadata['metadata']['dune.workflow']['jobscript_start'] \
   = recordResultsDict['jobscript_start']
  metadata['metadata']['dune.workflow']['jobscript_finish'] \
   = recordResultsDict['jobscript_finish']

  metadata['metadata']['dune.workflow']['jobscript_real_seconds'] \
   = recordResultsDict['jobscript_real_seconds']
  metadata['metadata']['dune.workflow']['jobscript_cpu_seconds'] \
   = recordResultsDict['jobscript_user_seconds'] + \
     recordResultsDict['jobscript_sys_seconds']

  metadata['metadata']['dune.workflow']['cpuinfo'] \
   = getStageDict['cpuinfo']
  metadata['metadata']['dune.workflow']['os_release'] \
   = getStageDict['os_release']
  metadata['metadata']['dune.workflow']['job_id'] \
   = jobsubJobID

  # Force lowercase in top level metadata keys names
  for keyName in metadata['metadata']:
    if keyName != keyName.lower():
      metadata['metadata'][keyName.lower()] = metadata['metadata'].pop(keyName)

  with open('tmp.json', 'w') as f:
    f.write(json.dumps(metadata, indent = 4, sort_keys = True))
  
########################################################################
logLine('====Start of justin-wrapper-job====')

for i in os.environ:
 print('%s=%s' % (i, os.environ[i]))

jobsubJobID = os.environ['JOBSUBJOBID']
siteName    = os.environ.get('GLIDEIN_DUNESite', 'XX_UNKNOWN')
jobPID      = os.getpid()

# Start of subprocess to send regular heartbeats
if os.fork() == 0:
  time.sleep(600)
  # Redirect stdout/stderr
  so = open('heartbeats.log', 'a+')
  os.dup2(so.fileno(), sys.stdout.fileno())
  se = open('heartbeats.log', 'a+')
  os.dup2(se.fileno(), sys.stderr.fileno())
  
  # Send heartbeats until parent exits
  while os.getppid() == jobPID:
    sendJsonURL('https://justin-allocator-###prodev###.dune.hep.ac.uk'
                '/api/allocator/send_heartbeat',
                { "method" : "send_heartbeat" }
               )
    time.sleep(600)
             
  sys.exit(0)    
# End of subprocess to send regular heartbeats

justinWorkdir = os.environ['PWD']

# Make $HOME directory and workspace subdirectory for Apptainer/Singularity
os.makedirs('home/workspace')

# Make jobutils scripts available to jobscripts in a known location
for i in ['justin-get-file', 'justin-allocated-files', 'pdjson2metadata']:
  os.symlink('/cvmfs/dune.opensciencegrid.org/products/dune/justin/'
             '###prodev###/NULL/jobutils/' + i, 'home/' + i)

# Assemble values to send to allocator
getStageDict = { 'method' : 'get_stage_json' }

for line in open('/proc/cpuinfo','r').readlines():
  if line.startswith('model name'):
    getStageDict['cpuinfo'] = line.split(':')[1].strip()
    break

getStageDict['os_release'] \
  = open('/etc/redhat-release','r').readlines()[0].strip()
getStageDict['hostname']    = os.environ.get('HOSTNAME', os.uname()[1])
getStageDict['site_name']   = siteName

getStageDict['site_job_id'] = 'XXX'
##  "site_job_id"     : "${JOB_GLIDEIN_SiteWMS_JobId:-unknown}",

for line in open(os.environ['_CONDOR_JOB_AD'],'r').readlines():
  if line.startswith('RequestCpus = '):
    getStageDict['processors'] = int(line.split('=')[1].strip())

  if line.startswith('RequestMemory = '):
    getStageDict['rss_bytes'] = 1048576 * int(line.split('=')[1].strip())

for line in open(os.environ['_CONDOR_MACHINE_AD'],'r').readlines():
  if line.startswith('GLIDEIN_Max_Walltime = '):
    getStageDict['wall_seconds'] = int(line.split('=')[1].strip())
    break

logLine('==== Before try apptainer ====')
getStageDict['has_inner_apptainer'] = (0 == os.system(
   '/cvmfs/oasis.opensciencegrid.org/mis/apptainer/current/bin/apptainer '
   'shell --shell /usr/bin/hostname '
   '/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:osg3.6'
   ))
logLine('==== After try apptainer ====')

# Make no roles CSR 
os.system('openssl req -batch -nodes -newkey rsa:2048 '
          '-keyout justin-jobs-no-roles.key.pem '
          '-out justin-jobs-no-roles.csr.pem')

getStageDict['csr-no-roles'] = \
  open('justin-jobs-no-roles.csr.pem', 'r').read()

justinJobsNoRolesKeyPem = \
  open('justin-jobs-no-roles.key.pem', 'r').read()

# Make production CSR
os.system('openssl req -batch -nodes -newkey rsa:2048 '
          '-keyout justin-jobs-production.key.pem '
          '-out justin-jobs-production.csr.pem')

getStageDict['csr-production'] = \
  open('justin-jobs-production.csr.pem', 'r').read()

justinJobsProductionKeyPem = \
  open('justin-jobs-production.key.pem', 'r').read()

for i in range(1,6):

  # Sleep for up to 60 seconds to spread out job start storms
  logLine('Random sleep ...')
##REINSTATE THIS AFTER TESTING!!!!
  ##time.sleep(random.randrange(61))

  logLine('====start justin-get-stage.json====')
  print(json.dumps(getStageDict))
  logLine('====end justin-get-stage.json====')

  receivedDict = sendJsonURL(
    'https://justin-allocator-###prodev###.dune.hep.ac.uk'
    '/api/allocator/get_stage',
    getStageDict)

  logLine('get_stage returns HTTP code %d' % receivedDict['status'])

  if receivedDict['status'] != 503 and receivedDict['status'] != 0:
    break
    
if receivedDict['status'] != 200:
  logLine('Exiting due to code %d. Received: \n%s'
          % (receivedDict['status'], receivedDict['response']))
  time.sleep(10)
  sys.exit(0)

# Stage-defined and standard environment variables for the jobscript
with open('home/justin-jobscript-env.sh','w') as f:
  for (n,v) in receivedDict['stage_env']:
    f.write('export %s="%s"\n' % (n,v))

# REMOVE THIS
  f.write('export JUSTIN_PRO_DEV=###prodev###\n')
# ^^^^^^^^^^^

  f.write('export JUSTIN_ALLOCATOR='
   'https://justin-allocator-###prodev###.dune.hep.ac.uk/api/allocator/\n')
  f.write('export JUSTIN_SITE_NAME=%s\n' % receivedDict['site_name'])
  f.write('export JUSTIN_WORKFLOW_ID=%d\n' % receivedDict['workflow_id'])
  f.write('export JUSTIN_STAGE_ID=%d\n' % receivedDict['stage_id'])
  f.write('export JUSTIN_SCOPE=%s\n' % receivedDict['scope'])
  f.write('export JUSTIN_JOBSUB_ID=%s\n' % receivedDict['jobsub_id'])
  f.write('unset JOBSUBJOBID\n')
  f.write('export JUSTIN_JOBSCRIPT_SECRET=%s\n' 
          % receivedDict['jobscript_secret'])
  f.write('export JUSTIN_SAM_WEB_URI='
          '"https://justin.dune.hep.ac.uk/api/samweb/%s/%s"\n' 
          % (receivedDict['jobsub_id'], receivedDict['jobscript_secret']))

# JSON for justin-get-file command to use
with open('home/justin-get-file.json', 'w') as f:
  f.write(json.dumps({ 'method' : 'get_file',
                       'jobsub_id' : receivedDict['jobsub_id'],
                       'jobscript_secret' : receivedDict['jobscript_secret']
                     }
                    )
         )

# JSON for justin-allocated-files command to use
with open('home/justin-allocated-files.json', 'w') as f:
  f.write(json.dumps({ 'method' : 'get_allocated_files',
                       'jobsub_id' : receivedDict['jobsub_id'],
                       'jobscript_secret' : receivedDict['jobscript_secret']
                     }
                    )
         )

# The jobscript in any scripting language supported by the container
with open('home/justin-jobscript','wb') as f:
  f.write(receivedDict['jobscript'].encode())

os.chmod('home/justin-jobscript', 0o755)

# Assemble proxy to be used by jobscript
with open('home/justin-jobs-no-roles.proxy.pem', 'w') as f:
  f.write(receivedDict['justin-jobs-no-roles.cert.pem'])
  f.write(justinJobsNoRolesKeyPem)
  f.write(receivedDict['justin-jobs-no-roles.chain.pem'])

os.chmod('home/justin-jobs-no-roles.proxy.pem', 0o400)

# Assemble proxy to be used by wrapper job itself
with open('justin-jobs-production.proxy.pem', 'w') as f:
  f.write(receivedDict['justin-jobs-production.cert.pem'])
  f.write(justinJobsProductionKeyPem)
  f.write(receivedDict['justin-jobs-production.chain.pem'])

os.chmod('justin-jobs-production.proxy.pem', 0o400)

# AWT jobs get production proxy too
if receivedDict['for_awt']:

  with open('home/justin-awt-rse-list.txt', 'w') as f:
    for (rseName, scheme, pfn) in receivedDict['awt_rses']:
      f.write('%s %s %s\n' % (rseName, scheme, pfn))

  with open('home/awt-proxy.pem', 'w') as f:
    f.write(receivedDict['justin-jobs-production.cert.pem'])
    f.write(justinJobsProductionKeyPem)
    f.write(receivedDict['justin-jobs-production.chain.pem'])

  os.chmod('home/awt-proxy.pem', 0o400)

if getStageDict['has_inner_apptainer']:
  # Wrapper to be run inside the container
  with open('home/jobscript-wrapper.sh','w') as f:
    f.write('''#!/bin/sh
export JUSTIN_PATH="$HOME"
export X509_USER_PROXY="$HOME/justin-jobs-no-roles.proxy.pem"
cd workspace
. ../justin-jobscript-env.sh
stdbuf -oL -eL ../justin-jobscript >jobscript.log 2>&1
''')

  os.chmod('home/jobscript-wrapper.sh', 0o755)

  logLine('Inner apptainer available. Run jobscript inside container')
  logLine('====Start of jobscript execution====')
  jobscriptStartTime=int(time.time())

  jobscriptOutcome = subprocess.run(
  '/usr/bin/time -o time.txt -f "%%e %%U %%S %%M" '
  '/cvmfs/oasis.opensciencegrid.org/mis/apptainer/current/bin/apptainer shell '
  '--shell /home/jobscript-wrapper.sh '
  '--containall '
  '--bind /cvmfs '
  '--workdir %s '
  '--home %s/home:/home '
  '/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:osg3.6 '
  % (justinWorkdir, justinWorkdir),
  stdout=subprocess.PIPE,
  shell=True 
  )
  
else:
  # Wrapper to be run outside a container
  with open('home/jobscript-wrapper.sh','w') as f:
    f.write('''#!/bin/sh
cd home
export HOME="$PWD"
export JUSTIN_PATH="$HOME"
export X509_USER_PROXY="$HOME/justin-jobs-no-roles.proxy.pem"
cd workspace
. ../justin-jobscript-env.sh
stdbuf -oL -eL ../justin-jobscript >jobscript.log 2>&1
''')

  os.chmod('home/jobscript-wrapper.sh', 0o755)

  logLine('Inner apptainer not available. Run jobscript in wrapper job context')
  logLine('====Start of jobscript execution====')
  jobscriptStartTime=int(time.time())

  jobscriptOutcome = subprocess.run('/usr/bin/time -o time.txt '
                                    '-f "%%e %%U %%S %%M"'
                                    'home/jobscript-wrapper.sh ',
                                    stdout=subprocess.PIPE,
                                    shell=True 
                                   )

jobscriptFinishTime=int(time.time())
logLine('====End of jobscript execution====')

logLine('#### start subprocess log ####')
print(jobscriptOutcome.stdout)
logLine('#### end subprocess log ####')

try:
  jobscriptLog = open('home/workspace/jobscript.log','r').read()
except:
  jobscriptLog = ''

logLine('#### start jobscript log ####')
print(jobscriptLog)
logLine('#### end jobscript log ####')

try:
  (jobscriptRealSeconds,
   jobscriptUserSeconds,
   jobscriptSysSeconds,
   jobscriptMaxRssKB) = open('time.txt','r').read().split()
except:
  jobscriptRealSeconds = 0
  jobscriptUserSeconds = 0
  jobscriptSysSeconds  = 0
  jobscriptMaxRssKB    = 0

justinRecordResultsDict = {
  'method'                 : 'record_results',
  'output_urls'            : [],
  'output_dids'            : [],
  'next_stage_dids'        : [],
  'jobscript_log'          : jobscriptLog[-10000:],
  'jobscript_exit'         : jobscriptOutcome.returncode,
  'jobscript_start'        : jobscriptStartTime,
  'jobscript_finish'       : jobscriptFinishTime,
  'jobscript_real_seconds' : int(float(jobscriptRealSeconds)),
  'jobscript_user_seconds' : int(float(jobscriptUserSeconds)),
  'jobscript_sys_seconds'  : int(float(jobscriptSysSeconds)),
  'jobscript_max_rss_kb'   : int(float(jobscriptMaxRssKB))
}

outputFiles = []

# Find files matching output patterns specified for this stage
for (forNextStage, destination, scope, pattern) in \
    receivedDict['patterns']:
  
  matches = glob.glob('home/workspace/' + pattern)
  for match in matches:
    matchingFile = match.split('/')[-1]
    outputFiles.append((destination, scope, matchingFile))
    
    if scope == '::URL::':
      justinRecordResultsDict['output_urls'].append(destination + '/' +
                                                        matchingFile)
    elif forNextStage:
      justinRecordResultsDict['next_stage_dids'].append(scope + ':' + 
                                                        matchingFile)    
    else:
      justinRecordResultsDict['output_dids'].append(scope + ':' + matchingFile)

justinRecordResultsDict['processed_dids'] = []
try:
  for i in open('home/workspace/justin-processed-dids.txt','r').readlines():
    justinRecordResultsDict['processed_dids'].append(i.strip())
except:
  pass

justinRecordResultsDict['processed_pfns'] = []
try:
  for i in open('home/workspace/justin-processed-pfns.txt','r').readlines():
    justinRecordResultsDict['processed_pfns'].append(i.strip())
except:
  pass

logLine('record results: ' + str(justinRecordResultsDict))
resultsResponseDict = sendJsonURL(
 'https://justin-allocator-###prodev###.dune.hep.ac.uk'
 '/api/allocator/record_results', 
 justinRecordResultsDict)

logLine('record_results returns HTTP code %d' % resultsResponseDict['status'])

if resultsResponseDict['status'] != 200:
  jobAborted(resultsResponseDict['status'], 'record_results', '')

# Create a logs.tgz file and upload with rucio
# At the very least jobscript.log will be there
tgzName = jobsubJobID.replace('@','-') + '.logs.tgz'
try:
  ret = os.system('cd home/workspace ; tar zcf ../../%s *.log' % tgzName)
except Exception as e:
  logLine('tar zcf %s *.log fails %s' % (tgzName, str(e)))
  ret = 1
  
if ret:
  jobAborted(900, 'create_logs_tgz', '')

for (rse,scheme) in receivedDict['output_rses'][:3]:
  logLine('Try justin-logs:%s to %s/%s' % (tgzName, rse, scheme))

  ret = executeJustinRucioUpload('--rse %s '
                                 '--protocol %s '
                                 '--scope justin-logs '
                                 '--dataset workflow_%d '
                                 '--timeout 1200 '
                                 '%s' 
                                 % (rse, 
                                    scheme, 
                                    receivedDict['workflow_id'], 
                                    tgzName))
  if ret == 0:
    break
  else:
    logLine('Failed to upload justin-logs:%s to %s' % (tgzName, rse))

if ret:
  logLine('Failed to upload justin-logs:%s' % tgzName)
  jobAborted(900, 'rucio_upload_logs_tgz', '')

# No other uploads if the jobscript returned an error 
if jobscriptOutcome.returncode != 0:
  jobAborted(900, 'jobscript_error', '')

# Now try to upload the output files we recorded as created by the job
logLine('Output RSEs: ' + str(receivedDict['output_rses']))

# TO DO: DROP THIS FILE WRITE?
# Write out the token for uploading to user's scratch
#with open('user-upload-token', 'w') as f:
#  f.write(resultsResponseDict['user_access_token'])

confirmResultsDict = { 'method'      : 'confirm_results',
                       'output_dids' : {},
                       'output_urls' : []  }

# Go through the list of output files
for (destination, fileScope, fileName) in outputFiles:
  
  if fileScope == '::URL::': 
    # Uploading to user scratch

    try:
      webdavPutFile(resultsResponseDict['user_access_token'], 
                    'home/workspace/' + fileName,
                    destination)
    except:
      jobAborted(900, 'webdav_upload', '')
    
    confirmResultsDict['output_urls'].append(destination + '/' + fileName)
  elif receivedDict['output_rses']:
    # Uploading to Rucio managed storage
    
    # Create tmp.json metadata file for this output file
    try:
      updateMetadataTmp(fileScope, fileName, 
                        getStageDict, receivedDict, justinRecordResultsDict)

      fileAdler32 = calculateAdler32('home/workspace/' + fileName)
      fileSize    = os.path.getsize('home/workspace/' + fileName)
    except Exception as e:
      logLine('updateMetadataTmp() fails ' + str(e))
      jobAborted(900, 'update_metadata', '')

    # First try to register with MetaCat
    ret = executeMetaCatCommand('file declare --json -f tmp.json '
                                '-s %d -c "adler32:%s" '
                                '"%s:%s"' 
                                  % (fileSize, fileAdler32,
                                     'dune', 'all'
         # we now use the above hard-coded dataset in MetaCat for everything :-(
                                     # fileScope, destination
                                     ))

#    # IF DEPLOYED METACAT SERVER DOES NOT NOW MATCH CVMFS VERSION !!!!!
#    ret = 0
#    print(open('tmp.json','r').read())
#    # END CLOWN WORLD STUFF

    if ret:
      logLine('Failed to register %s:%s in MetaCat' % (fileScope, fileName))
      jobAborted(900, 'metacat_registration', '')

    # If that succeeds, then try to register/upload with rucio    
    for (rse,scheme) in receivedDict['output_rses'][:3]:
      logLine('Try %s:%s to %s/%s' 
              % (fileScope, fileName, rse, scheme))

      ret = executeJustinRucioUpload('--rse %s '
                                     '--protocol %s '
                                     '--scope %s '
                                     '--dataset %s '
                                     '--timeout 1200 '
                                     'home/workspace/%s' %
                                     (rse,
                                      scheme,
                                      fileScope,
                                      destination,
                                      fileName))
      if ret == 0:
        break
      else:
        logLine('Failed to upload %s:%s to %s' % (fileScope, fileName, rse))

    if ret:
      logLine('Failed to upload %s:%s' % (fileScope, fileName))
      jobAborted(900, 'rucio_upload', '')

    # Add to list of uploaded files for confirm results
    confirmResultsDict['output_dids'][fileScope + ':' + fileName] = rse

# If all ok, then confirm that to the Workflow Allocator

logLine('confirm results: ' + str(confirmResultsDict))
confirmDict = sendJsonURL(
  'https://justin-allocator-###prodev###.dune.hep.ac.uk'
  '/api/allocator/confirm_results',
  confirmResultsDict)

logLine('confirm_results returns HTTP code %d' % confirmDict['status'])

if confirmDict['status'] != 200:
  jobAborted(900, 'confirm_results', '')

logLine('====End of justin-wrapper-job====')
time.sleep(10)
sys.exit(0)
########################################################################
