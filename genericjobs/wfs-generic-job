#!/bin/sh
#
# Generic Job script which will get a stage bootstrap script from the
# Workflow Allocator
#

echo '====Start of wfs-generic-job===='

echo '### date ; pwd ; printenv'
date
pwd
printenv

# Used by bootstrap script to find files from this generic job
export WFS_PATH=`pwd`

# Create the wfs-get-file command
base64 -d <<EOF > $WFS_PATH/wfs-get-file
IyEvYmluL3NoCiMKIyBTY3JpcHQgZm9yIHVzZSB3aXRoaW4gYm9vdHN0cmFwIHNjcmlwdCB0byBn
ZXQgdGhlIERJRCwgUEZOLCBhbmQgUlNFCiMgb2YgYSBmaWxlIHRvIHByb2Nlc3Mgd2l0aGluIHRo
ZSBzdGFnZSBhc3NpZ25lZCB0byB0aGUgam9iLgojCiMgQm9vdHN0cmFwIHNjcmlwdHMgY2FuIGV4
ZWN1dGUgdGhpcyBzY3JpcHQgYXM6ICRXRlNfUEFUSC93ZnMtZ2V0LWZpbGUKIwojIEVycm9yIG1l
c3NhZ2VzIHRvIHN0ZGVycgojIERJRCBQRk4gUlNFIHRvIHN0ZG91dCBvbiBvbmUgbGluZSBpZiBh
IGZpbGUgaXMgYXZhaWxhYmxlCiMKIyBUaGlzIHNjcmlwdCBtdXN0IGJlIGNvbnZlcnRlZCB0byBi
YXNlNjQgd2l0aCBzb21ldGhpbmcgbGlrZSB0aGUgZm9sbG93aW5nIAojIGFuZCBpbmNsdWRlZCBp
biB0aGUgaGVyZSBkb2N1bWVudCBuZWFyIHRoZSBzdGFydCBvZiB3ZnMtZ2VuZXJpYy1qb2IgOgoj
CiMgKG1hY09TKSBiYXNlNjQgLWIgNzYgd2ZzLWdldC1maWxlID4gd2ZzLWdldC1maWxlLmI2NAoj
IChMaW51eCkgYmFzZTY0ICAgICAgIHdmcy1nZXQtZmlsZSA+IHdmcy1nZXQtZmlsZS5iNjQKCmlm
IFsgISAtciAiJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIiBdIDsgdGhlbgogIGVjaG8gIiRX
RlNfUEFUSC93ZnMtZ2V0LWZpbGUuanNvbiBub3QgZm91bmQhIiA+JjIKICBleGl0IDIKZmkKCkdF
VF9GSUxFX1RNUD1gbWt0ZW1wIC90bXAvd2ZzX2dldF9maWxlX1hYWFhYWGAKCmh0dHBfY29kZT1g
Y3VybCBcCi0tc2lsZW50IFwKLS1oZWFkZXIgIlgtSm9iaWQ6ICRKT0JTVUJKT0JJRCIgXAotLWhl
YWRlciAiQWNjZXB0OiB0ZXh0L3BsYWluIiBcCi0tY2FwYXRoICR7WDUwOV9DRVJUSUZJQ0FURVM6
LS9ldGMvZ3JpZC1zZWN1cml0eS9jZXJ0aWZpY2F0ZXMvfSBcCi0tZGF0YSBAJFdGU19QQVRIL3dm
cy1nZXQtZmlsZS5qc29uIFwKLS1vdXRwdXQgJEdFVF9GSUxFX1RNUCBcCi0td3JpdGUtb3V0ICIl
e2h0dHBfY29kZX1cbiIgXApodHRwczovL3dmcy5kdW5lLmhlcC5hYy51ay93ZmEtY2dpIDI+L2Rl
di9udWxsYAoKaWYgWyAiJGh0dHBfY29kZSIgPSAyMDAgXSA7IHRoZW4KIGNhdCAkR0VUX0ZJTEVf
VE1QCiByZXRjb2RlPTAKZWxpZiBbICIkaHR0cF9jb2RlIiA9IDQwNCBdIDsgdGhlbiAKIGVjaG8g
Ik5vIGZpbGVzIGF2YWlsYWJsZSBmcm9tIHRoaXMgc3RhZ2UiID4mMgogcmV0Y29kZT0xCmVsc2UK
IGVjaG8gImdldF9maWxlIHJlY2VpdmVzOiIgPiYyCiBjYXQgJEdFVF9GSUxFX1RNUCA+JjIKIGVj
aG8gImdldC1maWxlIGZhaWxzIHdpdGggSFRUUCBjb2RlICRodHRwX2NvZGUgZnJvbSBhbGxvY2F0
b3IhIiA+JjIKIHJldGNvZGU9MwpmaQoKcm0gLWYgJEdFVF9GSUxFX1RNUApleGl0ICRyZXRjb2Rl
Cg==
EOF
chmod +x $WFS_PATH/wfs-get-file

# Assemble values we will need 
export site_name=${GLIDEIN_DUNESite:-XX_UNKNOWN}
export cpuinfo=`grep '^model name' /proc/cpuinfo | head -1 | cut -c14-`
export os_release=`head -1 /etc/redhat-release`
export hostname=`hostname`

echo "### $_CONDOR_JOB_AD"
cat $_CONDOR_JOB_AD

export processors=`grep '^RequestCpus = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_mb=`grep '^RequestMemory = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_bytes=`expr $rss_mb \* 1048576`
export wall_seconds=`grep '^GLIDEIN_Max_Walltime = ' $_CONDOR_MACHINE_AD | cut -d' ' -f3`

# Check requirements are present

if [ ! -r "$X509_USER_PROXY" ] ; then
 echo "Cannot read X509_USER_PROXY file = $X509_USER_PROXY"
 exit
fi

curl --version
if [ $? -ne 0 ] ; then
 echo Failed running curl
 exit
fi

# Create the JSON to send to the allocator
cat <<EOF >wfs-get-stage.json
{
  "method"       : "get_stage",
  "jobsub_id"    : "$JOBSUBJOBID",
  "site_name"    : "$site_name",
  "cpuinfo"      : "$cpuinfo",
  "os_release"   : "$os_release",
  "hostname"     : "$hostname",
  "rss_bytes"    : $rss_bytes,
  "processors"   : $processors,
  "wall_seconds" : $wall_seconds
}
EOF

echo '====start wfs-get-stage.json===='
cat wfs-get-stage.json
echo '====end wfs-get-stage.json===='

# Make the call to the Workflow Allocator
http_code=`curl \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath ${X509_CERTIFICATES:-/etc/grid-security/certificates/} \
--data @wfs-get-stage.json \
--output wfs-files.tar \
--write-out "%{http_code}\n" \
https://wfs.dune.hep.ac.uk/wfa-cgi`

echo curl returns HTTP code $http_code

if [ "$http_code" != "200" ] ; then
  echo "curl call to WFA fails with code $http_code"
  cat wfs-files.tar
  exit 1
fi

tar xvf wfs-files.tar

if [ -r wfs-env.sh ] ; then
  . ./wfs-env.sh
fi

echo '====Start wfs-get-file.json===='
cat wfs-get-file.json
echo '====End wfs-get-file.jsonh===='

# Run the bootstrap script
if [ -f wfs-bootstrap.sh ] ; then
  chmod +x wfs-bootstrap.sh

  echo '====Start wfs-bootstrap.sh===='
  cat wfs-bootstrap.sh
  echo '====End wfs-bootstrap.sh===='

  mkdir workspace
  echo '====Run wfs-bootstrap.sh===='
  ( cd workspace ; $WFS_PATH/wfs-bootstrap.sh ) >wfs-bootstrap.log 2>&1
  retval=$?
  echo '====After wfs-bootstrap.sh===='
else
  # How can this happen???
  echo No wfs-bootstrap.sh found
  exit 1
fi

echo '#### wfs-bootstrap.log'
cat wfs-bootstrap.log
echo '####'

# Make the lists of output files and files for the next stage
echo -n > wfs-outputs.txt
echo -n > wfs-next-stage-outputs.txt

cat wfs-output-patterns.txt | (
while read for_next_stage pattern
do  
  (
    cd workspace
    # $pattern is wildcard-expanded here - so a list of files
    for fn in $pattern
    do
      if [ -r "$fn" ] ; then
        echo "$fn" >> $WFS_PATH/wfs-outputs.txt
        if [ "$for_next_stage" = "True" ] ; then
          echo "$fn" >> $WFS_PATH/wfs-next-stage-outputs.txt    
        fi
      fi
    done
  )
done
)

next_stage_outputs=`echo \`sed 's/.*/"&"/' wfs-next-stage-outputs.txt\`|sed 's/ /,/g'`

# Just try the first RSE for now
rse=`echo $rse_list | cut -f1 -d' '`

for fn in `cat wfs-outputs.txt`
do
  echo "Would do rucio upload of $fn to $rse"
  echo "Metadata too? $fn.json"
  echo
done

# wfs-bootstrap.sh should produce a list of successfully processed input files
# and a list of files which still need to be processed by another job
if [ -f workspace/wfs-processed-inputs.txt ] ; then
  processed_inputs=`echo \`sed 's/.*/"&"/' workspace/wfs-processed-inputs.txt\`|sed 's/ /,/g'`
fi

if [ -f workspace/wfs-unprocessed-inputs.txt ] ; then
  unprocessed_inputs=`echo \`sed 's/.*/"&"/' workspace/wfs-unprocessed-inputs.txt\`|sed 's/ /,/g'`
fi

cat <<EOF >wfs-return-results.json
{
  "method": "return_results",
  "jobsub_id": "$JOBSUBJOBID",
  "cookie": "$WFS_COOKIE",
  "processed_inputs": [$processed_inputs],
  "unprocessed_inputs": [$unprocessed_inputs],
  "next_stage_outputs": [$next_stage_outputs],
  "bootstrap_log": "
EOF

tail -c 10000 wfs-bootstrap.log | sed 's/"/\\\"/g' >>wfs-return-results.json
  
echo '"}' >>wfs-return-results.json

echo "=====Start wfs-return-results.json=="
cat wfs-return-results.json
echo "=====End wfs-return-results.json=="

http_code=`curl \
--header "X-Jobid: $JOBSUBJOBID" \
--capath ${X509_CERTIFICATES:-/etc/grid-security/certificates/} \
--data @wfs-return-results.json \
--output return-results.txt \
--write-out "%{http_code}\n" \
https://wfs.dune.hep.ac.uk/wfa-cgi`

echo "return_results returns HTTP code $http_code"
cat return-results.txt

echo '====End of wfs-generic-job===='
exit 0

