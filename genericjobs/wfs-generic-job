#!/bin/bash
#
# Generic Job script which will get a stage's bootstrap script from the
# Workflow Allocator
#

# Everything goes to stdout
exec 2>&1

function log_line() {
echo `date -u +'%Y-%m-%d %H:%M:%S '`$1
}

log_line '====Start of wfs-generic-job===='

function job_aborted() {
  cat <<EOF >wfs-job-aborted.json
{
  "method"         : "job_aborted",
  "jobsub_id"      : "$JOBSUBJOBID",
  "http_code"      : $1,
  "aborted_method" : "$2"
}
EOF

  curl \
     --user-agent 'wfs-generic-job' \
     --header "X-Jobid: $JOBSUBJOBID" \
     --key $X509_USER_PROXY \
     --cert $X509_USER_PROXY \
     --cacert $X509_USER_PROXY \
     --capath $X509_CERTIFICATES \
     --data @wfs-job-aborted.json \
     https://wfs-pro.dune.hep.ac.uk/wfa-cgi/job_aborted_$1

  exit 1
}

#echo '### date ; pwd ; printenv'
#date
#pwd
#printenv
#echo "### $_CONDOR_JOB_AD"
#cat $_CONDOR_JOB_AD

echo '### ls -lt ###'
ls -lt
echo '##############'

# Used by bootstrap script to find files from this generic job
export WFS_PATH=`pwd`

# Create the wfs-get-file command
base64 -d <<EOF > $WFS_PATH/wfs-get-file
IyEvYmluL3NoCiMKIyBTY3JpcHQgZm9yIHVzZSB3aXRoaW4gYm9vdHN0cmFwIHNjcmlwdCB0byBn
ZXQgdGhlIERJRCwgUEZOLCBhbmQgUlNFCiMgb2YgYSBmaWxlIHRvIHByb2Nlc3Mgd2l0aGluIHRo
ZSBzdGFnZSBhc3NpZ25lZCB0byB0aGUgam9iLgojCiMgQm9vdHN0cmFwIHNjcmlwdHMgY2FuIGV4
ZWN1dGUgdGhpcyBzY3JpcHQgYXM6ICRXRlNfUEFUSC93ZnMtZ2V0LWZpbGUKIwojIEVycm9yIG1l
c3NhZ2VzIHRvIHN0ZGVycgojIERJRCBQRk4gUlNFIHRvIHN0ZG91dCBvbiBvbmUgbGluZSBpZiBh
IGZpbGUgaXMgYXZhaWxhYmxlCiMKIyBUaGlzIHNjcmlwdCBtdXN0IGJlIGNvbnZlcnRlZCB0byBi
YXNlNjQgd2l0aCBzb21ldGhpbmcgbGlrZSB0aGUgZm9sbG93aW5nIAojIGFuZCBpbmNsdWRlZCBp
biB0aGUgaGVyZSBkb2N1bWVudCBuZWFyIHRoZSBzdGFydCBvZiB3ZnMtZ2VuZXJpYy1qb2IgOgoj
CiMgKG1hY09TKSBiYXNlNjQgLWIgNzYgd2ZzLWdldC1maWxlID4gd2ZzLWdldC1maWxlLmI2NAoj
IChMaW51eCkgYmFzZTY0ICAgICAgIHdmcy1nZXQtZmlsZSA+IHdmcy1nZXQtZmlsZS5iNjQKCmlm
IFsgISAtciAiJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIiBdIDsgdGhlbgogIGVjaG8gIiRX
RlNfUEFUSC93ZnMtZ2V0LWZpbGUuanNvbiBub3QgZm91bmQhIiA+JjIKICBleGl0IDIKZmkKCkdF
VF9GSUxFX1RNUD1gbWt0ZW1wIC90bXAvd2ZzX2dldF9maWxlX1hYWFhYWGAKCmh0dHBfY29kZT1g
Y3VybCBcCi0tdXNlci1hZ2VudCAnd2ZzLWdldC1maWxlJyBcCi0tc2lsZW50IFwKLS1oZWFkZXIg
IlgtSm9iaWQ6ICRKT0JTVUJKT0JJRCIgXAotLWhlYWRlciAiQWNjZXB0OiB0ZXh0L3BsYWluIiBc
Ci0tY2FwYXRoICR7WDUwOV9DRVJUSUZJQ0FURVM6LS9ldGMvZ3JpZC1zZWN1cml0eS9jZXJ0aWZp
Y2F0ZXMvfSBcCi0tZGF0YSBAJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIFwKLS1vdXRwdXQg
JEdFVF9GSUxFX1RNUCBcCi0td3JpdGUtb3V0ICIle2h0dHBfY29kZX1cbiIgXApodHRwczovL3dm
cy1wcm8uZHVuZS5oZXAuYWMudWsvd2ZhLWNnaS9nZXRfZmlsZSAyPi9kZXYvbnVsbGAKCmlmIFsg
IiRodHRwX2NvZGUiID0gMjAwIF0gOyB0aGVuCiBjYXQgJEdFVF9GSUxFX1RNUAogcmV0Y29kZT0w
CmVsaWYgWyAiJGh0dHBfY29kZSIgPSA0MDQgXSA7IHRoZW4gCiBlY2hvICJObyBmaWxlcyBhdmFp
bGFibGUgZnJvbSB0aGlzIHN0YWdlIiA+JjIKIHJldGNvZGU9MQplbHNlCiBlY2hvICJnZXRfZmls
ZSByZWNlaXZlczoiID4mMgogY2F0ICRHRVRfRklMRV9UTVAgPiYyCiBlY2hvICJnZXQtZmlsZSBm
YWlscyB3aXRoIEhUVFAgY29kZSAkaHR0cF9jb2RlIGZyb20gYWxsb2NhdG9yISIgPiYyCiByZXRj
b2RlPTMKZmkKCnJtIC1mICRHRVRfRklMRV9UTVAKZXhpdCAkcmV0Y29kZQo=
EOF
chmod +x $WFS_PATH/wfs-get-file

# Assemble values to record
export site_name=${GLIDEIN_DUNESite:-XX_UNKNOWN}
export cpuinfo=`grep '^model name' /proc/cpuinfo | head -1 | cut -c14-`
export os_release=`head -1 /etc/redhat-release`
export hostname=`hostname`

export processors=`grep '^RequestCpus = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_mb=`grep '^RequestMemory = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_bytes=`expr $rss_mb \* 1048576`
export wall_seconds=`grep '^GLIDEIN_Max_Walltime = ' $_CONDOR_MACHINE_AD | cut -d' ' -f3`

export X509_CERTIFICATES=${X509_CERTIFICATES:-/etc/grid-security/certificates/}

# Check requirements are present

if [ ! -r "$X509_USER_PROXY" ] ; then
 # Stop if proxy file is missing or deleted
 log_line "Cannot read X509_USER_PROXY file = $X509_USER_PROXY"
 exit 1
fi

curl --version
if [ $? -ne 0 ] ; then
 log_line Failed running curl
 exit 1
fi

cat <<EOF >wfs-send-heartbeat.json
{
  "method"       : "send_heartbeat",
  "jobsub_id"    : "$JOBSUBJOBID"
}
EOF

(
# Subprocess to send regular heartbeats

while :
do
  # Stop if JSON file is missing or deleted
  if [ ! -r wfs-send-heartbeat.json ] ; then
    exit 1
  fi

  curl \
     --user-agent 'wfs-generic-job' \
     --header "X-Jobid: $JOBSUBJOBID" \
     --key $X509_USER_PROXY \
     --cert $X509_USER_PROXY \
     --cacert $X509_USER_PROXY \
     --capath $X509_CERTIFICATES \
     --data @wfs-send-heartbeat.json \
     https://wfs-pro.dune.hep.ac.uk/wfa-cgi/send_heartbeat

  sleep 600
done

) >wfs-heartbeat.log 2>&1 &

# Create the JSON to send to the allocator
cat <<EOF >wfs-get-stage.json
{
  "method"       : "get_stage",
  "jobsub_id"    : "$JOBSUBJOBID",
  "site_name"    : "${site_name:-XX-UNKNOWN}",
  "cpuinfo"      : "${cpuinfo:-Unknown}",
  "os_release"   : "${os_release:-Unknown}",
  "hostname"     : "${hostname:-unknown}",
  "rss_bytes"    : ${rss_bytes:-0},
  "processors"   : ${processors:-0},
  "wall_seconds" : ${wall_seconds:-0}
}
EOF

echo '====start wfs-get-stage.json===='
cat wfs-get-stage.json
echo '====end wfs-get-stage.json===='

# Make the call to the Workflow Allocator
http_code=`curl \
--user-agent 'wfs-generic-job' \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath $X509_CERTIFICATES \
--data @wfs-get-stage.json \
--output wfs-files.tar \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi/get_stage`

log_line curl returns HTTP code $http_code

if [ "$http_code" != "200" ] ; then
  log_line "curl call to WFA fails with code $http_code"
  cat wfs-files.tar
  job_aborted $http_code get_stage
fi

tar xvf wfs-files.tar

if [ -r wfs-env.sh ] ; then
  . ./wfs-env.sh
fi

echo '====Start wfs-output-patterns.txt===='
cat wfs-output-patterns.txt
echo '====End wfs-output-patterns.txt===='

echo '====Start wfs-get-file.json===='
cat wfs-get-file.json
echo '====End wfs-get-file.json===='

# Run the bootstrap script
if [ -f wfs-bootstrap.sh ] ; then
  chmod +x wfs-bootstrap.sh

  echo '====Start wfs-bootstrap.sh===='
  cat wfs-bootstrap.sh
  echo '====End wfs-bootstrap.sh===='

  mkdir workspace
  echo '====Run wfs-bootstrap.sh===='
  ( cd workspace ; stdbuf -oL -eL $WFS_PATH/wfs-bootstrap.sh ) > workspace/wfs-bootstrap.log 2>&1
  retval=$?
  echo '====After wfs-bootstrap.sh===='
else
  # How can this happen???
  log_line No wfs-bootstrap.sh found
  exit 1
fi

echo '#### wfs-bootstrap.log'
cat workspace/wfs-bootstrap.log
echo '####'

# Make the lists of output files and files for the next stage
echo -n > wfs-outputs.txt
echo -n > wfs-output-dids.txt
echo -n > wfs-next-stage-dids.txt

echo '### Contents of workspace directory ###'
ls -lR workspace
echo '###'

cat wfs-output-patterns.txt | (
while read for_next_stage dataset scope pattern
do  
  (
    cd workspace
    # $pattern is wildcard-expanded here - so a list of files
    for fn in $pattern
    do
      if [ -r "$fn" ] ; then
        # wfs-outputs.txt gets DATASET SCOPE FILENAME on each line
        echo "$dataset $scope $fn" >> $WFS_PATH/wfs-outputs.txt

        if [ "$for_next_stage" = "True" ] ; then
          # wfs-next-stage-dids gets a list of DIDs
          echo "$scope:$fn" >> $WFS_PATH/wfs-next-stage-dids.txt
        else
          # wfs-output-dids.txt gets DID = SCOPE:FILENAME
          echo "$scope:$fn" >> $WFS_PATH/wfs-output-dids.txt
        fi
      fi
    done
  )
done
)

echo '#### wfs-outputs.txt'
cat $WFS_PATH/wfs-outputs.txt
echo '####'
echo
echo '#### wfs-output-dids.txt'
cat $WFS_PATH/wfs-output-dids.txt
echo '####'
echo
echo '#### wfs-next-stage-dids.txt'
cat $WFS_PATH/wfs-next-stage-dids.txt
echo '####'
echo
echo '#### wfs-output-rse-list.txt'
cat $WFS_PATH/wfs-output-rse-list.txt
echo '####'

output_dids=`echo \`sed 's/.*/"&"/' wfs-output-dids.txt\`|sed 's/ /,/g'`

next_stage_dids=`echo \`sed 's/.*/"&"/' wfs-next-stage-dids.txt\`|sed 's/ /,/g'`

# wfs-bootstrap.sh should produce lists of successfully processed input files
if [ -f workspace/wfs-processed-dids.txt ] ; then
  processed_dids=`echo \`sed -r 's/.+/"&"/' workspace/wfs-processed-dids.txt\`|sed 's/ /,/g'`
fi

if [ -f workspace/wfs-processed-pfns.txt ] ; then
  processed_pfns=`echo \`sed -r 's/.+/"&"/' workspace/wfs-processed-pfns.txt\`|sed 's/ /,/g'`
fi

tail -c 10000 workspace/wfs-bootstrap.log | base64 --wrap=0 > wfs-bootstrap.log.b64

cat <<EOF >wfs-record-results.json
{
  "method": "record_results",
  "jobsub_id": "$JOBSUBJOBID",
  "processed_dids": [$processed_dids],
  "processed_pfns": [$processed_pfns],
  "output_dids": [$output_dids],
  "next_stage_dids": [$next_stage_dids],
  "bootstrap_log": "`cat wfs-bootstrap.log.b64`"
}
EOF

echo "=====Start wfs-record-results.json=="
cat wfs-record-results.json
echo "=====End wfs-record-results.json=="

http_code=`curl \
--retry 5 \
--retry-max-time 300 \
--max-time 600 \
--user-agent 'wfs-generic-job' \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--header "X-Jobid: $JOBSUBJOBID" \
--capath $X509_CERTIFICATES \
--data @wfs-record-results.json \
--output record-results.log \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi/record_results`

log_line "record_results returns HTTP code $http_code"
echo "=====Start record-results.log=="
touch record-results.log
cat record-results.log
echo "=====End record-results.log=="

if [ "$http_code" != 200 ] ; then
  job_aborted $http_code record_results
fi

# Just try the first RSE for now; eventually will do failovers on errors
export OUTPUT_RSE=`head -1 wfs-output-rse-list.txt | cut -f1 -d' '`
export OUTPUT_PROTOCOL=`head -1 wfs-output-rse-list.txt | cut -f2 -d' '`
export OUTPUT_PREFIX=`head -1 wfs-output-rse-list.txt | cut -f3 -d' '`
log_line "Using $OUTPUT_RSE for output"

cat $WFS_PATH/wfs-outputs.txt | (

cat <<EOF >$WFS_PATH/rucio.cfg
[client]
rucio_host = https://dune-rucio.fnal.gov
auth_host = https://auth-dune-rucio.fnal.gov
account = dunepro
auth_type = x509_proxy
request_retries = 3
EOF

echo '====Start rucio.cfg===='
cat $WFS_PATH/rucio.cfg
echo '====End rucio.cfg===='

source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh
setup rucio
setup metacat

export METACAT_AUTH_SERVER_URL=https://metacat.fnal.gov:8143/auth/dune
export METACAT_SERVER_URL=https://metacat.fnal.gov:9443/dune_meta_demo/app

rucio --config $WFS_PATH/rucio.cfg --version
if [ $? != 0 ] ; then
  job_aborted 900 'Rucio test fails'
fi

metacat auth login -m x509 dunepro
metacat auth whoami
if [ $? != 0 ] ; then
  job_aborted 900 'MetaCat whoami fails'
fi

touch wfs-output-dids-rses.txt

while read dataset scope fn
do
  file_size=`wc --bytes "workspace/$fn" | cut -f1 -d' '`

  # Make the JSON to send, including the file's JSON
  echo '{' > tmp.json
  echo "\"size\" : $file_size," >> tmp.json
  echo "\"namespace\" : \"$scope\"," >> tmp.json
  echo "\"name\" : \"$fn\"," >> tmp.json
  echo "\"metadata\" : {" >> tmp.json
  echo "\"DUNE.workflow\" : {" >> tmp.json
  echo "\"site_name\" : \"$WFS_SITE_NAME\"," >> tmp.json
  echo "\"request_id\" : $WFS_REQUEST_ID," >> tmp.json
  echo "\"stage_id\" : $WFS_STAGE_ID," >> tmp.json
  echo "\"hostname\" : \"$HOSTNAME\" }" >> tmp.json

  if [ -r "workspace/$fn.json" ] ; then  
    echo ',' >> tmp.json
    tail --bytes +2 "workspace/$fn.json" >> tmp.json
  else
    echo '}' >> tmp.json
  fi

  echo '}' >> tmp.json

  echo "==== Start MetaCat JSON for $fn ===="
  cat tmp.json
  echo "==== End MetaCat JSON for $fn ===="

  log_line "Declare file in MetaCat"
  metacat file declare --json tmp.json "$dataset"
  if [ $? = 0 ] ; then
    if [ "$OUTPUT_PREFIX" != "" ] ; then
      log_line "Upload $scope:$fn to $OUTPUT_RSE/$OUTPUT_PROTOCOL ($OUTPUT_PREFIX)"
      rucio --config $WFS_PATH/rucio.cfg --verbose \
            upload --rse "$OUTPUT_RSE" \
             --transfer-timeout 600 \
             --pfn "$OUTPUT_PREFIX/$fn" \
             --protocol "$OUTPUT_PROTOCOL" \
             --scope "$scope" --name "$fn" "workspace/$fn"
    else
      log_line "Upload $scope:$fn to $OUTPUT_RSE/$OUTPUT_PROTOCOL"
      rucio --config $WFS_PATH/rucio.cfg --verbose \
            upload --rse "$OUTPUT_RSE" \
             --transfer-timeout 600 \
             --protocol "$OUTPUT_PROTOCOL" \
             --scope "$scope" --name "$fn" "workspace/$fn"
    fi

    if [ $? = 0 ] ; then
      log_line "Add $scope:$fn to $dataset"
      rucio --config $WFS_PATH/rucio.cfg \
            --verbose attach "$dataset" "$scope:$fn"
      if [ $? != 0 ] ; then
        log_line "rucio attach fails"
        job_aborted 900 "rucio attach error"      
      fi
      echo "\"$scope:$fn\":\"$OUTPUT_RSE\"" >> wfs-output-dids-rses.txt
    else
      log_line "rucio upload fails"
      job_aborted 900 "rucio upload error"
    fi
  else
    log_line "metacat file declare fails"
    job_aborted 900 "metacat file declare error"
  fi 
done

)
if [ $? != 0 ] ; then
  # Exit/aborts inside subshell just exit the subshell with a non-zero code
  # So we exit the generic job script properly here
  exit 1
fi

# If all ok, then confirm that to the Workflow Allocator

cat <<EOF >wfs-confirm-results.json
{
  "method": "confirm_results",
  "jobsub_id": "$JOBSUBJOBID",
  "output_dids": {
EOF

echo `cat wfs-output-dids-rses.txt` | sed 's/ /,/g' >>wfs-confirm-results.json

echo '} }' >>wfs-confirm-results.json

echo "=====Start wfs-confirm-results.json=="
cat wfs-confirm-results.json
echo "=====End wfs-confirm-results.json=="

http_code=`curl \
--retry 5 \
--retry-max-time 300 \
--max-time 600 \
--user-agent 'wfs-generic-job' \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath $X509_CERTIFICATES \
--data @wfs-confirm-results.json \
--output confirm-results.log \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi/confirm_results`

log_line "confirm_results returns HTTP code $http_code"
echo "=====Start confirm-results.log=="
touch confirm-results.log
cat confirm-results.log
echo "=====End confirm-results.log=="

if [ "$http_code" != 200 ] ; then
  job_aborted $http_code confirm_results
fi

log_line '====End of wfs-generic-job===='
exit 0

