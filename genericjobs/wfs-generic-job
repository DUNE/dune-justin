#!/bin/sh
#
# Generic Job script which will get a stage bootstrap script from the
# Workflow Allocator
#

echo '====Start of wfs-generic-job===='

echo '### date ; pwd ; printenv'
date
pwd
printenv

# Used by bootstrap script to find files from this generic job
export WFS_PATH=`pwd`

# Create the wfs-get-file command
base64 -d <<EOF > $WFS_PATH/wfs-get-file
IyEvYmluL3NoCiMKIyBTY3JpcHQgZm9yIHVzZSB3aXRoaW4gYm9vdHN0cmFwIHNjcmlwdCB0byBn
ZXQgdGhlIERJRCwgUEZOLCBhbmQgUlNFCiMgb2YgYSBmaWxlIHRvIHByb2Nlc3Mgd2l0aGluIHRo
ZSBzdGFnZSBhc3NpZ25lZCB0byB0aGUgam9iLgojCiMgQm9vdHN0cmFwIHNjcmlwdHMgY2FuIGV4
ZWN1dGUgdGhpcyBzY3JpcHQgYXM6ICRXRlNfUEFUSC93ZnMtZ2V0LWZpbGUKIwojIEVycm9yIG1l
c3NhZ2VzIHRvIHN0ZGVycgojIERJRCBQRk4gUlNFIHRvIHN0ZG91dCBvbiBvbmUgbGluZSBpZiBh
IGZpbGUgaXMgYXZhaWxhYmxlCiMKIyBUaGlzIHNjcmlwdCBtdXN0IGJlIGNvbnZlcnRlZCB0byBi
YXNlNjQgd2l0aCBzb21ldGhpbmcgbGlrZSB0aGUgZm9sbG93aW5nIAojIGFuZCBpbmNsdWRlZCBp
biB0aGUgaGVyZSBkb2N1bWVudCBuZWFyIHRoZSBzdGFydCBvZiB3ZnMtZ2VuZXJpYy1qb2IgOgoj
CiMgKG1hY09TKSBiYXNlNjQgLWIgNzYgd2ZzLWdldC1maWxlID4gd2ZzLWdldC1maWxlLmI2NAoj
IChMaW51eCkgYmFzZTY0ICAgICAgIHdmcy1nZXQtZmlsZSA+IHdmcy1nZXQtZmlsZS5iNjQKCmlm
IFsgISAtciAiJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIiBdIDsgdGhlbgogIGVjaG8gIiRX
RlNfUEFUSC93ZnMtZ2V0LWZpbGUuanNvbiBub3QgZm91bmQhIiA+JjIKICBleGl0IDIKZmkKCkdF
VF9GSUxFX1RNUD1gbWt0ZW1wIC90bXAvd2ZzX2dldF9maWxlX1hYWFhYWGAKCmh0dHBfY29kZT1g
Y3VybCBcCi0tc2lsZW50IFwKLS1oZWFkZXIgIlgtSm9iaWQ6ICRKT0JTVUJKT0JJRCIgXAotLWhl
YWRlciAiQWNjZXB0OiB0ZXh0L3BsYWluIiBcCi0tY2FwYXRoICR7WDUwOV9DRVJUSUZJQ0FURVM6
LS9ldGMvZ3JpZC1zZWN1cml0eS9jZXJ0aWZpY2F0ZXMvfSBcCi0tZGF0YSBAJFdGU19QQVRIL3dm
cy1nZXQtZmlsZS5qc29uIFwKLS1vdXRwdXQgJEdFVF9GSUxFX1RNUCBcCi0td3JpdGUtb3V0ICIl
e2h0dHBfY29kZX1cbiIgXApodHRwczovL3dmcy1wcm8uZHVuZS5oZXAuYWMudWsvd2ZhLWNnaSAy
Pi9kZXYvbnVsbGAKCmlmIFsgIiRodHRwX2NvZGUiID0gMjAwIF0gOyB0aGVuCiBjYXQgJEdFVF9G
SUxFX1RNUAogcmV0Y29kZT0wCmVsaWYgWyAiJGh0dHBfY29kZSIgPSA0MDQgXSA7IHRoZW4gCiBl
Y2hvICJObyBmaWxlcyBhdmFpbGFibGUgZnJvbSB0aGlzIHN0YWdlIiA+JjIKIHJldGNvZGU9MQpl
bHNlCiBlY2hvICJnZXRfZmlsZSByZWNlaXZlczoiID4mMgogY2F0ICRHRVRfRklMRV9UTVAgPiYy
CiBlY2hvICJnZXQtZmlsZSBmYWlscyB3aXRoIEhUVFAgY29kZSAkaHR0cF9jb2RlIGZyb20gYWxs
b2NhdG9yISIgPiYyCiByZXRjb2RlPTMKZmkKCnJtIC1mICRHRVRfRklMRV9UTVAKZXhpdCAkcmV0
Y29kZQo=
EOF
chmod +x $WFS_PATH/wfs-get-file

# Create the wfs-rucio command
base64 -d <<EOF > $WFS_PATH/wfs-rucio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EOF
chmod +x $WFS_PATH/wfs-rucio

# Assemble values we will need 
export site_name=${GLIDEIN_DUNESite:-XX_UNKNOWN}
export cpuinfo=`grep '^model name' /proc/cpuinfo | head -1 | cut -c14-`
export os_release=`head -1 /etc/redhat-release`
export hostname=`hostname`

echo "### $_CONDOR_JOB_AD"
cat $_CONDOR_JOB_AD

export processors=`grep '^RequestCpus = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_mb=`grep '^RequestMemory = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_bytes=`expr $rss_mb \* 1048576`
export wall_seconds=`grep '^GLIDEIN_Max_Walltime = ' $_CONDOR_MACHINE_AD | cut -d' ' -f3`

export X509_CERTIFICATES=${X509_CERTIFICATES:-/etc/grid-security/certificates/}

# Check requirements are present

if [ ! -r "$X509_USER_PROXY" ] ; then
 echo "Cannot read X509_USER_PROXY file = $X509_USER_PROXY"
 exit
fi

curl --version
if [ $? -ne 0 ] ; then
 echo Failed running curl
 exit
fi

# Install a private version of the Rucio client
# Will be removed once we can get Rucio entirely from cvmfs
(
export LD_LIBRARY_PATH=""
export PATH="/cvmfs/fermilab.opensciencegrid.org/packages/external/python/3.8.8/linux-scientific7-x86_64-gcc-8.2.0-x7eyi5xeluhxm3nzllayh4lsmdz3pfai/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin"
export PYTHONPATH="/cvmfs/fermilab.opensciencegrid.org/packages/external/python/3.8.8/linux-scientific7-x86_64-gcc-8.2.0-x7eyi5xeluhxm3nzllayh4lsmdz3pfai/lib/python3.8"

. /cvmfs/grid.cern.ch/c7ui-test/etc/profile.d/setup-c7-ui-python3-example.sh

rm -Rf $PWD/wfs-rucio-local
export WFS_RUCIO_LOCAL=$PWD/wfs-rucio-local
export PATH="$WFS_RUCIO_LOCAL/bin:$PATH"
export PYTHONPATH="$WFS_RUCIO_LOCAL/lib/python3.8/site-packages:$WFS_RUCIO_LOCAL/lib64/python3.8/site-packages:$PYTHONPATH"

mkdir -p $WFS_RUCIO_LOCAL/bin
# Have to create symbolic links since cvmfs path 
# is too long for hash-bang at the start of a script
ln -sf /cvmfs/fermilab.opensciencegrid.org/packages/external/python/3.8.8/linux-scientific7-x86_64-gcc-8.2.0-x7eyi5xeluhxm3nzllayh4lsmdz3pfai/bin/python3.8 \
       $WFS_RUCIO_LOCAL/bin/python38
ln -sf $WFS_RUCIO_LOCAL/bin/python38 $WFS_RUCIO_LOCAL/bin/python3

# Have to install PIP for Python3
curl --insecure https://bootstrap.pypa.io/get-pip.py > get-pip.py
$WFS_RUCIO_LOCAL/bin/python3 get-pip.py --prefix $WFS_RUCIO_LOCAL \
  --ignore-installed

# Install Rucio with PIP - DOES NOT WORK WITH RUCIO 1.27 !!!
$WFS_RUCIO_LOCAL/bin/pip3 install --prefix $WFS_RUCIO_LOCAL \
  --ignore-installed rucio-clients==1.26

cat <<EOF > $WFS_RUCIO_LOCAL/etc/rucio.cfg
[client]
rucio_host = https://dune-rucio.fnal.gov
auth_host = https://auth-dune-rucio.fnal.gov
ca_cert = \$X509_CERTIFICATES
account = amcnab
auth_type = x509_proxy
client_x509_proxy = \$X509_USER_PROXY
request_retries = 3
EOF
) 2>&1

echo "#### ls -lR $WFS_RUCIO_LOCAL ####"
ls -lR $WFS_RUCIO_LOCAL
echo "####"

# Create the JSON to send to the allocator
cat <<EOF >wfs-get-stage.json
{
  "method"       : "get_stage",
  "jobsub_id"    : "$JOBSUBJOBID",
  "site_name"    : "$site_name",
  "cpuinfo"      : "$cpuinfo",
  "os_release"   : "$os_release",
  "hostname"     : "$hostname",
  "rss_bytes"    : $rss_bytes,
  "processors"   : $processors,
  "wall_seconds" : $wall_seconds
}
EOF

echo '====start wfs-get-stage.json===='
cat wfs-get-stage.json
echo '====end wfs-get-stage.json===='

# Make the call to the Workflow Allocator
http_code=`curl \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath $X509_CERTIFICATES \
--data @wfs-get-stage.json \
--output wfs-files.tar \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi`

echo curl returns HTTP code $http_code

if [ "$http_code" != "200" ] ; then
  echo "curl call to WFA fails with code $http_code"
  cat wfs-files.tar
  exit 1
fi

tar xvf wfs-files.tar

if [ -r wfs-env.sh ] ; then
  . ./wfs-env.sh
fi

echo '====Start wfs-output-patterns.txt===='
cat wfs-output-patterns.txt
echo '====End wfs-output-patterns.txt===='

echo '====Start wfs-get-file.json===='
cat wfs-get-file.json
echo '====End wfs-get-file.json===='

# Run the bootstrap script
if [ -f wfs-bootstrap.sh ] ; then
  chmod +x wfs-bootstrap.sh

  echo '====Start wfs-bootstrap.sh===='
  cat wfs-bootstrap.sh
  echo '====End wfs-bootstrap.sh===='

  mkdir workspace
  echo '====Run wfs-bootstrap.sh===='
  ( cd workspace ; $WFS_PATH/wfs-bootstrap.sh ) >wfs-bootstrap.log 2>&1
  retval=$?
  echo '====After wfs-bootstrap.sh===='
else
  # How can this happen???
  echo No wfs-bootstrap.sh found
  exit 1
fi

echo '#### wfs-bootstrap.log'
cat wfs-bootstrap.log
echo '####'

# Make the lists of output files and files for the next stage
echo -n > wfs-outputs.txt
echo -n > wfs-next-stage-outputs.txt

cat wfs-output-patterns.txt | (
while read for_next_stage dataset scope pattern
do  
  (
    cd workspace
    # $pattern is wildcard-expanded here - so a list of files
    for fn in $pattern
    do
      if [ -r "$fn" ] ; then
        # wfs-outputs.txt gets DATASET SCOPE FILENAME on each line
        echo "$dataset $scope $fn" >> $WFS_PATH/wfs-outputs.txt
        if [ "$for_next_stage" = "True" ] ; then
          # wfs-next-stage-outputs gets a list of DIDs
          echo "$scope:$fn" >> $WFS_PATH/wfs-next-stage-outputs.txt    
        fi
      fi
    done
  )
done
)

echo '#### wfs-outputs.txt'
cat $WFS_PATH/wfs-outputs.txt
echo '####'
echo
echo '#### wfs-next-stage-outputs.txt'
cat $WFS_PATH/wfs-next-stage-outputs.txt
echo '####'

next_stage_outputs=`echo \`sed 's/.*/"&"/' wfs-next-stage-outputs.txt\`|sed 's/ /,/g'`

# Just try the first RSE for now; eventually will do failovers on errors
output_rse=`echo $WFS_OUTPUT_RSE_LIST | cut -f1 -d' '`

cat $WFS_PATH/wfs-outputs.txt | (

while read dataset scope fn
do
  echo "Upload $scope:$fn"
  $WFS_PATH/wfs-rucio --verbose -a test upload --rse "$output_rse" --lifetime 3600 --scope "$scope" "workspace/$fn" 
  echo "wfs-rucio returns $?"

  if [ $? = 0 ] ; then
    echo "Add $scope:$fn to $dataset"
    $WFS_PATH/wfs-rucio --verbose -a test attach "$dataset" "$scope:$fn"
  fi
 
  echo "Metadata too? $fn.json"
  echo
done

)

# wfs-bootstrap.sh should produce a list of successfully processed input files
# and a list of files which still need to be processed by another job
if [ -f workspace/wfs-processed-inputs.txt ] ; then
  processed_inputs=`echo \`sed 's/.*/"&"/' workspace/wfs-processed-inputs.txt\`|sed 's/ /,/g'`
fi

if [ -f workspace/wfs-unprocessed-inputs.txt ] ; then
  unprocessed_inputs=`echo \`sed 's/.*/"&"/' workspace/wfs-unprocessed-inputs.txt\`|sed 's/ /,/g'`
fi

cat <<EOF >wfs-return-results.json
{
  "method": "return_results",
  "jobsub_id": "$JOBSUBJOBID",
  "cookie": "$WFS_COOKIE",
  "processed_inputs": [$processed_inputs],
  "unprocessed_inputs": [$unprocessed_inputs],
  "next_stage_outputs": [$next_stage_outputs],
  "bootstrap_log": "
EOF

tail -c 10000 wfs-bootstrap.log | sed 's/"/\\\"/g' >>wfs-return-results.json
  
echo '"}' >>wfs-return-results.json

echo "=====Start wfs-return-results.json=="
cat wfs-return-results.json
echo "=====End wfs-return-results.json=="

http_code=`curl \
--header "X-Jobid: $JOBSUBJOBID" \
--capath $X509_CERTIFICATES \
--data @wfs-return-results.json \
--output return-results.log \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi`

echo "return_results returns HTTP code $http_code"

echo '====End of wfs-generic-job===='
exit 0

