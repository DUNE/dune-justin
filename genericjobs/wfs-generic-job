#!/bin/bash
#
# Generic Job script which will get a stage bootstrap script from the
# Workflow Allocator
#

echo '====Start of wfs-generic-job===='

function job_aborted() {
  cat <<EOF >wfs-job-aborted.json
{
  "method"         : "job_aborted",
  "jobsub_id"      : "$JOBSUBJOBID",
  "http_code"      : $1,
  "aborted_method" : "$2"
}
EOF

  curl \
     --user-agent 'wfs-generic-job' \
     --header "X-Jobid: $JOBSUBJOBID" \
     --key $X509_USER_PROXY \
     --cert $X509_USER_PROXY \
     --cacert $X509_USER_PROXY \
     --capath $X509_CERTIFICATES \
     --data @wfs-job-aborted.json \
     https://wfs-pro.dune.hep.ac.uk/wfa-cgi/job_aborted_$1
     
  exit 1
}

echo '### date ; pwd ; printenv'
date
pwd
printenv

echo '### ls -lt ###'
ls -lt
echo '##############'

# Used by bootstrap script to find files from this generic job
export WFS_PATH=`pwd`

# Create the wfs-get-file command
base64 -d <<EOF > $WFS_PATH/wfs-get-file
IyEvYmluL3NoCiMKIyBTY3JpcHQgZm9yIHVzZSB3aXRoaW4gYm9vdHN0cmFwIHNjcmlwdCB0byBn
ZXQgdGhlIERJRCwgUEZOLCBhbmQgUlNFCiMgb2YgYSBmaWxlIHRvIHByb2Nlc3Mgd2l0aGluIHRo
ZSBzdGFnZSBhc3NpZ25lZCB0byB0aGUgam9iLgojCiMgQm9vdHN0cmFwIHNjcmlwdHMgY2FuIGV4
ZWN1dGUgdGhpcyBzY3JpcHQgYXM6ICRXRlNfUEFUSC93ZnMtZ2V0LWZpbGUKIwojIEVycm9yIG1l
c3NhZ2VzIHRvIHN0ZGVycgojIERJRCBQRk4gUlNFIHRvIHN0ZG91dCBvbiBvbmUgbGluZSBpZiBh
IGZpbGUgaXMgYXZhaWxhYmxlCiMKIyBUaGlzIHNjcmlwdCBtdXN0IGJlIGNvbnZlcnRlZCB0byBi
YXNlNjQgd2l0aCBzb21ldGhpbmcgbGlrZSB0aGUgZm9sbG93aW5nIAojIGFuZCBpbmNsdWRlZCBp
biB0aGUgaGVyZSBkb2N1bWVudCBuZWFyIHRoZSBzdGFydCBvZiB3ZnMtZ2VuZXJpYy1qb2IgOgoj
CiMgKG1hY09TKSBiYXNlNjQgLWIgNzYgd2ZzLWdldC1maWxlID4gd2ZzLWdldC1maWxlLmI2NAoj
IChMaW51eCkgYmFzZTY0ICAgICAgIHdmcy1nZXQtZmlsZSA+IHdmcy1nZXQtZmlsZS5iNjQKCmlm
IFsgISAtciAiJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIiBdIDsgdGhlbgogIGVjaG8gIiRX
RlNfUEFUSC93ZnMtZ2V0LWZpbGUuanNvbiBub3QgZm91bmQhIiA+JjIKICBleGl0IDIKZmkKCkdF
VF9GSUxFX1RNUD1gbWt0ZW1wIC90bXAvd2ZzX2dldF9maWxlX1hYWFhYWGAKCmh0dHBfY29kZT1g
Y3VybCBcCi0tdXNlci1hZ2VudCAnd2ZzLWdldC1maWxlJyBcCi0tc2lsZW50IFwKLS1oZWFkZXIg
IlgtSm9iaWQ6ICRKT0JTVUJKT0JJRCIgXAotLWhlYWRlciAiQWNjZXB0OiB0ZXh0L3BsYWluIiBc
Ci0tY2FwYXRoICR7WDUwOV9DRVJUSUZJQ0FURVM6LS9ldGMvZ3JpZC1zZWN1cml0eS9jZXJ0aWZp
Y2F0ZXMvfSBcCi0tZGF0YSBAJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIFwKLS1vdXRwdXQg
JEdFVF9GSUxFX1RNUCBcCi0td3JpdGUtb3V0ICIle2h0dHBfY29kZX1cbiIgXApodHRwczovL3dm
cy1wcm8uZHVuZS5oZXAuYWMudWsvd2ZhLWNnaS9nZXRfZmlsZSAyPi9kZXYvbnVsbGAKCmlmIFsg
IiRodHRwX2NvZGUiID0gMjAwIF0gOyB0aGVuCiBjYXQgJEdFVF9GSUxFX1RNUAogcmV0Y29kZT0w
CmVsaWYgWyAiJGh0dHBfY29kZSIgPSA0MDQgXSA7IHRoZW4gCiBlY2hvICJObyBmaWxlcyBhdmFp
bGFibGUgZnJvbSB0aGlzIHN0YWdlIiA+JjIKIHJldGNvZGU9MQplbHNlCiBlY2hvICJnZXRfZmls
ZSByZWNlaXZlczoiID4mMgogY2F0ICRHRVRfRklMRV9UTVAgPiYyCiBlY2hvICJnZXQtZmlsZSBm
YWlscyB3aXRoIEhUVFAgY29kZSAkaHR0cF9jb2RlIGZyb20gYWxsb2NhdG9yISIgPiYyCiByZXRj
b2RlPTMKZmkKCnJtIC1mICRHRVRfRklMRV9UTVAKZXhpdCAkcmV0Y29kZQo=
EOF
chmod +x $WFS_PATH/wfs-get-file

# Assemble values to record
export site_name=${GLIDEIN_DUNESite:-XX_UNKNOWN}
export cpuinfo=`grep '^model name' /proc/cpuinfo | head -1 | cut -c14-`
export os_release=`head -1 /etc/redhat-release`
export hostname=`hostname`

echo "### $_CONDOR_JOB_AD"
cat $_CONDOR_JOB_AD

export processors=`grep '^RequestCpus = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_mb=`grep '^RequestMemory = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_bytes=`expr $rss_mb \* 1048576`
export wall_seconds=`grep '^GLIDEIN_Max_Walltime = ' $_CONDOR_MACHINE_AD | cut -d' ' -f3`

export X509_CERTIFICATES=${X509_CERTIFICATES:-/etc/grid-security/certificates/}

# Check requirements are present

if [ ! -r "$X509_USER_PROXY" ] ; then
 echo "Cannot read X509_USER_PROXY file = $X509_USER_PROXY"
 exit
fi

curl --version
if [ $? -ne 0 ] ; then
 echo Failed running curl
 exit
fi

cat <<EOF >wfs-send-heartbeat.json
{
  "method"       : "send_heartbeat",
  "jobsub_id"    : "$JOBSUBJOBID"
}
EOF

(
# Subprocess to send regular heartbeats

while :
do
  # Stop if proxy file is missing or deleted
  if [ ! -r wfs-send-heartbeat.json ] ; then
    exit 1
  fi

  curl \
     --user-agent 'wfs-generic-job' \
     --header "X-Jobid: $JOBSUBJOBID" \
     --key $X509_USER_PROXY \
     --cert $X509_USER_PROXY \
     --cacert $X509_USER_PROXY \
     --capath $X509_CERTIFICATES \
     --data @wfs-send-heartbeat.json \
     https://wfs-pro.dune.hep.ac.uk/wfa-cgi/send_heartbeat

  sleep 600
done

) >wfs-heartbeat.log 2>&1 &

# Create the JSON to send to the allocator
cat <<EOF >wfs-get-stage.json
{
  "method"       : "get_stage",
  "jobsub_id"    : "$JOBSUBJOBID",
  "site_name"    : "${site_name:-XX-UNKNOWN}",
  "cpuinfo"      : "${cpuinfo:-Unknown}",
  "os_release"   : "${os_release:-Unknown}",
  "hostname"     : "${hostname:-unknown}",
  "rss_bytes"    : ${rss_bytes:-0},
  "processors"   : ${processors:-0},
  "wall_seconds" : ${wall_seconds:-0}
}
EOF

echo '====start wfs-get-stage.json===='
cat wfs-get-stage.json
echo '====end wfs-get-stage.json===='

# Make the call to the Workflow Allocator
http_code=`curl \
--user-agent 'wfs-generic-job' \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath $X509_CERTIFICATES \
--data @wfs-get-stage.json \
--output wfs-files.tar \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi/get_stage`

echo curl returns HTTP code $http_code

if [ "$http_code" != "200" ] ; then
  echo "curl call to WFA fails with code $http_code"
  cat wfs-files.tar
  job_aborted $http_code get_stage
fi

## Start of temporary Rucio client install. Eventually this
## section can be removed once we can get Rucio from cvmfs
(
## These three lines can be used to use the installed Rucio client
export RUCIO_HOME=$PWD/rucio-local
export PATH="$RUCIO_HOME/bin:$PATH"
export PYTHONPATH="$RUCIO_HOME/lib/python3.6/site-packages:/usr/lib64/python3.6/site-packages"
##

mkdir -p $RUCIO_HOME/bin $RUCIO_HOME/etc
pip3 install --no-cache-dir --prefix $RUCIO_HOME \
             --ignore-installed rucio-clients==1.26.9

cat <<EOF > $RUCIO_HOME/etc/rucio.cfg
[client]
rucio_host = https://dune-rucio.fnal.gov
auth_host = https://auth-dune-rucio.fnal.gov
account = amcnab
auth_type = x509_proxy
request_retries = 3
EOF

echo '====Start rucio.cfg===='
cat $RUCIO_HOME/etc/rucio.cfg
echo '====End rucio.cfg===='

rucio --version
if [ $? != 0 ] ; then
  job_aborted 900 'Rucio install fails'
fi
)
##Â End of Rucio install section

#

tar xvf wfs-files.tar

if [ -r wfs-env.sh ] ; then
  . ./wfs-env.sh
fi

echo '====Start wfs-output-patterns.txt===='
cat wfs-output-patterns.txt
echo '====End wfs-output-patterns.txt===='

echo '====Start wfs-get-file.json===='
cat wfs-get-file.json
echo '====End wfs-get-file.json===='

# Run the bootstrap script
if [ -f wfs-bootstrap.sh ] ; then
  chmod +x wfs-bootstrap.sh

  echo '====Start wfs-bootstrap.sh===='
  cat wfs-bootstrap.sh
  echo '====End wfs-bootstrap.sh===='

  mkdir workspace
  echo '====Run wfs-bootstrap.sh===='
  ( cd workspace ; $WFS_PATH/wfs-bootstrap.sh ) >wfs-bootstrap.log 2>&1
  retval=$?
  echo '====After wfs-bootstrap.sh===='
else
  # How can this happen???
  echo No wfs-bootstrap.sh found
  exit 1
fi

echo '#### wfs-bootstrap.log'
cat wfs-bootstrap.log
echo '####'

# Make the lists of output files and files for the next stage
echo -n > wfs-outputs.txt
echo -n > wfs-output-dids.txt
echo -n > wfs-next-stage-dids.txt

cat wfs-output-patterns.txt | (
while read for_next_stage dataset scope pattern
do  
  (
    cd workspace
    # $pattern is wildcard-expanded here - so a list of files
    for fn in $pattern
    do
      if [ -r "$fn" ] ; then
        # wfs-outputs.txt gets DATASET SCOPE FILENAME on each line
        echo "$dataset $scope $fn" >> $WFS_PATH/wfs-outputs.txt

        if [ "$for_next_stage" = "True" ] ; then
          # wfs-next-stage-dids gets a list of DIDs
          echo "$scope:$fn" >> $WFS_PATH/wfs-next-stage-dids.txt
        else
          # wfs-output-dids.txt gets DID = SCOPE:FILENAME
          echo "$scope:$fn" >> $WFS_PATH/wfs-output-dids.txt
        fi
      fi
    done
  )
done
)

echo '#### wfs-outputs.txt'
cat $WFS_PATH/wfs-outputs.txt
echo '####'
echo
echo '#### wfs-output-dids.txt'
cat $WFS_PATH/wfs-output-dids.txt
echo '####'
echo
echo '#### wfs-next-stage-dids.txt'
cat $WFS_PATH/wfs-next-stage-dids.txt
echo '####'

output_dids=`echo \`sed 's/.*/"&"/' wfs-output-dids.txt\`|sed 's/ /,/g'`

next_stage_dids=`echo \`sed 's/.*/"&"/' wfs-next-stage-dids.txt\`|sed 's/ /,/g'`

# wfs-bootstrap.sh should produce lists of successfully processed input files
if [ -f workspace/wfs-processed-dids.txt ] ; then
  processed_dids=`echo \`sed -r 's/.+/"&"/' workspace/wfs-processed-dids.txt\`|sed 's/ /,/g'`
fi

if [ -f workspace/wfs-processed-pfns.txt ] ; then
  processed_pfns=`echo \`sed -r 's/.+/"&"/' workspace/wfs-processed-pfns.txt\`|sed 's/ /,/g'`
fi

tail -c 10000 wfs-bootstrap.log | base64 --wrap=0 > wfs-bootstrap.log.b64

cat <<EOF >wfs-record-results.json
{
  "method": "record_results",
  "jobsub_id": "$JOBSUBJOBID",
  "processed_dids": [$processed_dids],
  "processed_pfns": [$processed_pfns],
  "output_dids": [$output_dids],
  "next_stage_dids": [$next_stage_dids],
  "bootstrap_log": "`cat wfs-bootstrap.log.b64`"
}
EOF

echo "=====Start wfs-record-results.json=="
cat wfs-record-results.json
echo "=====End wfs-record-results.json=="

http_code=`curl \
--retry 5 \
--retry-max-time 300 \
--max-time 600 \
--user-agent 'wfs-generic-job' \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--header "X-Jobid: $JOBSUBJOBID" \
--capath $X509_CERTIFICATES \
--data @wfs-record-results.json \
--output record-results.log \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi/record_results`

echo "record_results returns HTTP code $http_code"
echo "=====Start record-results.log=="
touch record-results.log
cat record-results.log
echo "=====End record-results.log=="

if [ "$http_code" != 200 ] ; then
  job_aborted $http_code record_results
fi

# Just try the first RSE for now; eventually will do failovers on errors
output_rse=`echo $WFS_OUTPUT_RSE_LIST | cut -f1 -d' '`
echo "Using $output_rse for output, out of $WFS_OUTPUT_RSE_LIST"

cat $WFS_PATH/wfs-outputs.txt | (

# Because in a subshell, we can change paths to use temporary Rucio install
export RUCIO_HOME=$PWD/rucio-local
export PATH="$RUCIO_HOME/bin:$PATH"
export PYTHONPATH="$RUCIO_HOME/lib/python3.6/site-packages:/usr/lib64/python3.6/site-packages"

while read dataset scope fn
do
  echo "Metadata first? Would be $fn.json"
  echo

  echo "Upload $scope:$fn"
  rucio --verbose -a test upload --rse "$output_rse" \
        --lifetime 86400 --scope "$scope" --name "$fn" "workspace/$fn" 
  echo "rucio upload returns $?"

  if [ $? = 0 ] ; then
    echo "Add $scope:$fn to $dataset"
    rucio --verbose -a test attach "$dataset" "$scope:$fn"
    echo "rucio attach returns $?"
  fi
 
done



)

# If all ok, then confirm that to the Workflow Allocator

cat <<EOF >wfs-confirm-results.json
{
  "method": "confirm_results",
  "jobsub_id": "$JOBSUBJOBID"
}
EOF

echo "=====Start wfs-confirm-results.json=="
cat wfs-confirm-results.json
echo "=====End wfs-confirm-results.json=="

http_code=`curl \
--retry 5 \
--retry-max-time 300 \
--max-time 600 \
--user-agent 'wfs-generic-job' \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath $X509_CERTIFICATES \
--data @wfs-confirm-results.json \
--output confirm-results.log \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/wfa-cgi/confirm_results`

echo "confirm_results returns HTTP code $http_code"
echo "=====Start confirm-results.log=="
touch confirm-results.log
cat confirm-results.log
echo "=====End confirm-results.log=="

if [ "$http_code" != 200 ] ; then
  job_aborted $http_code confirm_results
fi

echo '====End of wfs-generic-job===='
exit 0

