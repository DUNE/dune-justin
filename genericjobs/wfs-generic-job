#!/bin/bash
#
# Generic Job script which will get a stage's bootstrap script from the
# Workflow Allocator
#

# Everything goes to stdout
exec 2>&1

function log_line() {
echo `date -u +'%Y-%m-%d %H:%M:%S '`$1
}

log_line '====Start of wfs-generic-job===='

function job_aborted() {

  cat <<EOF >wfs-job-aborted.json
{
  "method"         : "job_aborted",
  "jobsub_id"      : "$JOBSUBJOBID",
  "http_code"      :  $2,
  "aborted_method" : "$3",
  "rse_name"       : "$4"
}
EOF

  curl \
     --user-agent 'wfs-generic-job' \
     --header "X-Jobid: $JOBSUBJOBID" \
     --key $X509_USER_PROXY \
     --cert $X509_USER_PROXY \
     --cacert $X509_USER_PROXY \
     --capath $X509_CERTIFICATES \
     --data @wfs-job-aborted.json \
     https://wfs-pro.dune.hep.ac.uk/api/allocator/job_aborted_$2

  # If in a subshell, this just exits that, not the job, so must be caught
  # The 1st argument lets the caller choose the exit code on errors
  exit $1
}

#echo '### date ; pwd ; printenv'
#date
#pwd
#printenv
#echo "### $_CONDOR_JOB_AD"
#cat $_CONDOR_JOB_AD

echo '### Start of  printenv | sort'
printenv | sort
echo '### End of printenv'

echo 'Before GFAL unsets, do  printenv | grep GFAL'
printenv | grep GFAL
unset GFAL_CONFIG_DIR GFAL_PLUGIN_DIR

echo '### pwd ; ls -lt ###'
pwd
ls -lt
echo '##############'

# Used by bootstrap script to find files from this generic job
export WFS_PATH=`pwd`

# Create the wfs-get-file command
base64 -d <<EOF > $WFS_PATH/wfs-get-file
IyEvYmluL3NoCiMKIyBTY3JpcHQgZm9yIHVzZSB3aXRoaW4gYm9vdHN0cmFwIHNjcmlwdCB0byBn
ZXQgdGhlIERJRCwgUEZOLCBhbmQgUlNFCiMgb2YgYSBmaWxlIHRvIHByb2Nlc3Mgd2l0aGluIHRo
ZSBzdGFnZSBhc3NpZ25lZCB0byB0aGUgam9iLgojCiMgQm9vdHN0cmFwIHNjcmlwdHMgY2FuIGV4
ZWN1dGUgdGhpcyBzY3JpcHQgYXM6ICRXRlNfUEFUSC93ZnMtZ2V0LWZpbGUKIwojIEVycm9yIG1l
c3NhZ2VzIHRvIHN0ZGVycgojIERJRCBQRk4gUlNFIHRvIHN0ZG91dCBvbiBvbmUgbGluZSBpZiBh
IGZpbGUgaXMgYXZhaWxhYmxlCiMKIyBUaGlzIHNjcmlwdCBtdXN0IGJlIGNvbnZlcnRlZCB0byBi
YXNlNjQgd2l0aCBzb21ldGhpbmcgbGlrZSB0aGUgZm9sbG93aW5nIAojIGFuZCBpbmNsdWRlZCBp
biB0aGUgaGVyZSBkb2N1bWVudCBuZWFyIHRoZSBzdGFydCBvZiB3ZnMtZ2VuZXJpYy1qb2IgOgoj
CiMgKG1hY09TKSBiYXNlNjQgLWIgNzYgd2ZzLWdldC1maWxlID4gd2ZzLWdldC1maWxlLmI2NAoj
IChMaW51eCkgYmFzZTY0ICAgICAgIHdmcy1nZXQtZmlsZSA+IHdmcy1nZXQtZmlsZS5iNjQKCmlm
IFsgISAtciAiJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIiBdIDsgdGhlbgogIGVjaG8gIiRX
RlNfUEFUSC93ZnMtZ2V0LWZpbGUuanNvbiBub3QgZm91bmQhIiA+JjIKICBleGl0IDIKZmkKCkdF
VF9GSUxFX1RNUD1gbWt0ZW1wIC90bXAvd2ZzX2dldF9maWxlX1hYWFhYWGAKCmh0dHBfY29kZT1g
Y3VybCBcCi0tdXNlci1hZ2VudCAnd2ZzLWdldC1maWxlJyBcCi0tc2lsZW50IFwKLS1oZWFkZXIg
IlgtSm9iaWQ6ICRKT0JTVUJKT0JJRCIgXAotLWhlYWRlciAiQWNjZXB0OiB0ZXh0L3BsYWluIiBc
Ci0tY2FwYXRoICR7WDUwOV9DRVJUSUZJQ0FURVM6LS9ldGMvZ3JpZC1zZWN1cml0eS9jZXJ0aWZp
Y2F0ZXMvfSBcCi0tZGF0YSBAJFdGU19QQVRIL3dmcy1nZXQtZmlsZS5qc29uIFwKLS1vdXRwdXQg
JEdFVF9GSUxFX1RNUCBcCi0td3JpdGUtb3V0ICIle2h0dHBfY29kZX1cbiIgXApodHRwczovL3dm
cy1wcm8uZHVuZS5oZXAuYWMudWsvYXBpL2FsbG9jYXRvci9nZXRfZmlsZSAyPi9kZXYvbnVsbGAK
CmlmIFsgIiRodHRwX2NvZGUiID0gMjAwIF0gOyB0aGVuCiBjYXQgJEdFVF9GSUxFX1RNUAogcmV0
Y29kZT0wCmVsaWYgWyAiJGh0dHBfY29kZSIgPSA0MDQgXSA7IHRoZW4gCiBlY2hvICJObyBmaWxl
cyBhdmFpbGFibGUgZnJvbSB0aGlzIHN0YWdlIiA+JjIKIHJldGNvZGU9MQplbHNlCiBlY2hvICJn
ZXRfZmlsZSByZWNlaXZlczoiID4mMgogY2F0ICRHRVRfRklMRV9UTVAgPiYyCiBlY2hvICJnZXQt
ZmlsZSBmYWlscyB3aXRoIEhUVFAgY29kZSAkaHR0cF9jb2RlIGZyb20gYWxsb2NhdG9yISIgPiYy
CiByZXRjb2RlPTMKZmkKCnJtIC1mICRHRVRfRklMRV9UTVAKZXhpdCAkcmV0Y29kZQo=
EOF
chmod +x $WFS_PATH/wfs-get-file

# Create the wfs-get-file command
base64 -d <<EOF > $WFS_PATH/wfs-allocated-files
IyEvYmluL3NoCiMKIyBTY3JpcHQgZm9yIHVzZSB3aXRoaW4gYm9vdHN0cmFwIHNjcmlwdCB0byBn
ZXQgdGhlIERJRHMgb2YgYWxsIGZpbGVzIAojIGFsbG9jYXRlZCB0byB0aGlzIGpvYgojCiMgQm9v
dHN0cmFwIHNjcmlwdHMgY2FuIGV4ZWN1dGUgdGhpcyBzY3JpcHQgYXM6ICRXRlNfUEFUSC93ZnMt
YWxsb2NhdGVkLWZpbGVzCiMKIyBFcnJvciBtZXNzYWdlcyB0byBzdGRlcnIKIyBaZXJvIG9yIG1v
cmUgRElEcyB0byBzdGRvdXQsIG9uZSBwZXIgbGluZQojCiMgVGhpcyBzY3JpcHQgbXVzdCBiZSBj
b252ZXJ0ZWQgdG8gYmFzZTY0IHdpdGggc29tZXRoaW5nIGxpa2UgdGhlIGZvbGxvd2luZyAKIyBh
bmQgaW5jbHVkZWQgaW4gdGhlIGhlcmUgZG9jdW1lbnQgbmVhciB0aGUgc3RhcnQgb2Ygd2ZzLWdl
bmVyaWMtam9iIDoKIwojIChtYWNPUykgYmFzZTY0IC1iIDc2IHdmcy1hbGxvY2F0ZWQtZmlsZXMg
PiB3ZnMtYWxsb2NhdGVkLWZpbGVzLmI2NAojIChMaW51eCkgYmFzZTY0ICAgICAgIHdmcy1hbGxv
Y2F0ZWQtZmlsZXMgPiB3ZnMtYWxsb2NhdGVkLWZpbGVzLmI2NAoKaWYgWyAhIC1yICIkV0ZTX1BB
VEgvd2ZzLWFsbG9jYXRlZC1maWxlcy5qc29uIiBdIDsgdGhlbgogIGVjaG8gIiRXRlNfUEFUSC93
ZnMtYWxsb2NhdGVkLWZpbGVzLmpzb24gbm90IGZvdW5kISIgPiYyCiAgZXhpdCAyCmZpCgpBTExP
Q0FURURfRklMRVNfVE1QPWBta3RlbXAgL3RtcC93ZnNfYWxsb2NhdGVkX2ZpbGVzX1hYWFhYWGAK
Cmh0dHBfY29kZT1gY3VybCBcCi0tdXNlci1hZ2VudCAnd2ZzLWFsbG9jYXRlZC1maWxlcycgXAot
LXNpbGVudCBcCi0taGVhZGVyICJYLUpvYmlkOiAkSk9CU1VCSk9CSUQiIFwKLS1oZWFkZXIgIkFj
Y2VwdDogdGV4dC9wbGFpbiIgXAotLWNhcGF0aCAke1g1MDlfQ0VSVElGSUNBVEVTOi0vZXRjL2dy
aWQtc2VjdXJpdHkvY2VydGlmaWNhdGVzL30gXAotLWRhdGEgQCRXRlNfUEFUSC93ZnMtZ2V0LWZp
bGUuanNvbiBcCi0tb3V0cHV0ICRBTExPQ0FURURfRklMRVNfVE1QIFwKLS13cml0ZS1vdXQgIiV7
aHR0cF9jb2RlfVxuIiBcCmh0dHBzOi8vd2ZzLXByby5kdW5lLmhlcC5hYy51ay9hcGkvYWxsb2Nh
dG9yL2dldF9hbGxvY2F0ZWRfZmlsZXMgMj4vZGV2L251bGxgCgppZiBbICIkaHR0cF9jb2RlIiA9
IDIwMCBdIDsgdGhlbgogY2F0ICRBTExPQ0FURURfRklMRVNfVE1QCiByZXRjb2RlPTAKZWxzZQog
ZWNobyAid2ZzLWFsbG9jYXRlZC1maWxlcyByZWNlaXZlczoiID4mMgogY2F0ICRBTExPQ0FURURf
RklMRVNfVE1QID4mMgogZWNobyAid2ZzLWFsbG9jYXRlZC1maWxlcyBmYWlscyB3aXRoIEhUVFAg
Y29kZSAkaHR0cF9jb2RlISIgPiYyCiByZXRjb2RlPTEKZmkKCnJtIC1mICRBTExPQ0FURURfRklM
RVNfVE1QCmV4aXQgJHJldGNvZGUK
EOF
chmod +x $WFS_PATH/wfs-allocated-files

# Create the wfs-metadata command
base64 -d <<EOF > $WFS_PATH/wfs-metadata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EOF
chmod +x $WFS_PATH/wfs-metadata

# Assemble values to record
export WFS_CPUINFO=`grep '^model name' /proc/cpuinfo | head -1 | cut -c14-`
export WFS_OS_RELEASE=`head -1 /etc/redhat-release`
export WFS_HOSTNAME=${HOSTNAME:-`hostname`}
export site_name=${GLIDEIN_DUNESite:-XX_UNKNOWN}
if [ "$site_name" = "XX_UNKNOWN" -a "$GLIDEIN_Site" = "FNAL" ] ; then
  export site_name=US_FNAL
fi
if [ "$site_name" = "XX_UNKNOWN" -a "$GLIDEIN_Site" = "FermiGrid" ] ; then
  export site_name=US_FNAL
fi
if [ "$site_name" = "US_FermiGrid" ] ; then
  export site_name=US_FNAL
fi

export processors=`grep '^RequestCpus = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_mb=`grep '^RequestMemory = ' $_CONDOR_JOB_AD | cut -d' ' -f3`
export rss_bytes=`expr $rss_mb \* 1048576`
export wall_seconds=`grep '^GLIDEIN_Max_Walltime = ' $_CONDOR_MACHINE_AD | cut -d' ' -f3`

export X509_CERTIFICATES=${X509_CERTIFICATES:-/etc/grid-security/certificates/}

# Check requirements are present

if [ ! -r "$X509_USER_PROXY" ] ; then
 # Stop if proxy file is missing or deleted
 log_line "Cannot read X509_USER_PROXY file = $X509_USER_PROXY"
 exit 0
fi

curl --version
if [ $? -ne 0 ] ; then
 log_line Failed running curl
 exit 0
fi

cat <<EOF >wfs-send-heartbeat.json
{
  "method"       : "send_heartbeat",
  "jobsub_id"    : "$JOBSUBJOBID"
}
EOF

(
# Subprocess to send regular heartbeats

while :
do
  # Stop if JSON file is missing or deleted
  if [ ! -r wfs-send-heartbeat.json ] ; then
    exit 0
  fi

  curl \
     --user-agent 'wfs-generic-job' \
     --header "X-Jobid: $JOBSUBJOBID" \
     --key $X509_USER_PROXY \
     --cert $X509_USER_PROXY \
     --cacert $X509_USER_PROXY \
     --capath $X509_CERTIFICATES \
     --data @wfs-send-heartbeat.json \
     https://wfs-pro.dune.hep.ac.uk/api/allocator/send_heartbeat

  sleep 600
done

) >wfs-heartbeat.log 2>&1 &

# Create the JSON to send to the allocator
cat <<EOF >wfs-get-stage.json
{
  "method"       : "get_stage",
  "jobsub_id"    : "$JOBSUBJOBID",
  "site_name"    : "${site_name:-XX-UNKNOWN}",
  "cpuinfo"      : "${WFS_CPUINFO:-Unknown}",
  "os_release"   : "${WFS_OS_RELEASE:-Unknown}",
  "hostname"     : "${WFS_HOSTNAME:-unknown}",
  "rss_bytes"    : ${rss_bytes:-0},
  "processors"   : ${processors:-0},
  "wall_seconds" : ${wall_seconds:-0},
  "site_job_id"  : "${JOB_GLIDEIN_SiteWMS_JobId:-unknown}"
}
EOF

echo '====start wfs-get-stage.json===='
cat wfs-get-stage.json
echo '====end wfs-get-stage.json===='

for i in 1 2 3 4 5 
do

# Sleep for up to 60 seconds to spread out job start storms
sleep `expr $RANDOM / 512`

# Make the call to the Workflow Allocator
http_code=`curl \
--retry 0 \
--user-agent 'wfs-generic-job' \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath $X509_CERTIFICATES \
--data @wfs-get-stage.json \
--output wfs-files.tar \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/api/allocator/get_stage`

log_line "($i/5) curl returns HTTP code $http_code"

if [ "$http_code" != "503" ] ; then
 break
fi
done

if [ "$http_code" != "200" ] ; then
  log_line "curl call to WFA to get stage fails with code $http_code"
  cat wfs-files.tar
  echo
  exit 0
fi

tar xvf wfs-files.tar

if [ -r wfs-env.sh ] ; then
  . ./wfs-env.sh
fi

echo '====Start wfs-output-patterns.txt===='
cat wfs-output-patterns.txt
echo '====End wfs-output-patterns.txt===='

echo '====Start wfs-env.sh===='
cat wfs-env.sh
echo '====End wfs-env.sh===='

echo '====Start wfs-bootstrap-env.sh===='
cat wfs-bootstrap-env.sh
echo '====End wfs-bootstrap-env.sh===='

echo '====Start wfs-get-file.json===='
cat wfs-get-file.json
echo '====End wfs-get-file.json===='

# Run the bootstrap script
if [ -f wfs-bootstrap.sh ] ; then
  chmod +x wfs-bootstrap.sh

  echo '====Start wfs-bootstrap.sh===='
  cat wfs-bootstrap.sh
  echo '====End wfs-bootstrap.sh===='

  mkdir workspace
  echo '====Run wfs-bootstrap.sh===='
  export WFS_BOOTSTRAP_SCRIPT_START=`date --iso-8601=seconds --utc`
  ( . ./wfs-bootstrap-env.sh
    cd workspace
    stdbuf -oL -eL $WFS_PATH/wfs-bootstrap.sh ) \
      > workspace/wfs-bootstrap.log 2>&1
  retval=$?
  export WFS_BOOTSTRAP_SCRIPT_FINISH=`date --iso-8601=seconds --utc`
  echo '====After wfs-bootstrap.sh===='
else
  # How can this happen???
  log_line No wfs-bootstrap.sh found
  exit 0
fi

#if [ "$retval" != 0 ] ; then
#  job_aborted 0 0 900 "Bootstrap script error $retval"
#fi

echo '#### wfs-bootstrap.log'
cat workspace/wfs-bootstrap.log
echo '####'

# Make the lists of output files and files for the next stage
echo -n > wfs-outputs.txt
echo -n > wfs-output-dids.txt
echo -n > wfs-next-stage-dids.txt

echo '### Contents of workspace directory ###'
ls -lR workspace
echo '###'

cat wfs-output-patterns.txt | (
while read lifetime for_next_stage dataset scope pattern
do  
  (
    cd workspace
    # $pattern is wildcard-expanded here - so a list of files
    for fn in $pattern
    do
      if [ -r "$fn" ] ; then
        # wfs-outputs.txt gets DATASET SCOPE FILENAME on each line
        echo "$lifetime $dataset $scope $fn" >> $WFS_PATH/wfs-outputs.txt

        if [ "$for_next_stage" = "True" ] ; then
          # wfs-next-stage-dids gets a list of DIDs
          echo "$scope:$fn" >> $WFS_PATH/wfs-next-stage-dids.txt
        else
          # wfs-output-dids.txt gets DID = SCOPE:FILENAME
          echo "$scope:$fn" >> $WFS_PATH/wfs-output-dids.txt
        fi
      fi
    done
  )
done
)

echo '#### wfs-outputs.txt'
cat $WFS_PATH/wfs-outputs.txt
echo '####'
echo
echo '#### wfs-output-dids.txt'
cat $WFS_PATH/wfs-output-dids.txt
echo '####'
echo
echo '#### wfs-next-stage-dids.txt'
cat $WFS_PATH/wfs-next-stage-dids.txt
echo '####'
echo
echo '#### wfs-output-rse-list.txt'
cat $WFS_PATH/wfs-output-rse-list.txt
echo '####'

output_dids=`echo \`sed 's/.*/"&"/' wfs-output-dids.txt\`|sed 's/ /,/g'`

next_stage_dids=`echo \`sed 's/.*/"&"/' wfs-next-stage-dids.txt\`|sed 's/ /,/g'`

# wfs-bootstrap.sh should produce lists of successfully processed input files
if [ -f workspace/wfs-processed-dids.txt ] ; then
  processed_dids=`echo \`sed -r 's/.+/"&"/' workspace/wfs-processed-dids.txt\`|sed 's/ /,/g'`
fi

if [ -f workspace/wfs-processed-pfns.txt ] ; then
  processed_pfns=`echo \`sed -r 's/.+/"&"/' workspace/wfs-processed-pfns.txt\`|sed 's/ /,/g'`
fi

tail -c 10000 workspace/wfs-bootstrap.log | base64 --wrap=0 > wfs-bootstrap.log.b64

cat <<EOF >wfs-record-results.json
{
  "method": "record_results",
  "jobsub_id": "$JOBSUBJOBID",
  "processed_dids": [$processed_dids],
  "processed_pfns": [$processed_pfns],
  "output_dids": [$output_dids],
  "next_stage_dids": [$next_stage_dids],
  "bootstrap_log": "`cat wfs-bootstrap.log.b64`"
}
EOF

echo "=====Start wfs-record-results.json=="
cat wfs-record-results.json
echo "=====End wfs-record-results.json=="

http_code=`curl \
--retry 5 \
--retry-max-time 300 \
--max-time 600 \
--user-agent 'wfs-generic-job' \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--header "X-Jobid: $JOBSUBJOBID" \
--capath $X509_CERTIFICATES \
--data @wfs-record-results.json \
--output record-results.log \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/api/allocator/record_results`

log_line "record_results returns HTTP code $http_code"
echo "=====Start record-results.log=="
touch record-results.log
cat record-results.log
echo "=====End record-results.log=="

if [ "$http_code" != 200 ] ; then
  job_aborted 0 $http_code record_results
fi

# Just try the first RSE for now; eventually will do failovers on errors
export OUTPUT_RSE=`head -1 wfs-output-rse-list.txt | cut -f1 -d' '`
export OUTPUT_PROTOCOL=`head -1 wfs-output-rse-list.txt | cut -f2 -d' '`
log_line "Using $OUTPUT_RSE for output"

cat $WFS_PATH/wfs-outputs.txt | (

cat <<EOF >$WFS_PATH/rucio.cfg
[client]
rucio_host = https://dune-rucio.fnal.gov
auth_host = https://auth-dune-rucio.fnal.gov
account = dunepro
auth_type = x509_proxy
request_retries = 3
EOF

echo '====Start rucio.cfg===='
cat $WFS_PATH/rucio.cfg
echo '====End rucio.cfg===='

source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh
setup rucio
setup metacat

export METACAT_AUTH_SERVER_URL=https://metacat.fnal.gov:8143/auth/dune
export METACAT_SERVER_URL=https://metacat.fnal.gov:9443/dune_meta_demo/app

rucio --config $WFS_PATH/rucio.cfg --version
if [ $? != 0 ] ; then
  job_aborted 1 900 'Rucio test fails'
fi

metacat auth login -m x509 dunepro
metacat auth whoami
if [ $? != 0 ] ; then
  job_aborted 1 900 'MetaCat whoami fails'
fi

touch wfs-output-dids-rses.txt

while read lifetime dataset scope fn
do
  $WFS_PATH/wfs-metadata "$scope" "$fn" > tmp.json
  echo "==== Start MetaCat JSON for $fn ===="
  cat tmp.json
  echo "==== End MetaCat JSON for $fn ===="

  log_line "Try to declare file in MetaCat (1/3)"
  metacat file declare --json tmp.json "$dataset"
  metacat_return_code=$?
  if [ $metacat_return_code != 0 ] ; then
    sleep 1
    log_line "Retry declare file in MetaCat (2/3)"
    metacat file declare --json tmp.json "$dataset"
    metacat_return_code=$?
    if [ $metacat_return_code != 0 ] ; then
      sleep 1
      log_line "Last chance to declare file in MetaCat (3/3)"
      metacat file declare --json tmp.json "$dataset"
      metacat_return_code=$?
    fi
  fi

  if [ $metacat_return_code = 0 ] ; then
    if [ "$lifetime" -gt 0 ] ; then
      lifetime_option="--lifetime $lifetime"
    fi
    log_line "Upload $scope:$fn to $OUTPUT_RSE/$OUTPUT_PROTOCOL"
    rucio --config $WFS_PATH/rucio.cfg --verbose \
          upload \
          $lifetime_option \
          --rse "$OUTPUT_RSE" \
          --transfer-timeout 1200 \
          --protocol "$OUTPUT_PROTOCOL" \
          --scope "$scope" --name "$fn" "workspace/$fn"

    if [ $? = 0 ] ; then
      log_line "Add $scope:$fn to $dataset"
      rucio --config $WFS_PATH/rucio.cfg \
            --verbose attach "$dataset" "$scope:$fn"
      if [ $? != 0 ] ; then
        log_line "rucio attach fails"
        job_aborted 1 900 "rucio attach error"
      fi
      echo "\"$scope:$fn\":\"$OUTPUT_RSE\"" >> wfs-output-dids-rses.txt
    else
      log_line "rucio upload fails"
      job_aborted 1 900 "rucio upload error" "$OUTPUT_RSE"
    fi
  else
    log_line "metacat file declaration fails"
    job_aborted 1 900 "metacat file declare error"
  fi 
done

)
if [ $? != 0 ] ; then
  # Exit/aborts inside subshell just exit the subshell with a non-zero code
  # So we exit the generic job script properly here
  exit 0
fi

# If all ok, then confirm that to the Workflow Allocator

cat <<EOF >wfs-confirm-results.json
{
  "method": "confirm_results",
  "jobsub_id": "$JOBSUBJOBID",
  "output_dids": {
EOF

echo `cat wfs-output-dids-rses.txt` | sed 's/ /,/g' >>wfs-confirm-results.json

echo '} }' >>wfs-confirm-results.json

echo "=====Start wfs-confirm-results.json=="
cat wfs-confirm-results.json
echo "=====End wfs-confirm-results.json=="

http_code=`curl \
--retry 5 \
--retry-max-time 300 \
--max-time 600 \
--user-agent 'wfs-generic-job' \
--header "X-Jobid: $JOBSUBJOBID" \
--key $X509_USER_PROXY \
--cert $X509_USER_PROXY \
--cacert $X509_USER_PROXY \
--capath $X509_CERTIFICATES \
--data @wfs-confirm-results.json \
--output confirm-results.log \
--write-out "%{http_code}\n" \
https://wfs-pro.dune.hep.ac.uk/api/allocator/confirm_results`

log_line "confirm_results returns HTTP code $http_code"
echo "=====Start confirm-results.log=="
touch confirm-results.log
cat confirm-results.log
echo "=====End confirm-results.log=="

if [ "$http_code" != 200 ] ; then
  job_aborted 0 $http_code confirm_results
fi

log_line '====End of wfs-generic-job===='
exit 0

